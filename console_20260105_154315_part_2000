2026-01-05 15:43:24,392 - INFO - Starting Ascend test suite: all
Starting Ascend test suite: all
2026-01-05 15:43:24,392 - INFO - Command args: Namespace(timeout_per_file=3600, suite='all', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/ascend_all_logs')
Command args: Namespace(timeout_per_file=3600, suite='all', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/ascend_all_logs')
2026-01-05 15:43:24,392 - INFO - Log directory: /data/ascend_all_logs/all/20260105_154324
Log directory: /data/ascend_all_logs/all/20260105_154324
2026-01-05 15:43:24,769 - INFO - ✅ Suite sanity check passed
✅ Suite sanity check passed
2026-01-05 15:43:25,063 - INFO - Total test files: 107
Total test files: 107
.
.
Begin (0/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_weight_loader_disable_mmap.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:43:57] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-11B-Vision-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-11B-Vision-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=466651902, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-11B-Vision-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=True, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:43:59] Ignore import error when loading sglang.srt.multimodal.processors.midashenglm: No module named 'torchaudio'
[2026-01-05 15:44:01] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 15:44:16] Automatically turn off --chunked-prefill-size as it is not supported for mllama
[2026-01-05 15:44:16] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 15:44:17] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:44:17] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:44:19] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 15:44:19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:44:20] Load weight begin. avail mem=60.53 GB
[2026-01-05 15:44:20] Multimodal attention backend not set. Use sdpa.
[2026-01-05 15:44:20] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:38,  9.54s/it]

Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:18<00:27,  9.06s/it]

Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:28<00:19,  9.52s/it]

Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:37<00:09,  9.54s/it]

Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:40<00:00,  7.06s/it]

Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:40<00:00,  8.11s/it]

[2026-01-05 15:45:01] Load weight end. type=MllamaForConditionalGeneration, dtype=torch.bfloat16, avail mem=40.16 GB, mem usage=20.37 GB.
[2026-01-05 15:45:01] Using KV cache dtype: torch.bfloat16
[2026-01-05 15:45:01] The available memory for KV cache is 34.07 GB.
[2026-01-05 15:45:01] KV Cache is allocated. #tokens: 223232, K size: 17.04 GB, V size: 17.04 GB
[2026-01-05 15:45:01] Memory pool end. avail mem=5.04 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 15:45:04] max_total_num_tokens=223232, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=5.04 GB
[2026-01-05 15:45:06] INFO:     Started server process [66932]
[2026-01-05 15:45:06] INFO:     Waiting for application startup.
[2026-01-05 15:45:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 15:45:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 15:45:06] INFO:     Application startup complete.
[2026-01-05 15:45:06] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 15:45:07] INFO:     127.0.0.1:42082 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 15:45:08] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 15:45:08.879747888 compiler_depend.ts:198] Warning: Driver Version: "s" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 15:45:08] INFO:     127.0.0.1:42086 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 15:45:18] INFO:     127.0.0.1:36956 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 15:45:28] INFO:     127.0.0.1:34130 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 15:45:34] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2944, in run_scheduler_process
    scheduler.event_loop_overlap()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1167, in event_loop_overlap
    batch_result = self.run_batch(batch)
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2226, in run_batch
    batch_result = self.model_worker.forward_batch_generation(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 421, in forward_batch_generation
    out = self.model_runner.forward(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 2895, in forward
    output = self._forward_raw(
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 2973, in _forward_raw
    ret = self.forward_extend(
          ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 2840, in forward_extend
    return self.model.forward(
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/mllama.py", line 995, in forward
    cross_attention_states = self.vision_model(
                             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/mllama.py", line 426, in forward
    output = self.transformer(
             ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/mllama.py", line 279, in forward
    hidden_states = encoder_layer(
                    ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/mllama.py", line 230, in forward
    hidden_state = self.self_attn(hidden_state, attention_mask=attention_mask)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/attention/vision.py", line 776, in forward
    output = self.qkv_backend.forward(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/attention/vision.py", line 251, in forward
    output = F.scaled_dot_product_attention(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: NPU out of memory. Tried to allocate 2.47 GiB (NPU 0; 60.96 GiB total capacity; 57.83 GiB already allocated; 57.83 GiB current active; 2.10 GiB free; 58.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.

[2026-01-05 15:45:34] SIGQUIT received. signum=None, frame=None. It usually means one child failed.
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_ascend_weight_loader_disable_mmap.py", line 47, in test_weight_loader_mmap
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.
E
======================================================================
ERROR: test_weight_loader_mmap (__main__.TestWeightLoaderDisableMmap.test_weight_loader_mmap)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_ascend_weight_loader_disable_mmap.py", line 47, in test_weight_loader_mmap
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 120.048s

FAILED (errors=1)
[CI Test Method] TestWeightLoaderDisableMmap.test_weight_loader_mmap
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-11B-Vision-Instruct --weight-loader-disable-mmap --attention-backend ascend --disable-cuda-graph --mem-fraction-static 0.9 --device npu --host 127.0.0.1 --port 21000
.
.
End (0/106):
filename='ascend/function/test_ascend_weight_loader_disable_mmap.py', elapsed=139, estimated_time=400
.
.


[CI Retry] ascend/function/test_ascend_weight_loader_disable_mmap.py failed with non-retriable error: RuntimeError - not retrying


✗ FAILED: ascend/function/test_ascend_weight_loader_disable_mmap.py returned exit code 1

.
.
Begin (1/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_tbo_token_distribution_threshold.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-15:46:15 (PID:68814, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-15:46:45 (PID:69131, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestDisableTboTokenDistributionThreshold)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_ascend_tbo_token_distribution_threshold.py", line 35, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

======================================================================
ERROR: setUpClass (__main__.TestTboTokenDistributionThresholdBase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_ascend_tbo_token_distribution_threshold.py", line 35, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 60.025s

FAILED (errors=2)
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B --attention-backend ascend --disable-cuda-graph --tbo-token-distribution-threshold 0 --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B --attention-backend ascend --disable-cuda-graph --tbo-token-distribution-threshold 0.8 --device npu --host 127.0.0.1 --port 21000
.
.
End (1/106):
filename='ascend/function/test_ascend_tbo_token_distribution_threshold.py', elapsed=79, estimated_time=400
.
.


[CI Retry] ascend/function/test_ascend_tbo_token_distribution_threshold.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_ascend_tbo_token_distribution_threshold.py returned exit code 1

.
.
Begin (2/106):
python3 /data/l30079981/260105/ascend/function/test_disaggregation_decode_tp.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_disaggregation_utils.py:53: UserWarning: No RDMA devices specified for disaggregation test, using default settings.
  warnings.warn(msg)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 15:47:33] WARNING server_args.py:2238: Cuda graph is disabled for prefill server when piecewise cuda graph is not enabled.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 15:47:33] WARNING server_args.py:2219: KV cache is forced as chunk cache for decode server
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21100, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=139810403, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_transfer_backend='ascend', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=2, disaggregation_decode_dp=1, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 15:47:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21200, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=899788918, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=2, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='ascend', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 15:47:35] Using default HuggingFace chat template with detected content format: string
[2026-01-05 15:47:35] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:50] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 15:47:52] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:52] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:52] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:53] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 15:47:53] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 15:47:53] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:54] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:54] Load weight begin. avail mem=60.47 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 15:47:55] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 15:47:55] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.89it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.48it/s]
[2026-01-05 15:47:56] Load weight begin. avail mem=60.59 GB

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.86it/s]

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.66it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.68it/s]


Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.37it/s]
[2026-01-05 15:47:57] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=45.21 GB, mem usage=15.26 GB.
[2026-01-05 15:47:57] Using KV cache dtype: torch.bfloat16
[2026-01-05 15:47:57] The available memory for KV cache is 33.08 GB.
[2026-01-05 15:47:57] KV Cache is allocated. #tokens: 270976, K size: 16.55 GB, V size: 16.55 GB
[2026-01-05 15:47:57] Memory pool end. avail mem=11.08 GB

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.08it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.34it/s]
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 15:47:59] max_total_num_tokens=270976, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.08 GB

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]

[2026-01-05 15:48:00] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=45.33 GB, mem usage=15.26 GB.
[2026-01-05 15:48:00] Using KV cache dtype: torch.bfloat16
[2026-01-05 15:48:00] The available memory for KV cache is 33.18 GB.
[2026-01-05 15:48:00] KV Cache is allocated. #tokens: 271744, K size: 16.59 GB, V size: 16.59 GB
[2026-01-05 15:48:00] Memory pool end. avail mem=11.11 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=78, family=2, type=2, proto=0, laddr=('172.16.1.62', 56538), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=85, family=2, type=2, proto=0, laddr=('172.16.1.62', 46177), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2026-01-05 15:48:00] Invalid or no transfer protocol specified, using default protocol.
2026-01-05 15:48:00.728392 error 70363 pid[70363] [BM devmm_svm_gva.cpp:373] Heap is nullptr or alloc memory too large bytesize:1073741824 heap_size:0
2026-01-05 15:48:00.728420 error 70363 pid[70363] [BM devmm_svm_gva.cpp:450] gva alloc mem failed. (size=0x40000000)
2026-01-05 15:48:00.728426 error 70363 pid[70363] [BM devmm_svm_gva.cpp:558] HalGvaInitMemory alloc mem failed. (flag=0 size=0x2000000)
2026-01-05 15:48:00.728434 error 70363 pid[70363] [BM hybm_entry.cpp:177] initialize mete memory with size: 33554432, flag: 0 failed: -1
2026-01-05 15:48:00.728445 error 70363 pid[70363] [SMEM smem_trans.cpp:50] hybm core init failed: -1
2026-01-05 15:48:00.728463 error 70363 pid[70363] [ADAPTER pytransfer.cpp:68] Failed to init smem_trans, ret=-1
[2026-01-05 15:48:00] Ascend Transfer Engine initialization failed.
[2026-01-05 15:48:00] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 341, in __init__
    self.init_disaggregation()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 928, in init_disaggregation
    self.disagg_prefill_bootstrap_queue = PrefillBootstrapQueue(
                                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/prefill.py", line 110, in __init__
    self.kv_manager = self._init_kv_manager()
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/prefill.py", line 180, in _init_kv_manager
    kv_manager: BaseKVManager = kv_manager_class(
                                ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/mooncake/conn.py", line 164, in __init__
    self.init_engine()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/conn.py", line 25, in init_engine
    self.engine = AscendTransferEngine(
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/transfer_engine.py", line 46, in __init__
    self.initialize()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/transfer_engine.py", line 75, in initialize
    raise RuntimeError("Ascend Transfer Engine initialization failed.")
RuntimeError: Ascend Transfer Engine initialization failed.

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:2966: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PULL) at 0xfffae40fe350>
  parent_process.send_signal(signal.SIGQUIT)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:2966: ResourceWarning: Unclosed context <zmq.Context() at 0xfffa8d1b05f0>
  parent_process.send_signal(signal.SIGQUIT)
[2026-01-05 15:48:00] Received sigquit from a child process. It usually means the child failed.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 15:48:01] max_total_num_tokens=271744, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.11 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=78, family=2, type=2, proto=0, laddr=('172.16.1.62', 44312), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=85, family=2, type=2, proto=0, laddr=('172.16.1.62', 39545), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2026-01-05 15:48:02] Invalid or no transfer protocol specified, using default protocol.
2026-01-05 15:48:02.706687 error 70366 pid[70366] [BM devmm_svm_gva.cpp:373] Heap is nullptr or alloc memory too large bytesize:1073741824 heap_size:0
2026-01-05 15:48:02.706713 error 70366 pid[70366] [BM devmm_svm_gva.cpp:450] gva alloc mem failed. (size=0x40000000)
2026-01-05 15:48:02.706720 error 70366 pid[70366] [BM devmm_svm_gva.cpp:558] HalGvaInitMemory alloc mem failed. (flag=0 size=0x2000000)
2026-01-05 15:48:02.706729 error 70366 pid[70366] [BM hybm_entry.cpp:177] initialize mete memory with size: 33554432, flag: 0 failed: -1
2026-01-05 15:48:02.706739 error 70366 pid[70366] [SMEM smem_trans.cpp:50] hybm core init failed: -1
2026-01-05 15:48:02.706756 error 70366 pid[70366] [ADAPTER pytransfer.cpp:68] Failed to init smem_trans, ret=-1
[2026-01-05 15:48:02] Ascend Transfer Engine initialization failed.
[2026-01-05 15:48:02] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 341, in __init__
    self.init_disaggregation()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 885, in init_disaggregation
    self.disagg_decode_prealloc_queue = DecodePreallocQueue(
                                        ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 241, in __init__
    self.kv_manager = self._init_kv_manager()
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 309, in _init_kv_manager
    kv_manager: BaseKVManager = kv_manager_class(
                                ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/mooncake/conn.py", line 164, in __init__
    self.init_engine()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/conn.py", line 25, in init_engine
    self.engine = AscendTransferEngine(
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/transfer_engine.py", line 46, in __init__
    self.initialize()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/ascend/transfer_engine.py", line 75, in initialize
    raise RuntimeError("Ascend Transfer Engine initialization failed.")
RuntimeError: Ascend Transfer Engine initialization failed.

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:2966: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PULL) at 0xfffac1c7b000>
  parent_process.send_signal(signal.SIGQUIT)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:2966: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac1c7e9f0>
  parent_process.send_signal(signal.SIGQUIT)
[2026-01-05 15:48:02] Received sigquit from a child process. It usually means the child failed.
E
======================================================================
ERROR: setUpClass (__main__.TestDisaggregationDecodeTp)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_disaggregation_decode_tp.py", line 35, in setUpClass
    cls.wait_server_ready(cls.prefill_url + "/health")
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_disaggregation_utils.py", line 89, in wait_server_ready
    raise RuntimeError(f"Server {url} failed to start in {timeout}s")
RuntimeError: Server http://127.0.0.1:21100/health failed to start in 3000s

----------------------------------------------------------------------
Ran 0 tests in 3000.951s

FAILED (errors=1)
cls.base_host='127.0.0.1' cls.lb_port='21000' cls.prefill_port='21100' cls.decode_port='21200'
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct --disaggregation-mode prefill --disaggregation-decode-tp 2 --disaggregation-transfer-backend ascend --disable-cuda-graph --attention-backend ascend --mem-fraction-static 0.8 --host 127.0.0.1 --port 21100
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct --disaggregation-mode decode --base-gpu-id 2 --disaggregation-transfer-backend ascend --disable-cuda-graph --attention-backend ascend --mem-fraction-static 0.8 --host 127.0.0.1 --port 21200
.
.
End (2/106):
filename='ascend/function/test_disaggregation_decode_tp.py', elapsed=3020, estimated_time=400
.
.


[CI Retry] ascend/function/test_disaggregation_decode_tp.py failed with non-retriable error: RuntimeError - not retrying


✗ FAILED: ascend/function/test_disaggregation_decode_tp.py returned exit code 1

.
.
Begin (3/106):
python3 /data/l30079981/260105/ascend/function/test_hicache.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:37:53] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=947990879, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=True, hicache_ratio=2.0, hicache_size=100, hicache_write_policy='write_through', hicache_io_backend='kernel_ascend', hicache_mem_layout='page_first_direct', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:37:54] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:38:11] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:38:12] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:38:12] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:38:14] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:38:14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:38:15] Load weight begin. avail mem=60.53 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.86it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.46it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.84it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.77it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]

[2026-01-05 16:38:18] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=45.27 GB, mem usage=15.26 GB.
[2026-01-05 16:38:18] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:38:18] The available memory for KV cache is 27.08 GB.
[2026-01-05 16:38:18] KV Cache is allocated. #tokens: 221696, K size: 13.54 GB, V size: 13.54 GB
[2026-01-05 16:38:18] Memory pool end. avail mem=17.15 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:38:19] max_total_num_tokens=221696, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=17.15 GB
[2026-01-05 16:38:19] Allocating 100.01 GB host memory for hierarchical KV cache.
[2026-01-05 16:38:42] INFO:     Started server process [72161]
[2026-01-05 16:38:42] INFO:     Waiting for application startup.
[2026-01-05 16:38:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:38:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:38:42] INFO:     Application startup complete.
[2026-01-05 16:38:42] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:38:43] INFO:     127.0.0.1:50428 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 16:38:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:38:43.701227121 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 16:38:45] INFO:     127.0.0.1:50432 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:38:55] INFO:     127.0.0.1:47586 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:39:05] INFO:     127.0.0.1:34448 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:39:08] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:39:08] The server is fired up and ready to roll!
[2026-01-05 16:39:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:39:16] INFO:     127.0.0.1:35936 - "GET /health_generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct --attention-backend ascend --disable-cuda-graph --enable-hierarchical-cache --mem-fraction-static 0.7 --hicache-size 100 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestHiCache.test_mmlu
ChatCompletionSampler initialized with self.system_message=None self.temperature=0.0 self.max_tokens=2048 self.reasoning_effort=None self.extra_body=None

  0%|          | 0/64 [00:00<?, ?it/s][2026-01-05 16:39:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:39:22] Prefill batch, #new-seq: 27, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 16:39:22] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 28, #queue-req: 0,
[2026-01-05 16:39:26] Decode batch, #running-req: 32, #token: 8960, token usage: 0.04, cpu graph: False, gen throughput (token/s): 13.26, #queue-req: 0,
[2026-01-05 16:39:27] Decode batch, #running-req: 32, #token: 9472, token usage: 0.04, cpu graph: False, gen throughput (token/s): 747.17, #queue-req: 0,
[2026-01-05 16:39:27] INFO:     127.0.0.1:52710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:27] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.04, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:28] INFO:     127.0.0.1:52716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.04, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:29] Decode batch, #running-req: 32, #token: 10368, token usage: 0.05, cpu graph: False, gen throughput (token/s): 693.76, #queue-req: 0,
[2026-01-05 16:39:31] Decode batch, #running-req: 32, #token: 13184, token usage: 0.06, cpu graph: False, gen throughput (token/s): 738.67, #queue-req: 0,
[2026-01-05 16:39:32] INFO:     127.0.0.1:52740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:32] INFO:     127.0.0.1:52702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:32] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:32] INFO:     127.0.0.1:52724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:33] INFO:     127.0.0.1:52706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:33] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 32, #queue-req: 0,
[2026-01-05 16:39:33] INFO:     127.0.0.1:52728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:33] Decode batch, #running-req: 32, #token: 12544, token usage: 0.06, cpu graph: False, gen throughput (token/s): 668.87, #queue-req: 0,
[2026-01-05 16:39:33] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:33] INFO:     127.0.0.1:52712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:33] INFO:     127.0.0.1:52726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:33] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:34] INFO:     127.0.0.1:52678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:34] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:34] INFO:     127.0.0.1:52736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:34] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:34] INFO:     127.0.0.1:52734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:34] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:35] INFO:     127.0.0.1:52688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:35] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:35] INFO:     127.0.0.1:52720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:35] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:35] Decode batch, #running-req: 32, #token: 12672, token usage: 0.06, cpu graph: False, gen throughput (token/s): 578.16, #queue-req: 0,
[2026-01-05 16:39:35] INFO:     127.0.0.1:52684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:35] INFO:     127.0.0.1:52732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:35] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:35] INFO:     127.0.0.1:52730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:39:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 33, #queue-req: 0,
[2026-01-05 16:39:35] INFO:     127.0.0.1:52704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:36] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:36] INFO:     127.0.0.1:52696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:36] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:37] INFO:     127.0.0.1:52694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:37] INFO:     127.0.0.1:52682 - "POST /v1/chat/completions HTTP/1.1" 200 OK

  2%|▏         | 1/64 [00:15<16:32, 15.75s/it][2026-01-05 16:39:37] Decode batch, #running-req: 32, #token: 12672, token usage: 0.06, cpu graph: False, gen throughput (token/s): 605.91, #queue-req: 0,
[2026-01-05 16:39:37] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:37] INFO:     127.0.0.1:52722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:38] INFO:     127.0.0.1:52698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:39] INFO:     127.0.0.1:52690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:39] Decode batch, #running-req: 32, #token: 12800, token usage: 0.06, cpu graph: False, gen throughput (token/s): 650.69, #queue-req: 0,
[2026-01-05 16:39:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:39] INFO:     127.0.0.1:52738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:39] INFO:     127.0.0.1:52718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:40] INFO:     127.0.0.1:52680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:40] INFO:     127.0.0.1:52700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:40] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:41] INFO:     127.0.0.1:52728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:41] Decode batch, #running-req: 32, #token: 12672, token usage: 0.06, cpu graph: False, gen throughput (token/s): 609.82, #queue-req: 0,
[2026-01-05 16:39:42] INFO:     127.0.0.1:52708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:42] INFO:     127.0.0.1:52684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:42] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.06, #running-req: 31, #queue-req: 0,
[2026-01-05 16:39:42] INFO:     127.0.0.1:52740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:42] INFO:     127.0.0.1:52706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.06, #running-req: 30, #queue-req: 0,
[2026-01-05 16:39:43] Decode batch, #running-req: 31, #token: 12928, token usage: 0.06, cpu graph: False, gen throughput (token/s): 658.22, #queue-req: 0,
[2026-01-05 16:39:43] INFO:     127.0.0.1:52710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:43] INFO:     127.0.0.1:52688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:44] INFO:     127.0.0.1:52678 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:44] INFO:     127.0.0.1:52686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:44] INFO:     127.0.0.1:52704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:45] Decode batch, #running-req: 26, #token: 10624, token usage: 0.05, cpu graph: False, gen throughput (token/s): 609.57, #queue-req: 0,
[2026-01-05 16:39:45] INFO:     127.0.0.1:52702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:45] INFO:     127.0.0.1:52730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:46] INFO:     127.0.0.1:52724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:46] INFO:     127.0.0.1:52736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:46] INFO:     127.0.0.1:52716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:46] INFO:     127.0.0.1:52738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:47] INFO:     127.0.0.1:52696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:47] Decode batch, #running-req: 19, #token: 8960, token usage: 0.04, cpu graph: False, gen throughput (token/s): 505.45, #queue-req: 0,
[2026-01-05 16:39:47] INFO:     127.0.0.1:52722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:47] INFO:     127.0.0.1:52726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:48] INFO:     127.0.0.1:52718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:48] INFO:     127.0.0.1:52732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:48] Decode batch, #running-req: 15, #token: 7424, token usage: 0.03, cpu graph: False, gen throughput (token/s): 375.82, #queue-req: 0,
[2026-01-05 16:39:49] INFO:     127.0.0.1:52712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:50] INFO:     127.0.0.1:52720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:50] INFO:     127.0.0.1:52734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:50] Decode batch, #running-req: 12, #token: 5888, token usage: 0.03, cpu graph: False, gen throughput (token/s): 312.32, #queue-req: 0,
[2026-01-05 16:39:50] INFO:     127.0.0.1:52698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:51] INFO:     127.0.0.1:52708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:51] INFO:     127.0.0.1:52682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:52] INFO:     127.0.0.1:52694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:52] Decode batch, #running-req: 8, #token: 4608, token usage: 0.02, cpu graph: False, gen throughput (token/s): 224.69, #queue-req: 0,
[2026-01-05 16:39:52] INFO:     127.0.0.1:52700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:53] INFO:     127.0.0.1:52690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:53] INFO:     127.0.0.1:52680 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:54] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 139.48, #queue-req: 0,
[2026-01-05 16:39:55] INFO:     127.0.0.1:52728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:56] Decode batch, #running-req: 4, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 107.04, #queue-req: 0,
[2026-01-05 16:39:56] INFO:     127.0.0.1:52684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:39:57] Decode batch, #running-req: 3, #token: 2304, token usage: 0.01, cpu graph: False, gen throughput (token/s): 73.39, #queue-req: 0,
[2026-01-05 16:39:59] Decode batch, #running-req: 3, #token: 2432, token usage: 0.01, cpu graph: False, gen throughput (token/s): 70.21, #queue-req: 0,
[2026-01-05 16:40:00] INFO:     127.0.0.1:52706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:40:01] Decode batch, #running-req: 2, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 58.46, #queue-req: 0,
[2026-01-05 16:40:02] Decode batch, #running-req: 2, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.03, #queue-req: 0,
[2026-01-05 16:40:04] Decode batch, #running-req: 2, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.64, #queue-req: 0,
[2026-01-05 16:40:06] Decode batch, #running-req: 2, #token: 2432, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.53, #queue-req: 0,
[2026-01-05 16:40:08] Decode batch, #running-req: 2, #token: 2432, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.55, #queue-req: 0,
[2026-01-05 16:40:09] Decode batch, #running-req: 2, #token: 2432, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.64, #queue-req: 0,
[2026-01-05 16:40:11] Decode batch, #running-req: 2, #token: 2560, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.22, #queue-req: 0,
[2026-01-05 16:40:13] Decode batch, #running-req: 2, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.95, #queue-req: 0,
[2026-01-05 16:40:14] Decode batch, #running-req: 2, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.64, #queue-req: 0,
[2026-01-05 16:40:16] Decode batch, #running-req: 2, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.33, #queue-req: 0,
[2026-01-05 16:40:18] Decode batch, #running-req: 2, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.71, #queue-req: 0,
[2026-01-05 16:40:19] Decode batch, #running-req: 2, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 48.00, #queue-req: 0,
[2026-01-05 16:40:21] Decode batch, #running-req: 2, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.08, #queue-req: 0,
[2026-01-05 16:40:23] Decode batch, #running-req: 2, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 48.41, #queue-req: 0,
[2026-01-05 16:40:24] Decode batch, #running-req: 2, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 47.53, #queue-req: 0,
[2026-01-05 16:40:26] Decode batch, #running-req: 2, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 46.44, #queue-req: 0,
[2026-01-05 16:40:28] Decode batch, #running-req: 2, #token: 3456, token usage: 0.02, cpu graph: False, gen throughput (token/s): 46.18, #queue-req: 0,
[2026-01-05 16:40:30] Decode batch, #running-req: 2, #token: 3456, token usage: 0.02, cpu graph: False, gen throughput (token/s): 45.53, #queue-req: 0,
[2026-01-05 16:40:31] Decode batch, #running-req: 2, #token: 3456, token usage: 0.02, cpu graph: False, gen throughput (token/s): 45.93, #queue-req: 0,
[2026-01-05 16:40:33] Decode batch, #running-req: 2, #token: 3712, token usage: 0.02, cpu graph: False, gen throughput (token/s): 45.67, #queue-req: 0,
[2026-01-05 16:40:35] Decode batch, #running-req: 2, #token: 3712, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.10, #queue-req: 0,
[2026-01-05 16:40:37] Decode batch, #running-req: 2, #token: 3712, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.23, #queue-req: 0,
[2026-01-05 16:40:38] Decode batch, #running-req: 2, #token: 3840, token usage: 0.02, cpu graph: False, gen throughput (token/s): 48.16, #queue-req: 0,
[2026-01-05 16:40:40] Decode batch, #running-req: 2, #token: 3968, token usage: 0.02, cpu graph: False, gen throughput (token/s): 48.37, #queue-req: 0,
[2026-01-05 16:40:41] Decode batch, #running-req: 2, #token: 3968, token usage: 0.02, cpu graph: False, gen throughput (token/s): 49.04, #queue-req: 0,
[2026-01-05 16:40:43] Decode batch, #running-req: 2, #token: 3968, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.88, #queue-req: 0,
[2026-01-05 16:40:45] Decode batch, #running-req: 2, #token: 4224, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.32, #queue-req: 0,
[2026-01-05 16:40:47] Decode batch, #running-req: 2, #token: 4224, token usage: 0.02, cpu graph: False, gen throughput (token/s): 48.05, #queue-req: 0,
[2026-01-05 16:40:48] Decode batch, #running-req: 2, #token: 4224, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.74, #queue-req: 0,
[2026-01-05 16:40:50] Decode batch, #running-req: 2, #token: 4480, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.67, #queue-req: 0,
[2026-01-05 16:40:52] Decode batch, #running-req: 2, #token: 4480, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.53, #queue-req: 0,
[2026-01-05 16:40:53] Decode batch, #running-req: 2, #token: 4480, token usage: 0.02, cpu graph: False, gen throughput (token/s): 47.23, #queue-req: 0,
[2026-01-05 16:40:54] INFO:     127.0.0.1:52692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:40:54] INFO:     127.0.0.1:52714 - "POST /v1/chat/completions HTTP/1.1" 200 OK

  3%|▎         | 2/64 [01:32<53:28, 51.75s/it]
100%|██████████| 64/64 [01:32<00:00,  1.45s/it]
.
----------------------------------------------------------------------
Ran 1 test in 198.928s

OK
Total latency: 92.721 s
Score: 0.703
Writing report to /tmp/mmlu__root_.cache_modelscope_hub_models_AI-ModelScope_Llama-3.1-8B-Instruct.html
{'other': np.float64(0.8125), 'other:std': np.float64(0.3903123748998999), 'score:std': np.float64(0.4568809849129202), 'stem': np.float64(0.5454545454545454), 'stem:std': np.float64(0.49792959773196915), 'humanities': np.float64(0.6956521739130435), 'humanities:std': np.float64(0.4601306627938419), 'social_sciences': np.float64(0.7142857142857143), 'social_sciences:std': np.float64(0.45175395145262565), 'score': np.float64(0.703125)}
Writing results to /tmp/mmlu__root_.cache_modelscope_hub_models_AI-ModelScope_Llama-3.1-8B-Instruct.json
.
.
End (3/106):
filename='ascend/function/test_hicache.py', elapsed=217, estimated_time=400
.
.

.
.
Begin (4/106):
python3 /data/l30079981/260105/ascend/function/test_torch_compile.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:41:31 (PID:74505, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestTorchCompile)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_torch_compile.py", line 24, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 30.012s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct --enable-torch-compile --cuda-graph-max-bs 4 --attention-backend ascend --disable-cuda-graph --base-gpu-id 2 --device npu --host 127.0.0.1 --port 21000
.
.
End (4/106):
filename='ascend/function/test_torch_compile.py', elapsed=50, estimated_time=400
.
.


[CI Retry] ascend/function/test_torch_compile.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_torch_compile.py returned exit code 1

.
.
Begin (5/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_lora_backend.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:42:21] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3.1-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3.1-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=820895643, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3.1-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:42:21] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:42:38] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:42:40] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:42:40] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:42:41] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:42:41] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:42:42] Load weight begin. avail mem=60.53 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.82s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.33s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.07it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.00it/s]

[2026-01-05 16:42:47] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=45.27 GB, mem usage=15.26 GB.
[2026-01-05 16:42:47] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:42:47] The available memory for KV cache is 33.13 GB.
[2026-01-05 16:42:47] KV Cache is allocated. #tokens: 271360, K size: 16.57 GB, V size: 16.57 GB
[2026-01-05 16:42:47] Memory pool end. avail mem=11.09 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:42:48] max_total_num_tokens=271360, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.09 GB
[2026-01-05 16:42:49] INFO:     Started server process [75120]
[2026-01-05 16:42:49] INFO:     Waiting for application startup.
[2026-01-05 16:42:49] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:42:49] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:42:49] INFO:     Application startup complete.
[2026-01-05 16:42:49] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:42:50] INFO:     127.0.0.1:44912 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 16:42:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:42:50.296585063 compiler_depend.ts:198] Warning: Driver Version: "9" is invalid or not supported yet. (function operator())
[2026-01-05 16:42:53] INFO:     127.0.0.1:44916 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:43:03] INFO:     127.0.0.1:48144 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:43:13] INFO:     127.0.0.1:55578 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:43:13] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:43:13] The server is fired up and ready to roll!
[2026-01-05 16:43:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:43:24] INFO:     127.0.0.1:33548 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 16:43:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:43:25] INFO:     127.0.0.1:33550 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 16:43:25] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:43:26] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: False, gen throughput (token/s): 0.82, #queue-req: 0,
[2026-01-05 16:43:26] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:43:26] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2026-01-05 16:43:26] INFO:     127.0.0.1:49624 - "GET /get_server_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 75120 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 83.550s

OK
[CI Test Method] TestLoraBackend.test_lora_backend
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3.1-8B-Instruct --lora-backend triton --attention-backend ascend --disable-cuda-graph --mem-fraction-static 0.8 --device npu --host 127.0.0.1 --port 21000
.
.
End (5/106):
filename='ascend/function/test_ascend_lora_backend.py', elapsed=103, estimated_time=400
.
.

.
.
Begin (6/106):
python3 /data/l30079981/260105/ascend/function/test_json_mode.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b79d3-79ccc41d096b6c4931bfe24d;4f058e54-bac4-4fee-b7c9-d4125c4d611b)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b79d3-79ccc41d096b6c4931bfe24d;4f058e54-bac4-4fee-b7c9-d4125c4d611b)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-16:44:04 (PID:77384, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b79f2-3fe223141e5613ed766be214;8cac20a0-0c41-45c2-ae6c-8bee66eab30c)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b79f2-3fe223141e5613ed766be214;8cac20a0-0c41-45c2-ae6c-8bee66eab30c)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-16:44:34 (PID:77697, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b7a0f-38ab164735f1ac3a0194969d;740faf13-174d-48c9-9f5e-474996e22002)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b7a0f-38ab164735f1ac3a0194969d;740faf13-174d-48c9-9f5e-474996e22002)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-16:45:04 (PID:78010, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestJSONModeLLGuidance)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_json_mode.py", line 105, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

======================================================================
ERROR: setUpClass (__main__.TestJSONModeOutlines)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_json_mode.py", line 105, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

======================================================================
ERROR: setUpClass (__main__.TestJSONModeXGrammar)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_json_mode.py", line 105, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 90.031s

FAILED (errors=3)
command=python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-Instruct --max-running-requests 10 --grammar-backend llguidance --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-Instruct --max-running-requests 10 --grammar-backend outlines --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-Instruct --max-running-requests 10 --grammar-backend xgrammar --device npu --host 127.0.0.1 --port 21000
.
.
End (6/106):
filename='ascend/function/test_json_mode.py', elapsed=109, estimated_time=400
.
.


[CI Retry] ascend/function/test_json_mode.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_json_mode.py returned exit code 1

.
.
Begin (7/106):
python3 /data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 680, in __post_init__
    self._handle_model_specific_adjustments()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 1013, in _handle_model_specific_adjustments
    hf_config = self.get_model_config().hf_config
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:45:53 (PID:78625, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
ETraceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 58, in test_chat_completion
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'
ETraceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 68, in test_chat_completion_stream
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'
ETraceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 35, in test_completion
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'
ETraceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 47, in test_comptetion_stream
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:46:23 (PID:78938, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:46:52 (PID:79251, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestOpenAIServerWithEAGLE3AndHiddenStatesEnabled)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 338, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

======================================================================
ERROR: test_chat_completion (__main__.TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_chat_completion)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 58, in test_chat_completion
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

======================================================================
ERROR: test_chat_completion_stream (__main__.TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_chat_completion_stream)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 68, in test_chat_completion_stream
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

======================================================================
ERROR: test_completion (__main__.TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_completion)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 35, in test_completion
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

======================================================================
ERROR: test_comptetion_stream (__main__.TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_comptetion_stream)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 47, in test_comptetion_stream
    for return_hidden_states in self.return_hidden_states:
                                ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TestOpenAIServerWithEAGLEAndHiddenStatesEnabled' object has no attribute 'return_hidden_states'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

======================================================================
ERROR: setUpClass (__main__.TestOpenAIServerWithHiddenStatesEnabled)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 224, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

======================================================================
ERROR: setUpClass (__main__.TestOpenAIServerWithHiddenStatesEnabledAndCUDAGraphDisabled)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_openai_server_hidden_states_bak.py", line 252, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 4 tests in 90.039s

FAILED (errors=7)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/ --speculative-algorithm EAGLE3 --speculative-draft-model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/sglang-EAGLE3-LLaMA3.1-Instruct-8B --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2 --mem-fraction-static 0.8 --chunked-prefill-size 128 --max-running-requests 8 --dtype float16 --enable-return-hidden-states --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_chat_completion
----------------------testcase3
[CI Test Method] TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_chat_completion_stream
----------------------testcase4
[CI Test Method] TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_completion
----------------------testcase1
[CI Test Method] TestOpenAIServerWithEAGLEAndHiddenStatesEnabled.test_comptetion_stream
----------------------testcase2
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct --enable-return-hidden-states --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct --enable-return-hidden-states --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
.
.
End (7/106):
filename='ascend/function/test_openai_server_hidden_states_bak.py', elapsed=109, estimated_time=400
.
.


[CI Retry] ascend/function/test_openai_server_hidden_states_bak.py failed with non-retriable error: AttributeError - not retrying


✗ FAILED: ascend/function/test_openai_server_hidden_states_bak.py returned exit code 1

.
.
Begin (8/106):
python3 /data/l30079981/260105/ascend/function/test_hicache_page.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 671, in __post_init__
    self._handle_npu_backends()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 799, in _handle_npu_backends
    set_default_server_args(self)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/hardware_backend/npu/utils.py", line 80, in set_default_server_args
    if args.use_mla_backend():
       ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4585, in use_mla_backend
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct/' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:47:42 (PID:79866, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestHiCachePage)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_hicache_page.py", line 20, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 30.011s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct/ --enable-hierarchical-cache --page-size 128 --hicache-write-policy write_back --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
.
.
End (8/106):
filename='ascend/function/test_hicache_page.py', elapsed=49, estimated_time=400
.
.


[CI Retry] ascend/function/test_hicache_page.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_hicache_page.py returned exit code 1

.
.
Begin (9/106):
python3 /data/l30079981/260105/ascend/function/test_torch_compile_moe.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen1.5-MoE-A2.7B/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen1.5-MoE-A2.7B/'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 680, in __post_init__
    self._handle_model_specific_adjustments()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 1013, in _handle_model_specific_adjustments
    hf_config = self.get_model_config().hf_config
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen1.5-MoE-A2.7B/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen1.5-MoE-A2.7B/' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:48:31 (PID:80479, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestTorchCompileMoe)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_torch_compile_moe.py", line 24, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 30.011s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen1.5-MoE-A2.7B/ --enable-torch-compile --torch-compile-max-bs 4 --attention-backend ascend --disable-cuda-graph --mem-fraction-static 0.85 --device npu --host 127.0.0.1 --port 21000
.
.
End (9/106):
filename='ascend/function/test_torch_compile_moe.py', elapsed=49, estimated_time=400
.
.


[CI Retry] ascend/function/test_torch_compile_moe.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_torch_compile_moe.py returned exit code 1

.
.
Begin (10/106):
python3 /data/l30079981/260105/ascend/function/test_pytorch_sampling_backend.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:49:21] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.791, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=321816239, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:49:21] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:49:38] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:49:39] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:49:39] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:49:40] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:49:40] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:49:42] Load weight begin. avail mem=60.53 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.93it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.54it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.94it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.87it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.83it/s]

[2026-01-05 16:49:44] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=45.27 GB, mem usage=15.26 GB.
[2026-01-05 16:49:44] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:49:44] The available memory for KV cache is 32.58 GB.
[2026-01-05 16:49:44] KV Cache is allocated. #tokens: 266880, K size: 16.30 GB, V size: 16.30 GB
[2026-01-05 16:49:44] Memory pool end. avail mem=11.64 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:49:45] max_total_num_tokens=266880, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.64 GB
[2026-01-05 16:49:47] INFO:     Started server process [81092]
[2026-01-05 16:49:47] INFO:     Waiting for application startup.
[2026-01-05 16:49:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:49:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:49:47] INFO:     Application startup complete.
[2026-01-05 16:49:47] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:49:48] INFO:     127.0.0.1:42394 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 16:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:49:48.087537122 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 16:49:52] INFO:     127.0.0.1:42398 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:50:02] INFO:     127.0.0.1:33320 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:50:11] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:11] The server is fired up and ready to roll!
[2026-01-05 16:50:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:13] INFO:     127.0.0.1:46340 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 16:50:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:15] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cpu graph: False, gen throughput (token/s): 1.06, #queue-req: 0,
[2026-01-05 16:50:15] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:16] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:16] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: False, gen throughput (token/s): 22.75, #queue-req: 0,
[2026-01-05 16:50:17] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:18] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: False, gen throughput (token/s): 23.32, #queue-req: 0,
[2026-01-05 16:50:19] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:20] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: False, gen throughput (token/s): 23.66, #queue-req: 0,
[2026-01-05 16:50:20] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:50:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:20] Prefill batch, #new-seq: 9, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 16:50:22] Decode batch, #running-req: 10, #token: 0, token usage: 0.00, cpu graph: False, gen throughput (token/s): 179.16, #queue-req: 0,
[2026-01-05 16:50:22] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
.command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/Llama-3.1-8B-Instruct --attention-backend ascend --disable-cuda-graph --sampling-backend pytorch --disable-radix-cache --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestPyTorchSamplingBackend.test_greedy
[CI Test Method] TestPyTorchSamplingBackend.test_mmlu
ChatCompletionSampler initialized with self.system_message=None self.temperature=0.1 self.max_tokens=2048 self.reasoning_effort=None self.extra_body=None

  0%|          | 0/64 [00:00<?, ?it/s][2026-01-05 16:50:27] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:50:27] Prefill batch, #new-seq: 31, #new-token: 7168, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 16:50:31] Decode batch, #running-req: 32, #token: 8960, token usage: 0.03, cpu graph: False, gen throughput (token/s): 127.50, #queue-req: 0,
[2026-01-05 16:50:35] INFO:     127.0.0.1:55734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:35] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 0,
[2026-01-05 16:50:35] Decode batch, #running-req: 32, #token: 9728, token usage: 0.04, cpu graph: False, gen throughput (token/s): 324.22, #queue-req: 0,
[2026-01-05 16:50:36] INFO:     127.0.0.1:55738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:36] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.04, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:39] Decode batch, #running-req: 32, #token: 10752, token usage: 0.04, cpu graph: False, gen throughput (token/s): 321.39, #queue-req: 0,
[2026-01-05 16:50:43] INFO:     127.0.0.1:55750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:43] Decode batch, #running-req: 32, #token: 13056, token usage: 0.05, cpu graph: False, gen throughput (token/s): 320.98, #queue-req: 0,
[2026-01-05 16:50:44] INFO:     127.0.0.1:55720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:44] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:46] INFO:     127.0.0.1:55726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:46] INFO:     127.0.0.1:55742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:46] INFO:     127.0.0.1:55724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:50:47] Decode batch, #running-req: 32, #token: 12928, token usage: 0.05, cpu graph: False, gen throughput (token/s): 310.73, #queue-req: 0,
[2026-01-05 16:50:48] INFO:     127.0.0.1:55730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:48] INFO:     127.0.0.1:55758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:48] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:49] INFO:     127.0.0.1:55744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:49] INFO:     127.0.0.1:55756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:51] INFO:     127.0.0.1:55740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:51] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:52] Decode batch, #running-req: 32, #token: 13184, token usage: 0.05, cpu graph: False, gen throughput (token/s): 307.84, #queue-req: 0,
[2026-01-05 16:50:52] INFO:     127.0.0.1:55762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:52] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:52] INFO:     127.0.0.1:55718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:52] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:52] INFO:     127.0.0.1:55702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:52] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:50:53] INFO:     127.0.0.1:55716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:50:55] INFO:     127.0.0.1:55752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:55] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:55] INFO:     127.0.0.1:55748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:55] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:56] INFO:     127.0.0.1:55754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:56] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:56] INFO:     127.0.0.1:55706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:56] INFO:     127.0.0.1:55760 - "POST /v1/chat/completions HTTP/1.1" 200 OK

  2%|▏         | 1/64 [00:29<30:41, 29.23s/it][2026-01-05 16:50:56] Decode batch, #running-req: 32, #token: 12544, token usage: 0.05, cpu graph: False, gen throughput (token/s): 299.69, #queue-req: 0,
[2026-01-05 16:50:56] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 0, token usage: 0.05, #running-req: 30, #queue-req: 0,
[2026-01-05 16:50:56] INFO:     127.0.0.1:55710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:56] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:57] INFO:     127.0.0.1:55728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:50:57] INFO:     127.0.0.1:55714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 0,
[2026-01-05 16:50:58] INFO:     127.0.0.1:55708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:58] INFO:     127.0.0.1:55722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:50:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 0, token usage: 0.04, #running-req: 30, #queue-req: 0,
[2026-01-05 16:51:00] Decode batch, #running-req: 32, #token: 12416, token usage: 0.05, cpu graph: False, gen throughput (token/s): 304.75, #queue-req: 0,
[2026-01-05 16:51:00] INFO:     127.0.0.1:55746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:00] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.04, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:01] INFO:     127.0.0.1:55732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:01] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.04, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:02] INFO:     127.0.0.1:55742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:03] INFO:     127.0.0.1:55724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.04, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:04] INFO:     127.0.0.1:55750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:04] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:04] Decode batch, #running-req: 32, #token: 12544, token usage: 0.05, cpu graph: False, gen throughput (token/s): 311.99, #queue-req: 0,
[2026-01-05 16:51:05] INFO:     127.0.0.1:55700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.05, #running-req: 32, #queue-req: 0,
[2026-01-05 16:51:06] INFO:     127.0.0.1:55734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:07] INFO:     127.0.0.1:55702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:08] Decode batch, #running-req: 30, #token: 12160, token usage: 0.05, cpu graph: False, gen throughput (token/s): 314.85, #queue-req: 0,
[2026-01-05 16:51:10] INFO:     127.0.0.1:55738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:10] INFO:     127.0.0.1:55744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:11] INFO:     127.0.0.1:55716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:11] INFO:     127.0.0.1:55726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:12] INFO:     127.0.0.1:55748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:12] Decode batch, #running-req: 25, #token: 11136, token usage: 0.04, cpu graph: False, gen throughput (token/s): 312.00, #queue-req: 0,
[2026-01-05 16:51:13] INFO:     127.0.0.1:55756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:13] INFO:     127.0.0.1:55708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:15] INFO:     127.0.0.1:55758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:15] INFO:     127.0.0.1:55720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:15] INFO:     127.0.0.1:55722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:15] INFO:     127.0.0.1:55728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:15] Decode batch, #running-req: 20, #token: 9216, token usage: 0.03, cpu graph: False, gen throughput (token/s): 278.08, #queue-req: 0,
[2026-01-05 16:51:16] INFO:     127.0.0.1:55752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:17] INFO:     127.0.0.1:55762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:17] INFO:     127.0.0.1:55710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:17] INFO:     127.0.0.1:55730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:18] Decode batch, #running-req: 15, #token: 7808, token usage: 0.03, cpu graph: False, gen throughput (token/s): 236.39, #queue-req: 0,
[2026-01-05 16:51:19] INFO:     127.0.0.1:55712 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:20] INFO:     127.0.0.1:55740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:21] INFO:     127.0.0.1:55718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:21] Decode batch, #running-req: 12, #token: 6400, token usage: 0.02, cpu graph: False, gen throughput (token/s): 205.38, #queue-req: 0,
[2026-01-05 16:51:22] INFO:     127.0.0.1:55754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:22] INFO:     127.0.0.1:55714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:22] INFO:     127.0.0.1:55760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:23] Decode batch, #running-req: 9, #token: 5504, token usage: 0.02, cpu graph: False, gen throughput (token/s): 169.77, #queue-req: 0,
[2026-01-05 16:51:24] INFO:     127.0.0.1:55706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:24] INFO:     127.0.0.1:55724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:25] INFO:     127.0.0.1:55732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:25] INFO:     127.0.0.1:55742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:26] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 128.97, #queue-req: 0,
[2026-01-05 16:51:27] INFO:     127.0.0.1:55746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:28] Decode batch, #running-req: 4, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 92.60, #queue-req: 0,
[2026-01-05 16:51:30] Decode batch, #running-req: 4, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 79.06, #queue-req: 0,
[2026-01-05 16:51:30] INFO:     127.0.0.1:55750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:30] INFO:     127.0.0.1:55700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:51:32] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, cpu graph: False, gen throughput (token/s): 54.07, #queue-req: 0,
[2026-01-05 16:51:33] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 43.99, #queue-req: 0,
[2026-01-05 16:51:35] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 43.59, #queue-req: 0,
[2026-01-05 16:51:37] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 43.31, #queue-req: 0,
[2026-01-05 16:51:39] Decode batch, #running-req: 2, #token: 2304, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.17, #queue-req: 0,
[2026-01-05 16:51:41] Decode batch, #running-req: 2, #token: 2304, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.45, #queue-req: 0,
[2026-01-05 16:51:43] Decode batch, #running-req: 2, #token: 2304, token usage: 0.01, cpu graph: False, gen throughput (token/s): 43.46, #queue-req: 0,
[2026-01-05 16:51:45] Decode batch, #running-req: 2, #token: 2560, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.29, #queue-req: 0,
[2026-01-05 16:51:46] Decode batch, #running-req: 2, #token: 2560, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.97, #queue-req: 0,
[2026-01-05 16:51:48] Decode batch, #running-req: 2, #token: 2560, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.64, #queue-req: 0,
[2026-01-05 16:51:50] Decode batch, #running-req: 2, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.86, #queue-req: 0,
[2026-01-05 16:51:52] Decode batch, #running-req: 2, #token: 2816, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.99, #queue-req: 0,
[2026-01-05 16:51:54] Decode batch, #running-req: 2, #token: 2816, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.92, #queue-req: 0,
[2026-01-05 16:51:56] Decode batch, #running-req: 2, #token: 2816, token usage: 0.01, cpu graph: False, gen throughput (token/s): 43.09, #queue-req: 0,
[2026-01-05 16:51:58] Decode batch, #running-req: 2, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.70, #queue-req: 0,
[2026-01-05 16:52:00] Decode batch, #running-req: 2, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.74, #queue-req: 0,
[2026-01-05 16:52:01] Decode batch, #running-req: 2, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.97, #queue-req: 0,
[2026-01-05 16:52:03] Decode batch, #running-req: 2, #token: 3328, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.24, #queue-req: 0,
[2026-01-05 16:52:05] Decode batch, #running-req: 2, #token: 3328, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.19, #queue-req: 0,
[2026-01-05 16:52:07] Decode batch, #running-req: 2, #token: 3328, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.28, #queue-req: 0,
[2026-01-05 16:52:09] Decode batch, #running-req: 2, #token: 3584, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.46, #queue-req: 0,
[2026-01-05 16:52:11] Decode batch, #running-req: 2, #token: 3584, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.11, #queue-req: 0,
[2026-01-05 16:52:13] Decode batch, #running-req: 2, #token: 3584, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.40, #queue-req: 0,
[2026-01-05 16:52:15] Decode batch, #running-req: 2, #token: 3840, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.73, #queue-req: 0,
[2026-01-05 16:52:17] Decode batch, #running-req: 2, #token: 3840, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.55, #queue-req: 0,
[2026-01-05 16:52:19] Decode batch, #running-req: 2, #token: 3840, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.64, #queue-req: 0,
[2026-01-05 16:52:21] Decode batch, #running-req: 2, #token: 3968, token usage: 0.01, cpu graph: False, gen throughput (token/s): 42.46, #queue-req: 0,
[2026-01-05 16:52:22] Decode batch, #running-req: 2, #token: 4096, token usage: 0.02, cpu graph: False, gen throughput (token/s): 43.47, #queue-req: 0,
[2026-01-05 16:52:24] Decode batch, #running-req: 2, #token: 4096, token usage: 0.02, cpu graph: False, gen throughput (token/s): 44.01, #queue-req: 0,
[2026-01-05 16:52:26] Decode batch, #running-req: 2, #token: 4096, token usage: 0.02, cpu graph: False, gen throughput (token/s): 44.11, #queue-req: 0,
[2026-01-05 16:52:28] Decode batch, #running-req: 2, #token: 4352, token usage: 0.02, cpu graph: False, gen throughput (token/s): 44.15, #queue-req: 0,
[2026-01-05 16:52:30] Decode batch, #running-req: 2, #token: 4352, token usage: 0.02, cpu graph: False, gen throughput (token/s): 44.24, #queue-req: 0,
[2026-01-05 16:52:32] Decode batch, #running-req: 2, #token: 4352, token usage: 0.02, cpu graph: False, gen throughput (token/s): 43.24, #queue-req: 0,
[2026-01-05 16:52:32] INFO:     127.0.0.1:55704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:52:32] INFO:     127.0.0.1:55736 - "POST /v1/chat/completions HTTP/1.1" 200 OK

  3%|▎         | 2/64 [02:05<1:10:52, 68.58s/it]
100%|██████████| 64/64 [02:05<00:00,  1.96s/it]
.
----------------------------------------------------------------------
Ran 2 tests in 209.840s

OK
Total latency: 125.378 s
Score: 0.797
Writing report to /tmp/mmlu__root_.cache_modelscope_hub_models_AI-ModelScope_Llama-3.1-8B-Instruct.html
{'other': np.float64(0.8125), 'other:std': np.float64(0.3903123748998999), 'score:std': np.float64(0.40232478717449166), 'stem': np.float64(0.8181818181818182), 'stem:std': np.float64(0.385694607919935), 'humanities': np.float64(0.782608695652174), 'humanities:std': np.float64(0.41247099915239727), 'social_sciences': np.float64(0.7857142857142857), 'social_sciences:std': np.float64(0.41032590332414504), 'score': np.float64(0.796875)}
Writing results to /tmp/mmlu__root_.cache_modelscope_hub_models_AI-ModelScope_Llama-3.1-8B-Instruct.json
.
.
End (10/106):
filename='ascend/function/test_pytorch_sampling_backend.py', elapsed=229, estimated_time=400
.
.

.
.
Begin (11/106):
python3 /data/l30079981/260105/ascend/function/test_reasoning_content.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:53:10] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.791, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=797232671, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-1234', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='deepseek-r1', tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:53:11] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:53:30] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:53:31] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:53:31] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:53:32] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:53:32] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:53:33] Load weight begin. avail mem=60.53 GB
[2026-01-05 16:53:34] Found local HF snapshot for deepseek-ai/DeepSeek-R1-Distill-Qwen-7B at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60; skipping download.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.15s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.37s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]

[2026-01-05 16:53:37] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=46.20 GB, mem usage=14.34 GB.
[2026-01-05 16:53:37] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:53:37] The available memory for KV cache is 33.51 GB.
[2026-01-05 16:53:37] KV Cache is allocated. #tokens: 627456, K size: 16.76 GB, V size: 16.76 GB
[2026-01-05 16:53:37] Memory pool end. avail mem=11.45 GB
[2026-01-05 16:53:38] Capture cuda graph begin. This can take up to several minutes. avail mem=11.45 GB
[2026-01-05 16:53:38] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]

  0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=11.40 GB):   0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=11.40 GB):   8%|▊         | 1/12 [00:01<00:15,  1.44s/it]
Capturing batches (bs=56 avail_mem=11.25 GB):   8%|▊         | 1/12 [00:01<00:15,  1.44s/it]
Capturing batches (bs=56 avail_mem=11.25 GB):  17%|█▋        | 2/12 [00:01<00:06,  1.45it/s]
Capturing batches (bs=48 avail_mem=11.23 GB):  17%|█▋        | 2/12 [00:01<00:06,  1.45it/s]
Capturing batches (bs=48 avail_mem=11.23 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.22it/s]
Capturing batches (bs=40 avail_mem=11.23 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.22it/s]
Capturing batches (bs=40 avail_mem=11.23 GB):  33%|███▎      | 4/12 [00:01<00:02,  2.93it/s]
Capturing batches (bs=32 avail_mem=11.23 GB):  33%|███▎      | 4/12 [00:01<00:02,  2.93it/s]
Capturing batches (bs=32 avail_mem=11.23 GB):  42%|████▏     | 5/12 [00:02<00:01,  3.59it/s]
Capturing batches (bs=24 avail_mem=11.23 GB):  42%|████▏     | 5/12 [00:02<00:01,  3.59it/s]
Capturing batches (bs=24 avail_mem=11.23 GB):  50%|█████     | 6/12 [00:02<00:01,  4.14it/s]
Capturing batches (bs=16 avail_mem=11.23 GB):  50%|█████     | 6/12 [00:02<00:01,  4.14it/s]
Capturing batches (bs=16 avail_mem=11.23 GB):  58%|█████▊    | 7/12 [00:02<00:01,  4.57it/s]
Capturing batches (bs=12 avail_mem=11.22 GB):  58%|█████▊    | 7/12 [00:02<00:01,  4.57it/s]
Capturing batches (bs=12 avail_mem=11.22 GB):  67%|██████▋   | 8/12 [00:02<00:00,  4.90it/s]
Capturing batches (bs=8 avail_mem=11.22 GB):  67%|██████▋   | 8/12 [00:02<00:00,  4.90it/s]
Capturing batches (bs=8 avail_mem=11.22 GB):  75%|███████▌  | 9/12 [00:02<00:00,  5.16it/s]
Capturing batches (bs=4 avail_mem=11.22 GB):  75%|███████▌  | 9/12 [00:02<00:00,  5.16it/s]
Capturing batches (bs=4 avail_mem=11.22 GB):  83%|████████▎ | 10/12 [00:02<00:00,  5.36it/s]
Capturing batches (bs=2 avail_mem=11.22 GB):  83%|████████▎ | 10/12 [00:02<00:00,  5.36it/s]
Capturing batches (bs=2 avail_mem=11.22 GB):  92%|█████████▏| 11/12 [00:03<00:00,  5.50it/s]
Capturing batches (bs=1 avail_mem=11.17 GB):  92%|█████████▏| 11/12 [00:03<00:00,  5.50it/s]
Capturing batches (bs=1 avail_mem=11.17 GB): 100%|██████████| 12/12 [00:03<00:00,  5.64it/s]
Capturing batches (bs=1 avail_mem=11.17 GB): 100%|██████████| 12/12 [00:03<00:00,  3.63it/s]
[2026-01-05 16:53:43] Capture cuda graph end. Time elapsed: 5.09 s. mem usage=0.28 GB. avail mem=11.17 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:53:44] max_total_num_tokens=627456, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2451, context_len=131072, available_gpu_mem=11.17 GB
[2026-01-05 16:53:45] INFO:     Started server process [83405]
[2026-01-05 16:53:45] INFO:     Waiting for application startup.
[2026-01-05 16:53:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-05 16:53:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-05 16:53:45] INFO:     Application startup complete.
[2026-01-05 16:53:45] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:53:46] INFO:     127.0.0.1:33200 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 16:53:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:53:46.313655654 compiler_depend.ts:198] Warning: Driver Version: "߫" is invalid or not supported yet. (function operator())
[2026-01-05 16:53:51] INFO:     127.0.0.1:33204 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:54:01] INFO:     127.0.0.1:43442 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:54:08] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:54:08] The server is fired up and ready to roll!
[2026-01-05 16:54:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:12] INFO:     127.0.0.1:53060 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 16:54:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:14] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 0.87, #queue-req: 0,
[2026-01-05 16:54:15] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 46.57, #queue-req: 0,
[2026-01-05 16:54:15] INFO:     127.0.0.1:53062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
.[2026-01-05 16:54:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:16] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 40.21, #queue-req: 0,
[2026-01-05 16:54:16] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.27, #queue-req: 0,
[2026-01-05 16:54:17] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cpu graph: True, gen throughput (token/s): 49.14, #queue-req: 0,
[2026-01-05 16:54:17] INFO:     127.0.0.1:53064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
.[2026-01-05 16:54:17] INFO:     127.0.0.1:56282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:54:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:18] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 40.68, #queue-req: 0,
[2026-01-05 16:54:19] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.08, #queue-req: 0,
.[2026-01-05 16:54:19] INFO:     127.0.0.1:56284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:54:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:20] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 41.31, #queue-req: 0,
[2026-01-05 16:54:21] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 47.89, #queue-req: 0,
[2026-01-05 16:54:22] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.91, #queue-req: 0,
.[2026-01-05 16:54:22] INFO:     127.0.0.1:56286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:54:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:54:23] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 40.35, #queue-req: 0,
[2026-01-05 16:54:23] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 47.90, #queue-req: 0,
./usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:54:43] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.791, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=594709881, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-1234', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:54:44] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:55:02] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:55:04] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:55:04] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:55:05] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:55:05] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:55:06] Load weight begin. avail mem=60.53 GB
[2026-01-05 16:55:07] Found local HF snapshot for deepseek-ai/DeepSeek-R1-Distill-Qwen-7B at /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60; skipping download.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.03s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]

[2026-01-05 16:55:10] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=46.20 GB, mem usage=14.34 GB.
[2026-01-05 16:55:10] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:55:10] The available memory for KV cache is 33.51 GB.
[2026-01-05 16:55:10] KV Cache is allocated. #tokens: 627456, K size: 16.76 GB, V size: 16.76 GB
[2026-01-05 16:55:10] Memory pool end. avail mem=11.45 GB
[2026-01-05 16:55:10] Capture cuda graph begin. This can take up to several minutes. avail mem=11.45 GB
[2026-01-05 16:55:10] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]

  0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=11.40 GB):   0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=11.40 GB):   8%|▊         | 1/12 [00:01<00:17,  1.57s/it]
Capturing batches (bs=56 avail_mem=11.25 GB):   8%|▊         | 1/12 [00:01<00:17,  1.57s/it]
Capturing batches (bs=56 avail_mem=11.25 GB):  17%|█▋        | 2/12 [00:01<00:07,  1.33it/s]
Capturing batches (bs=48 avail_mem=11.23 GB):  17%|█▋        | 2/12 [00:01<00:07,  1.33it/s]
Capturing batches (bs=48 avail_mem=11.23 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.05it/s]
Capturing batches (bs=40 avail_mem=11.23 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.05it/s]
Capturing batches (bs=40 avail_mem=11.23 GB):  33%|███▎      | 4/12 [00:02<00:02,  2.73it/s]
Capturing batches (bs=32 avail_mem=11.23 GB):  33%|███▎      | 4/12 [00:02<00:02,  2.73it/s]
Capturing batches (bs=32 avail_mem=11.23 GB):  42%|████▏     | 5/12 [00:02<00:02,  3.37it/s]
Capturing batches (bs=24 avail_mem=11.23 GB):  42%|████▏     | 5/12 [00:02<00:02,  3.37it/s]
Capturing batches (bs=24 avail_mem=11.23 GB):  50%|█████     | 6/12 [00:02<00:01,  3.92it/s]
Capturing batches (bs=16 avail_mem=11.23 GB):  50%|█████     | 6/12 [00:02<00:01,  3.92it/s]
Capturing batches (bs=16 avail_mem=11.23 GB):  58%|█████▊    | 7/12 [00:02<00:01,  4.37it/s]
Capturing batches (bs=12 avail_mem=11.22 GB):  58%|█████▊    | 7/12 [00:02<00:01,  4.37it/s]
Capturing batches (bs=12 avail_mem=11.22 GB):  67%|██████▋   | 8/12 [00:02<00:00,  4.74it/s]
Capturing batches (bs=8 avail_mem=11.22 GB):  67%|██████▋   | 8/12 [00:02<00:00,  4.74it/s]
Capturing batches (bs=8 avail_mem=11.22 GB):  75%|███████▌  | 9/12 [00:02<00:00,  5.01it/s]
Capturing batches (bs=4 avail_mem=11.22 GB):  75%|███████▌  | 9/12 [00:02<00:00,  5.01it/s]
Capturing batches (bs=4 avail_mem=11.22 GB):  83%|████████▎ | 10/12 [00:03<00:00,  5.21it/s]
Capturing batches (bs=2 avail_mem=11.22 GB):  83%|████████▎ | 10/12 [00:03<00:00,  5.21it/s]
Capturing batches (bs=2 avail_mem=11.22 GB):  92%|█████████▏| 11/12 [00:03<00:00,  5.37it/s]
Capturing batches (bs=1 avail_mem=11.17 GB):  92%|█████████▏| 11/12 [00:03<00:00,  5.37it/s]
Capturing batches (bs=1 avail_mem=11.17 GB): 100%|██████████| 12/12 [00:03<00:00,  5.53it/s]
Capturing batches (bs=1 avail_mem=11.17 GB): 100%|██████████| 12/12 [00:03<00:00,  3.44it/s]
[2026-01-05 16:55:16] Capture cuda graph end. Time elapsed: 5.28 s. mem usage=0.28 GB. avail mem=11.17 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:55:17] max_total_num_tokens=627456, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2451, context_len=131072, available_gpu_mem=11.17 GB
[2026-01-05 16:55:18] INFO:     Started server process [86065]
[2026-01-05 16:55:18] INFO:     Waiting for application startup.
[2026-01-05 16:55:18] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-05 16:55:18] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-05 16:55:18] INFO:     Application startup complete.
[2026-01-05 16:55:18] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:55:19] INFO:     127.0.0.1:50838 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 16:55:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:55:19.129416942 compiler_depend.ts:198] Warning: Driver Version: "|" is invalid or not supported yet. (function operator())
[2026-01-05 16:55:24] INFO:     127.0.0.1:50842 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:55:34] INFO:     127.0.0.1:33856 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:55:41] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 16:55:41] The server is fired up and ready to roll!
[2026-01-05 16:55:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:45] INFO:     127.0.0.1:33702 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 16:55:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:46] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 0.89, #queue-req: 0,
[2026-01-05 16:55:47] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 47.14, #queue-req: 0,
[2026-01-05 16:55:47] INFO:     127.0.0.1:33704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
.[2026-01-05 16:55:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:48] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 41.44, #queue-req: 0,
[2026-01-05 16:55:49] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.38, #queue-req: 0,
[2026-01-05 16:55:49] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cpu graph: True, gen throughput (token/s): 47.74, #queue-req: 0,
[2026-01-05 16:55:49] INFO:     127.0.0.1:49026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
.[2026-01-05 16:55:49] INFO:     127.0.0.1:49028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:55:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:50] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 41.37, #queue-req: 0,
[2026-01-05 16:55:51] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.46, #queue-req: 0,
.[2026-01-05 16:55:52] INFO:     127.0.0.1:49030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:55:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:52] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 41.83, #queue-req: 0,
[2026-01-05 16:55:53] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.36, #queue-req: 0,
[2026-01-05 16:55:54] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cpu graph: True, gen throughput (token/s): 48.52, #queue-req: 0,
.[2026-01-05 16:55:54] INFO:     127.0.0.1:49032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 16:55:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 16:55:55] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 41.48, #queue-req: 0,
[2026-01-05 16:55:56] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 47.67, #queue-req: 0,
.
----------------------------------------------------------------------
Ran 10 tests in 184.658s

OK
command=python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --reasoning-parser deepseek-r1 --device npu --host 127.0.0.1 --port 21000 --api-key sk-1234
[CI Test Method] TestReasoningContentAPI.test_nonstreaming_separate_reasoning_false
[CI Test Method] TestReasoningContentAPI.test_nonstreaming_separate_reasoning_true
[CI Test Method] TestReasoningContentAPI.test_streaming_separate_reasoning_false
[CI Test Method] TestReasoningContentAPI.test_streaming_separate_reasoning_true
[CI Test Method] TestReasoningContentAPI.test_streaming_separate_reasoning_true_stream_reasoning_false
command=python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --device npu --host 127.0.0.1 --port 21000 --api-key sk-1234
[CI Test Method] TestReasoningContentWithoutParser.test_nonstreaming_separate_reasoning_false
[CI Test Method] TestReasoningContentWithoutParser.test_nonstreaming_separate_reasoning_true
[CI Test Method] TestReasoningContentWithoutParser.test_streaming_separate_reasoning_false
[CI Test Method] TestReasoningContentWithoutParser.test_streaming_separate_reasoning_true
[CI Test Method] TestReasoningContentWithoutParser.test_streaming_separate_reasoning_true_stream_reasoning_false
.
.
End (11/106):
filename='ascend/function/test_reasoning_content.py', elapsed=204, estimated_time=400
.
.

.
.
Begin (12/106):
python3 /data/l30079981/260105/ascend/function/test_fp8_kvcache.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-16:56:34 (PID:88834, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestFp8KvcacheLlama)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_fp8_kvcache.py", line 31, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 30.011s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/meta-llama/Meta-Llama-3.1-8B-Instruct --kv-cache-dtype fp8_e4m3 --quantization-param-path /data/l30079981/260105/ascend/function/kv_cache_scales_llama3_8b.json --device npu --host 127.0.0.1 --port 21000
.
.
End (12/106):
filename='ascend/function/test_fp8_kvcache.py', elapsed=49, estimated_time=400
.
.


[CI Retry] ascend/function/test_fp8_kvcache.py failed with unknown failure type - not retrying


✗ FAILED: ascend/function/test_fp8_kvcache.py returned exit code 1

.
.
Begin (13/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_eplb_algorithm.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:57:22] WARNING model_config.py:814: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:22] WARNING server_args.py:1830: Cuda graph is disabled because deepep_mode=`normal`
[2026-01-05 16:57:22] WARNING server_args.py:1833: DeepEP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[16].
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:22] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=147728198, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=16, moe_a2a_backend='deepep', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='normal', ep_num_redundant_experts=16, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=True, enable_async_eplb=False, eplb_algorithm='dynamic', eplb_rebalance_num_iterations=50, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode='stat', expert_distribution_recorder_buffer_size=50, enable_expert_distribution_metrics=True, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:57:22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
[2026-01-05 16:57:23] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:39 TP5 EP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:39 TP2 EP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:57:39 TP0 EP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP4 EP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 16:57:40 TP12 EP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:57:40 TP5 EP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:40 TP5 EP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP6 EP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP2 EP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:40 TP2 EP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP0 EP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:40 TP0 EP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP9 EP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:40 TP1 EP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP10 EP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP4 EP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP4 EP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP12 EP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP12 EP12] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP8 EP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP13 EP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:41 TP14 EP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP7 EP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:57:41 TP6 EP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP6 EP6] Init torch distributed begin.
[2026-01-05 16:57:41 TP15 EP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:41 TP3 EP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP11 EP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:41 TP9 EP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP9 EP9] Init torch distributed begin.
[2026-01-05 16:57:41 TP1 EP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:41 TP1 EP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:42 TP10 EP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP10 EP10] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:42 TP8 EP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP8 EP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:42 TP14 EP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP14 EP14] Init torch distributed begin.
[2026-01-05 16:57:42 TP13 EP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP13 EP13] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:57:42 TP7 EP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP7 EP7] Init torch distributed begin.
[2026-01-05 16:57:42 TP15 EP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP15 EP15] Init torch distributed begin.
[2026-01-05 16:57:42 TP12 EP12] Context: self.device='npu' self.gpu_id=12 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=12 self.tp_size=16
[2026-01-05 16:57:42 TP12 EP12] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 317, in __init__
    self.init_model_worker()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 455, in init_model_worker
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 253, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 368, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 744, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/npu/utils.py", line 91, in set_device
    torch_npu._C._npu_setDevice(device_id)
RuntimeError: Initialize:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:159 NPU function error: c10_npu::SetDevice(device_id), error code is 107001
[ERROR] 2026-01-05-16:57:42 (PID:89769, Device:0, RankID:-1) ERR00100 PTA call acl api failed
[Error]: Invalid device ID.
        Check whether the device ID is valid.
[PID: 89769] 2026-01-05-16:57:42.294.058 Invalid_Argument(EE1001): The argument is invalid.Reason: Set device failed, invalid device, set drv device=12, valid device range is [0, 8)
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        rtSetDevice execute failed, reason=[device id error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
        open device 12 failed, runtime result = 107001.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:162]


[2026-01-05 16:57:42] Received sigquit from a child process. It usually means the child failed.
[2026-01-05 16:57:42 TP3 EP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP3 EP3] Init torch distributed begin.
[2026-01-05 16:57:42 TP11 EP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:57:42 TP11 EP11] Init torch distributed begin.
E/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:58:02] WARNING model_config.py:814: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:02] WARNING server_args.py:1830: Cuda graph is disabled because deepep_mode=`normal`
[2026-01-05 16:58:02] WARNING server_args.py:1833: DeepEP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[16].
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:03] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=400678693, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=16, moe_a2a_backend='deepep', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='normal', ep_num_redundant_experts=16, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=True, enable_async_eplb=False, eplb_algorithm='static', eplb_rebalance_num_iterations=50, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode='stat', expert_distribution_recorder_buffer_size=50, enable_expert_distribution_metrics=True, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:58:03] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
[2026-01-05 16:58:04] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/environ.py:416: UserWarning: Environment variable SGL_ENABLE_JIT_DEEPGEMM is deprecated, please use SGLANG_ENABLE_JIT_DEEPGEMM
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 16:58:20 TP2 EP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:58:21 TP6 EP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:21 TP8 EP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 16:58:21 TP2 EP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:21 TP2 EP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 16:58:21 TP0 EP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 16:58:21 TP10 EP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:21 TP14 EP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:58:21 TP11 EP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:58:21 TP9 EP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:58:22 TP6 EP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP6 EP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:22 TP7 EP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP8 EP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP8 EP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:22 TP15 EP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:22 TP4 EP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP5 EP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP0 EP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP0 EP0] Init torch distributed begin.
[2026-01-05 16:58:22 TP3 EP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:58:22 TP12 EP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:58:22 TP1 EP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP14 EP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP14 EP14] Init torch distributed begin.
[2026-01-05 16:58:22 TP10 EP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP10 EP10] Init torch distributed begin.
[2026-01-05 16:58:22 TP13 EP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:58:22 TP11 EP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP11 EP11] Init torch distributed begin.
[2026-01-05 16:58:22 TP9 EP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:22 TP9 EP9] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:58:23 TP7 EP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP7 EP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:58:23 TP15 EP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP15 EP15] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:58:23 TP4 EP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP4 EP4] Init torch distributed begin.
[2026-01-05 16:58:23 TP5 EP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP5 EP5] Init torch distributed begin.
[2026-01-05 16:58:23 TP3 EP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP3 EP3] Init torch distributed begin.
[2026-01-05 16:58:23 TP12 EP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP12 EP12] Init torch distributed begin.
[2026-01-05 16:58:23 TP1 EP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP1 EP1] Init torch distributed begin.
[2026-01-05 16:58:23 TP13 EP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-05 16:58:23 TP13 EP13] Init torch distributed begin.
[2026-01-05 16:58:23 TP8 EP8] Context: self.device='npu' self.gpu_id=8 os.environ.get('CUDA_VISIBLE_DEVICES')=None self.tp_rank=8 self.tp_size=16
[2026-01-05 16:58:23 TP8 EP8] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 317, in __init__
    self.init_model_worker()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 455, in init_model_worker
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 253, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 368, in __init__
    min_per_gpu_memory = self.init_torch_distributed()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 744, in init_torch_distributed
    torch.get_device_module(self.device).set_device(self.gpu_id)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/npu/utils.py", line 91, in set_device
    torch_npu._C._npu_setDevice(device_id)
RuntimeError: Initialize:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:159 NPU function error: c10_npu::SetDevice(device_id), error code is 107001
[ERROR] 2026-01-05-16:58:23 (PID:95247, Device:0, RankID:-1) ERR00100 PTA call acl api failed
[Error]: Invalid device ID.
        Check whether the device ID is valid.
[PID: 95247] 2026-01-05-16:58:23.315.985 Invalid_Argument(EE1001): The argument is invalid.Reason: Set device failed, invalid device, set drv device=8, valid device range is [0, 8)
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        rtSetDevice execute failed, reason=[device id error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
        open device 8 failed, runtime result = 107001.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:162]


[2026-01-05 16:58:23] Received sigquit from a child process. It usually means the child failed.
E
======================================================================
ERROR: setUpClass (__main__.TestEplbAlgorithm)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_ascend_eplb_algorithm.py", line 32, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.

======================================================================
ERROR: setUpClass (__main__.TestEplbAlgorithmStatic)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_ascend_eplb_algorithm.py", line 32, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 80.027s

FAILED (errors=2)
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8 --trust-remote-code --tp-size 16 --dp-size 1 --attention-backend ascend --quantization modelslim --mem-fraction-static 0.9 --enable-dp-attention --moe-a2a-backend deepep --deepep-mode normal --disable-cuda-graph --enable-eplb --ep-num-redundant-experts 16 --eplb-rebalance-num-iterations 50 --expert-distribution-recorder-buffer-size 50 --enable-expert-distribution-metrics --expert-distribution-recorder-mode stat --ep-dispatch-algorithm static --eplb-algorithm dynamic --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-R1-W8A8 --trust-remote-code --tp-size 16 --dp-size 1 --attention-backend ascend --quantization modelslim --mem-fraction-static 0.9 --enable-dp-attention --moe-a2a-backend deepep --deepep-mode normal --disable-cuda-graph --enable-eplb --ep-num-redundant-experts 16 --eplb-rebalance-num-iterations 50 --expert-distribution-recorder-buffer-size 50 --enable-expert-distribution-metrics --expert-distribution-recorder-mode stat --ep-dispatch-algorithm static --eplb-algorithm static --device npu --host 127.0.0.1 --port 21000
.
.
End (13/106):
filename='ascend/function/test_ascend_eplb_algorithm.py', elapsed=99, estimated_time=400
.
.


[CI Retry] ascend/function/test_ascend_eplb_algorithm.py failed with non-retriable error: RuntimeError - not retrying


✗ FAILED: ascend/function/test_ascend_eplb_algorithm.py returned exit code 1

.
.
Begin (14/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_debug_tensor_dump_output_folder.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 16:59:01] WARNING server_args.py:2450: Cuda graph and server warmup are disabled because of using tensor dump mode
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:59:02] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=True, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.791, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=397745805, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder='./', debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 16:59:03] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 16:59:19] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 16:59:21] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:59:21] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:59:22] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2026-01-05 16:59:22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 16:59:23] Load weight begin. avail mem=60.53 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.38it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.37it/s]

[2026-01-05 16:59:24] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=58.07 GB, mem usage=2.46 GB.
[2026-01-05 16:59:24] Using KV cache dtype: torch.bfloat16
[2026-01-05 16:59:24] The available memory for KV cache is 45.40 GB.
[2026-01-05 16:59:24] KV Cache is allocated. #tokens: 1487744, K size: 22.70 GB, V size: 22.70 GB
[2026-01-05 16:59:24] Memory pool end. avail mem=10.64 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 16:59:25] max_total_num_tokens=1487744, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=131072, available_gpu_mem=10.64 GB
[2026-01-05 16:59:27] INFO:     Started server process [100696]
[2026-01-05 16:59:27] INFO:     Waiting for application startup.
[2026-01-05 16:59:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:59:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 16:59:27] The server is fired up and ready to roll!
[2026-01-05 16:59:27] INFO:     Application startup complete.
[2026-01-05 16:59:27] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 16:59:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 16:59:33.536546519 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 16:59:54] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:59:33. last_heartbeat time: 08:00:00
[2026-01-05 16:59:54] INFO:     127.0.0.1:37968 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 16:59:56] Dump 00000th pass to TP0_PP0_Rank0_pid101005/Pass00000.pt
[2026-01-05 16:59:56] Dump 00001th pass to TP0_PP0_Rank0_pid101005/Pass00001.pt
[2026-01-05 16:59:57] Received output for rid='HEALTH_CHECK_1767603573.8291647' but the state was deleted in TokenizerManager.
[2026-01-05 17:00:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 17:00:04] Dump 00002th pass to TP0_PP0_Rank0_pid101005/Pass00002.pt
[2026-01-05 17:00:04] Dump 00003th pass to TP0_PP0_Rank0_pid101005/Pass00003.pt
[2026-01-05 17:00:05] INFO:     127.0.0.1:41864 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 17:00:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 17:00:05] Dump 00004th pass to TP0_PP0_Rank0_pid101005/Pass00004.pt
[2026-01-05 17:00:05] Dump 00005th pass to TP0_PP0_Rank0_pid101005/Pass00005.pt
[2026-01-05 17:00:06] INFO:     127.0.0.1:41866 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 17:00:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
.[2026-01-05 17:00:07] Dump 00006th pass to TP0_PP0_Rank0_pid101005/Pass00006.pt
[2026-01-05 17:00:07] Dump 00007th pass to TP0_PP0_Rank0_pid101005/Pass00007.pt
[2026-01-05 17:00:07] Dump 00008th pass to TP0_PP0_Rank0_pid101005/Pass00008.pt
[2026-01-05 17:00:07] Dump 00009th pass to TP0_PP0_Rank0_pid101005/Pass00009.pt
[2026-01-05 17:00:07] Dump 00010th pass to TP0_PP0_Rank0_pid101005/Pass00010.pt
[2026-01-05 17:00:07] Dump 00011th pass to TP0_PP0_Rank0_pid101005/Pass00011.pt
[2026-01-05 17:00:08] Dump 00012th pass to TP0_PP0_Rank0_pid101005/Pass00012.pt
[2026-01-05 17:00:08] Dump 00013th pass to TP0_PP0_Rank0_pid101005/Pass00013.pt
[2026-01-05 17:00:08] Dump 00014th pass to TP0_PP0_Rank0_pid101005/Pass00014.pt
[2026-01-05 17:00:08] Dump 00015th pass to TP0_PP0_Rank0_pid101005/Pass00015.pt
[2026-01-05 17:00:08] Dump 00016th pass to TP0_PP0_Rank0_pid101005/Pass00016.pt
[2026-01-05 17:00:08] Dump 00017th pass to TP0_PP0_Rank0_pid101005/Pass00017.pt
[2026-01-05 17:00:08] Dump 00018th pass to TP0_PP0_Rank0_pid101005/Pass00018.pt
[2026-01-05 17:00:08] Dump 00019th pass to TP0_PP0_Rank0_pid101005/Pass00019.pt
[2026-01-05 17:00:08] Dump 00020th pass to TP0_PP0_Rank0_pid101005/Pass00020.pt
[2026-01-05 17:00:08] Dump 00021th pass to TP0_PP0_Rank0_pid101005/Pass00021.pt
[2026-01-05 17:00:08] Dump 00022th pass to TP0_PP0_Rank0_pid101005/Pass00022.pt
[2026-01-05 17:00:08] Dump 00023th pass to TP0_PP0_Rank0_pid101005/Pass00023.pt
[2026-01-05 17:00:09] Dump 00024th pass to TP0_PP0_Rank0_pid101005/Pass00024.pt
[2026-01-05 17:00:09] Dump 00025th pass to TP0_PP0_Rank0_pid101005/Pass00025.pt
[2026-01-05 17:00:09] Dump 00026th pass to TP0_PP0_Rank0_pid101005/Pass00026.pt
[2026-01-05 17:00:09] Dump 00027th pass to TP0_PP0_Rank0_pid101005/Pass00027.pt
[2026-01-05 17:00:09] Dump 00028th pass to TP0_PP0_Rank0_pid101005/Pass00028.pt
[2026-01-05 17:00:09] Dump 00029th pass to TP0_PP0_Rank0_pid101005/Pass00029.pt
[2026-01-05 17:00:09] Dump 00030th pass to TP0_PP0_Rank0_pid101005/Pass00030.pt
[2026-01-05 17:00:09] Dump 00031th pass to TP0_PP0_Rank0_pid101005/Pass00031.pt
[2026-01-05 17:00:09] Dump 00032th pass to TP0_PP0_Rank0_pid101005/Pass00032.pt
[2026-01-05 17:00:09] Dump 00033th pass to TP0_PP0_Rank0_pid101005/Pass00033.pt
[2026-01-05 17:00:09] Dump 00034th pass to TP0_PP0_Rank0_pid101005/Pass00034.pt
[2026-01-05 17:00:09] Dump 00035th pass to TP0_PP0_Rank0_pid101005/Pass00035.pt
[2026-01-05 17:00:09] Dump 00036th pass to TP0_PP0_Rank0_pid101005/Pass00036.pt
[2026-01-05 17:00:10] Dump 00037th pass to TP0_PP0_Rank0_pid101005/Pass00037.pt
[2026-01-05 17:00:10] Dump 00038th pass to TP0_PP0_Rank0_pid101005/Pass00038.pt
[2026-01-05 17:00:10] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_ascend_debug_tensor_dump_output_folder.py", line 52, in test_debug_tensor_dump_output_folder
    self.assertTrue(os.path.exists("./" + "pytorch_dump_logits.npy"))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 715, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true
/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 100696 is still running
  _warn("subprocess %s is still running" % self.pid,
E
======================================================================
ERROR: test_debug_tensor_dump_output_folder (__main__.TestDebugTensorDumpOutputFolder.test_debug_tensor_dump_output_folder)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: False is not true

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 86.345s

FAILED (errors=1)
[CI Test Method] TestDebugTensorDumpOutputFolder.test_debug_tensor_dump_output_folder
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct --debug-tensor-dump-output-folder ./ --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000

✗ TIMEOUT: ascend/function/test_ascend_debug_tensor_dump_output_folder.py after 4200 seconds

.
.
Begin (15/106):
python3 /data/l30079981/260105/ascend/function/test_large_max_new_tokens.py
.
.

Traceback (most recent call last):
  File "/data/l30079981/260105/ascend/function/test_large_max_new_tokens.py", line 16, in <module>
    from fixtures import popen_launch_workers_and_router
ModuleNotFoundError: No module named 'fixtures'
.
.
End (15/106):
filename='ascend/function/test_large_max_new_tokens.py', elapsed=1, estimated_time=400
.
.


[CI Retry] ascend/function/test_large_max_new_tokens.py failed with non-retriable error: ModuleNotFoundError - not retrying


✗ FAILED: ascend/function/test_large_max_new_tokens.py returned exit code 1

.
.
Begin (16/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_pp_single_node.py
.
.

[2026-01-05 18:08:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:08:49] Dump 00039th pass to TP0_PP0_Rank0_pid101005/Pass00039.pt
[2026-01-05 18:08:49] Dump 00040th pass to TP0_PP0_Rank0_pid101005/Pass00040.pt
[2026-01-05 18:08:50] INFO:     127.0.0.1:33872 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 18:08:50] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 18:08:50] INFO:     127.0.0.1:33874 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 18:08:50] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:08:50] Dump 00041th pass to TP0_PP0_Rank0_pid101005/Pass00041.pt
[2026-01-05 18:08:53] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B --chunked-prefill-size 256 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwenPPTieWeightsAccuracy.test_gsm8k

[2026-01-05 18:08:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:08:53] Dump 00042th pass to TP0_PP0_Rank0_pid101005/Pass00042.pt
[2026-01-05 18:08:54] Prefill batch, #new-seq: 60, #new-token: 8192, #cached-token: 38400, token usage: 0.00, #running-req: 1, #queue-req: 67,
[2026-01-05 18:08:56] Dump 00043th pass to TP0_PP0_Rank0_pid101005/Pass00043.pt
  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-18:09:08 (PID:103826, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
[2026-01-05 18:09:13] Prefill batch, #new-seq: 63, #new-token: 8192, #cached-token: 40320, token usage: 0.01, #running-req: 61, #queue-req: 4,
[2026-01-05 18:09:17] Dump 00044th pass to TP0_PP0_Rank0_pid101005/Pass00044.pt
[2026-01-05 18:09:50] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.01, #running-req: 124, #queue-req: 0,
[2026-01-05 18:09:50] Dump 00045th pass to TP0_PP0_Rank0_pid101005/Pass00045.pt
[2026-01-05 18:09:53] Dump 00046th pass to TP0_PP0_Rank0_pid101005/Pass00046.pt
[2026-01-05 18:09:54] Dump 00047th pass to TP0_PP0_Rank0_pid101005/Pass00047.pt
[2026-01-05 18:09:55] Dump 00048th pass to TP0_PP0_Rank0_pid101005/Pass00048.pt
[2026-01-05 18:09:56] Dump 00049th pass to TP0_PP0_Rank0_pid101005/Pass00049.pt
[2026-01-05 18:09:57] Dump 00050th pass to TP0_PP0_Rank0_pid101005/Pass00050.pt
[2026-01-05 18:09:58] Decode batch, #running-req: 128, #token: 17920, token usage: 0.01, cpu graph: False, gen throughput (token/s): 0.13, #queue-req: 0,
[2026-01-05 18:09:58] Dump 00051th pass to TP0_PP0_Rank0_pid101005/Pass00051.pt
[2026-01-05 18:10:00] Dump 00052th pass to TP0_PP0_Rank0_pid101005/Pass00052.pt
[2026-01-05 18:10:01] Dump 00053th pass to TP0_PP0_Rank0_pid101005/Pass00053.pt
[2026-01-05 18:10:01] Dump 00054th pass to TP0_PP0_Rank0_pid101005/Pass00054.pt
[2026-01-05 18:10:02] Dump 00055th pass to TP0_PP0_Rank0_pid101005/Pass00055.pt
[2026-01-05 18:10:03] Dump 00056th pass to TP0_PP0_Rank0_pid101005/Pass00056.pt
[2026-01-05 18:10:05] Dump 00057th pass to TP0_PP0_Rank0_pid101005/Pass00057.pt
[2026-01-05 18:10:06] Dump 00058th pass to TP0_PP0_Rank0_pid101005/Pass00058.pt
[2026-01-05 18:10:07] Dump 00059th pass to TP0_PP0_Rank0_pid101005/Pass00059.pt
[2026-01-05 18:10:08] Dump 00060th pass to TP0_PP0_Rank0_pid101005/Pass00060.pt
[2026-01-05 18:10:09] Dump 00061th pass to TP0_PP0_Rank0_pid101005/Pass00061.pt
[2026-01-05 18:10:10] Dump 00062th pass to TP0_PP0_Rank0_pid101005/Pass00062.pt
[2026-01-05 18:10:11] Dump 00063th pass to TP0_PP0_Rank0_pid101005/Pass00063.pt
[2026-01-05 18:10:12] Dump 00064th pass to TP0_PP0_Rank0_pid101005/Pass00064.pt
[2026-01-05 18:10:13] Dump 00065th pass to TP0_PP0_Rank0_pid101005/Pass00065.pt
[2026-01-05 18:10:14] Dump 00066th pass to TP0_PP0_Rank0_pid101005/Pass00066.pt
[2026-01-05 18:10:15] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:15] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK

[2026-01-05 18:10:15] Dump 00067th pass to TP0_PP0_Rank0_pid101005/Pass00067.pt
[2026-01-05 18:10:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.01, #running-req: 126, #queue-req: 0,
[2026-01-05 18:10:16] Dump 00068th pass to TP0_PP0_Rank0_pid101005/Pass00068.pt
[2026-01-05 18:10:18] Dump 00069th pass to TP0_PP0_Rank0_pid101005/Pass00069.pt
[2026-01-05 18:10:18] Dump 00070th pass to TP0_PP0_Rank0_pid101005/Pass00070.pt
[2026-01-05 18:10:19] Dump 00071th pass to TP0_PP0_Rank0_pid101005/Pass00071.pt
[2026-01-05 18:10:20] Dump 00072th pass to TP0_PP0_Rank0_pid101005/Pass00072.pt
[2026-01-05 18:10:21] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
  0%|          | 1/200 [01:21<4:31:15, 81.79s/it]
[2026-01-05 18:10:22] Dump 00073th pass to TP0_PP0_Rank0_pid101005/Pass00073.pt
[2026-01-05 18:10:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
[2026-01-05 18:10:22] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
  2%|▏         | 3/200 [01:28<1:17:30, 23.61s/it]
[2026-01-05 18:10:23] Dump 00074th pass to TP0_PP0_Rank0_pid101005/Pass00074.pt
[2026-01-05 18:10:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 128, #queue-req: 0,
[2026-01-05 18:10:23] Dump 00075th pass to TP0_PP0_Rank0_pid101005/Pass00075.pt
[2026-01-05 18:10:24] Dump 00076th pass to TP0_PP0_Rank0_pid101005/Pass00076.pt
[2026-01-05 18:10:25] Dump 00077th pass to TP0_PP0_Rank0_pid101005/Pass00077.pt
[2026-01-05 18:10:26] Dump 00078th pass to TP0_PP0_Rank0_pid101005/Pass00078.pt
[2026-01-05 18:10:27] Dump 00079th pass to TP0_PP0_Rank0_pid101005/Pass00079.pt
[2026-01-05 18:10:28] Dump 00080th pass to TP0_PP0_Rank0_pid101005/Pass00080.pt
[2026-01-05 18:10:29] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
  2%|▏         | 4/200 [01:29<51:33, 15.78s/it]
[2026-01-05 18:10:29] Dump 00081th pass to TP0_PP0_Rank0_pid101005/Pass00081.pt
[2026-01-05 18:10:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
[2026-01-05 18:10:30] Dump 00082th pass to TP0_PP0_Rank0_pid101005/Pass00082.pt
[2026-01-05 18:10:31] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
  2%|▎         | 5/200 [01:35<41:05, 12.65s/it]
[2026-01-05 18:10:31] Dump 00083th pass to TP0_PP0_Rank0_pid101005/Pass00083.pt
[2026-01-05 18:10:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
[2026-01-05 18:10:31] Dump 00084th pass to TP0_PP0_Rank0_pid101005/Pass00084.pt
[2026-01-05 18:10:32] Dump 00085th pass to TP0_PP0_Rank0_pid101005/Pass00085.pt
[2026-01-05 18:10:33] Dump 00086th pass to TP0_PP0_Rank0_pid101005/Pass00086.pt
[2026-01-05 18:10:34] Dump 00087th pass to TP0_PP0_Rank0_pid101005/Pass00087.pt
[2026-01-05 18:10:35] Dump 00088th pass to TP0_PP0_Rank0_pid101005/Pass00088.pt
[2026-01-05 18:10:36] Dump 00089th pass to TP0_PP0_Rank0_pid101005/Pass00089.pt
[2026-01-05 18:10:37] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
  3%|▎         | 6/200 [01:37<29:41,  9.18s/it]
[2026-01-05 18:10:37] Dump 00090th pass to TP0_PP0_Rank0_pid101005/Pass00090.pt
[2026-01-05 18:10:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:10:38] Dump 00091th pass to TP0_PP0_Rank0_pid101005/Pass00091.pt
[2026-01-05 18:10:39] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
  4%|▎         | 7/200 [01:43<26:45,  8.32s/it]
[2026-01-05 18:10:39] Dump 00092th pass to TP0_PP0_Rank0_pid101005/Pass00092.pt
[2026-01-05 18:10:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:10:40] Dump 00093th pass to TP0_PP0_Rank0_pid101005/Pass00093.pt
[2026-01-05 18:10:40] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:40] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
  4%|▍         | 8/200 [01:45<20:03,  6.27s/it]
  4%|▍         | 9/200 [01:47<15:15,  4.79s/it]
[2026-01-05 18:10:40] Dump 00094th pass to TP0_PP0_Rank0_pid101005/Pass00094.pt
[2026-01-05 18:10:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:10:41] Dump 00095th pass to TP0_PP0_Rank0_pid101005/Pass00095.pt
[2026-01-05 18:10:42] Dump 00096th pass to TP0_PP0_Rank0_pid101005/Pass00096.pt
[2026-01-05 18:10:43] Dump 00097th pass to TP0_PP0_Rank0_pid101005/Pass00097.pt
[2026-01-05 18:10:44] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:44] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:44] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
  5%|▌         | 10/200 [01:47<09:09,  2.89s/it]
  6%|▌         | 11/200 [01:50<09:42,  3.08s/it]
  6%|▋         | 13/200 [01:50<06:00,  1.93s/it]
[2026-01-05 18:10:44] Dump 00098th pass to TP0_PP0_Rank0_pid101005/Pass00098.pt
[2026-01-05 18:10:45] Decode batch, #running-req: 128, #token: 24576, token usage: 0.02, cpu graph: False, gen throughput (token/s): 108.40, #queue-req: 0,
[2026-01-05 18:10:45] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:45] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
  6%|▋         | 13/200 [01:50<06:00,  1.93s/it]
[2026-01-05 18:10:45] Dump 00099th pass to TP0_PP0_Rank0_pid101005/Pass00099.pt
[2026-01-05 18:10:47] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
  7%|▋         | 14/200 [01:51<05:27,  1.76s/it]
[2026-01-05 18:10:47] Dump 00100th pass to TP0_PP0_Rank0_pid101005/Pass00100.pt
[2026-01-05 18:10:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:10:47] Dump 00101th pass to TP0_PP0_Rank0_pid101005/Pass00101.pt
[2026-01-05 18:10:48] Dump 00102th pass to TP0_PP0_Rank0_pid101005/Pass00102.pt
[2026-01-05 18:10:49] Dump 00103th pass to TP0_PP0_Rank0_pid101005/Pass00103.pt
[2026-01-05 18:10:50] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:50] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:50] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
  8%|▊         | 15/200 [01:53<05:30,  1.79s/it]
  8%|▊         | 16/200 [01:56<06:07,  2.00s/it]
  9%|▉         | 18/200 [01:56<04:07,  1.36s/it]
[2026-01-05 18:10:50] Dump 00104th pass to TP0_PP0_Rank0_pid101005/Pass00104.pt
[2026-01-05 18:10:51] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:51] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
  9%|▉         | 18/200 [01:56<04:07,  1.36s/it]
[2026-01-05 18:10:51] Dump 00105th pass to TP0_PP0_Rank0_pid101005/Pass00105.pt
[2026-01-05 18:10:52] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:10:52] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:52] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
 10%|▉         | 19/200 [01:57<03:54,  1.29s/it]
 10%|█         | 20/200 [01:59<04:13,  1.41s/it]
 11%|█         | 22/200 [01:59<02:50,  1.04it/s]
[2026-01-05 18:10:52] Dump 00106th pass to TP0_PP0_Rank0_pid101005/Pass00106.pt
[2026-01-05 18:10:53] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:10:53] Dump 00107th pass to TP0_PP0_Rank0_pid101005/Pass00107.pt
[2026-01-05 18:10:55] Dump 00108th pass to TP0_PP0_Rank0_pid101005/Pass00108.pt
[2026-01-05 18:10:56] Dump 00109th pass to TP0_PP0_Rank0_pid101005/Pass00109.pt
[2026-01-05 18:10:57] Dump 00110th pass to TP0_PP0_Rank0_pid101005/Pass00110.pt
[2026-01-05 18:10:58] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:58] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
 11%|█         | 22/200 [01:59<02:50,  1.04it/s]
 12%|█▏        | 23/200 [02:04<05:14,  1.78s/it]
[2026-01-05 18:10:58] Dump 00111th pass to TP0_PP0_Rank0_pid101005/Pass00111.pt
[2026-01-05 18:10:59] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:10:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
 12%|█▏        | 24/200 [02:04<06:14,  2.13s/it]
[2026-01-05 18:10:59] Dump 00112th pass to TP0_PP0_Rank0_pid101005/Pass00112.pt
[2026-01-05 18:11:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:11:01] Dump 00113th pass to TP0_PP0_Rank0_pid101005/Pass00113.pt
[2026-01-05 18:11:01] Dump 00114th pass to TP0_PP0_Rank0_pid101005/Pass00114.pt
[2026-01-05 18:11:02] Dump 00115th pass to TP0_PP0_Rank0_pid101005/Pass00115.pt
[2026-01-05 18:11:03] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
 12%|█▎        | 25/200 [02:06<05:44,  1.97s/it]
[2026-01-05 18:11:03] Dump 00116th pass to TP0_PP0_Rank0_pid101005/Pass00116.pt
[2026-01-05 18:11:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:11:04] Dump 00117th pass to TP0_PP0_Rank0_pid101005/Pass00117.pt
[2026-01-05 18:11:05] Dump 00118th pass to TP0_PP0_Rank0_pid101005/Pass00118.pt
[2026-01-05 18:11:06] Dump 00119th pass to TP0_PP0_Rank0_pid101005/Pass00119.pt
[2026-01-05 18:11:07] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:07] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:07] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:07] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
 13%|█▎        | 26/200 [02:09<06:51,  2.36s/it]
 14%|█▎        | 27/200 [02:13<07:54,  2.74s/it]
 15%|█▌        | 30/200 [02:13<04:13,  1.49s/it]
 15%|█▌        | 30/200 [02:13<04:13,  1.49s/it]
[2026-01-05 18:11:07] Dump 00120th pass to TP0_PP0_Rank0_pid101005/Pass00120.pt
[2026-01-05 18:11:08] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:08] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
 15%|█▌        | 30/200 [02:13<04:13,  1.49s/it]
[2026-01-05 18:11:08] Dump 00121th pass to TP0_PP0_Rank0_pid101005/Pass00121.pt
[2026-01-05 18:11:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:11:11] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:11] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:11] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
 16%|█▌        | 31/200 [02:15<04:03,  1.44s/it]
 16%|█▌        | 32/200 [02:17<04:38,  1.66s/it]
 17%|█▋        | 34/200 [02:17<03:34,  1.29s/it]
[2026-01-05 18:11:11] Dump 00122th pass to TP0_PP0_Rank0_pid101005/Pass00122.pt
[2026-01-05 18:11:12] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:11:12] Dump 00123th pass to TP0_PP0_Rank0_pid101005/Pass00123.pt
[2026-01-05 18:11:13] Dump 00124th pass to TP0_PP0_Rank0_pid101005/Pass00124.pt
[2026-01-05 18:11:14] Dump 00125th pass to TP0_PP0_Rank0_pid101005/Pass00125.pt
[2026-01-05 18:11:15] Dump 00126th pass to TP0_PP0_Rank0_pid101005/Pass00126.pt
[2026-01-05 18:11:16] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:16] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
 17%|█▋        | 34/200 [02:17<03:34,  1.29s/it]
 18%|█▊        | 35/200 [02:23<05:24,  1.97s/it]
[2026-01-05 18:11:16] Dump 00127th pass to TP0_PP0_Rank0_pid101005/Pass00127.pt
[2026-01-05 18:11:17] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:11:17] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:17] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
 18%|█▊        | 36/200 [02:23<06:01,  2.20s/it]
 18%|█▊        | 37/200 [02:24<05:23,  1.98s/it]
 20%|█▉        | 39/200 [02:24<02:39,  1.01it/s]
[2026-01-05 18:11:17] Dump 00128th pass to TP0_PP0_Rank0_pid101005/Pass00128.pt
[2026-01-05 18:11:19] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:19] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:19] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.02, #running-req: 128, #queue-req: 0,
 20%|█▉        | 39/200 [02:24<02:39,  1.01it/s]
 20%|██        | 40/200 [02:25<02:41,  1.01s/it]
[2026-01-05 18:11:19] Dump 00129th pass to TP0_PP0_Rank0_pid101005/Pass00129.pt
[2026-01-05 18:11:20] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 131, #queue-req: 0,
[2026-01-05 18:11:20] Dump 00130th pass to TP0_PP0_Rank0_pid101005/Pass00130.pt
[2026-01-05 18:11:21] Dump 00131th pass to TP0_PP0_Rank0_pid101005/Pass00131.pt
[2026-01-05 18:11:22] Dump 00132th pass to TP0_PP0_Rank0_pid101005/Pass00132.pt
[2026-01-05 18:11:23] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:23] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:23] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
 20%|██        | 41/200 [02:25<02:17,  1.15it/s]
 21%|██        | 42/200 [02:29<03:59,  1.52s/it]
 22%|██▏       | 44/200 [02:29<04:11,  1.61s/it]
[2026-01-05 18:11:23] Dump 00133th pass to TP0_PP0_Rank0_pid101005/Pass00133.pt
[2026-01-05 18:11:24] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 18:11:24] Dump 00134th pass to TP0_PP0_Rank0_pid101005/Pass00134.pt
[2026-01-05 18:11:26] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
 22%|██▏       | 44/200 [02:29<04:11,  1.61s/it]
[2026-01-05 18:11:26] Dump 00135th pass to TP0_PP0_Rank0_pid101005/Pass00135.pt
[2026-01-05 18:11:26] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:11:26] Dump 00136th pass to TP0_PP0_Rank0_pid101005/Pass00136.pt
[2026-01-05 18:11:27] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:27] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
 22%|██▎       | 45/200 [02:32<04:40,  1.81s/it]
 23%|██▎       | 46/200 [02:34<04:34,  1.79s/it]
[2026-01-05 18:11:27] Dump 00137th pass to TP0_PP0_Rank0_pid101005/Pass00137.pt
[2026-01-05 18:11:28] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:11:28] Dump 00138th pass to TP0_PP0_Rank0_pid101005/Pass00138.pt
[2026-01-05 18:11:30] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:30] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:30] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
 24%|██▎       | 47/200 [02:34<03:41,  1.45s/it]
 24%|██▍       | 48/200 [02:36<04:05,  1.61s/it]
 25%|██▌       | 50/200 [02:36<02:50,  1.14s/it]
[2026-01-05 18:11:30] Dump 00139th pass to TP0_PP0_Rank0_pid101005/Pass00139.pt
[2026-01-05 18:11:30] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 18:11:30] Dump 00140th pass to TP0_PP0_Rank0_pid101005/Pass00140.pt
[2026-01-05 18:11:32] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:32] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:32] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:32] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
 25%|██▌       | 50/200 [02:36<02:50,  1.14s/it]
 26%|██▌       | 51/200 [02:38<03:22,  1.36s/it]
 27%|██▋       | 54/200 [02:38<02:08,  1.14it/s]
 27%|██▋       | 54/200 [02:38<02:08,  1.14it/s]
[2026-01-05 18:11:32] Dump 00141th pass to TP0_PP0_Rank0_pid101005/Pass00141.pt
[2026-01-05 18:11:33] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-05 18:11:33] Dump 00142th pass to TP0_PP0_Rank0_pid101005/Pass00142.pt
[2026-01-05 18:11:35] Dump 00143th pass to TP0_PP0_Rank0_pid101005/Pass00143.pt
[2026-01-05 18:11:36] Dump 00144th pass to TP0_PP0_Rank0_pid101005/Pass00144.pt
[2026-01-05 18:11:37] Dump 00145th pass to TP0_PP0_Rank0_pid101005/Pass00145.pt
[2026-01-05 18:11:38] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:38] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:38] INFO:     127.0.0.1:34074 - "POST /generate HTTP/1.1" 200 OK
 27%|██▋       | 54/200 [02:38<02:08,  1.14it/s]
 28%|██▊       | 55/200 [02:44<03:52,  1.60s/it]
 28%|██▊       | 57/200 [02:44<04:51,  2.04s/it]
[2026-01-05 18:11:38] Dump 00146th pass to TP0_PP0_Rank0_pid101005/Pass00146.pt
[2026-01-05 18:11:39] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:39] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
 28%|██▊       | 57/200 [02:44<04:51,  2.04s/it]
[2026-01-05 18:11:39] Dump 00147th pass to TP0_PP0_Rank0_pid101005/Pass00147.pt
[2026-01-05 18:11:41] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:11:41] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:41] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
 29%|██▉       | 58/200 [02:45<04:26,  1.88s/it]
 30%|██▉       | 59/200 [02:48<04:32,  1.93s/it]
 30%|███       | 61/200 [02:48<02:54,  1.26s/it]
[2026-01-05 18:11:41] Dump 00148th pass to TP0_PP0_Rank0_pid101005/Pass00148.pt
[2026-01-05 18:11:42] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:11:42] Dump 00149th pass to TP0_PP0_Rank0_pid101005/Pass00149.pt
[2026-01-05 18:11:43] Dump 00150th pass to TP0_PP0_Rank0_pid101005/Pass00150.pt
[2026-01-05 18:11:44] Dump 00151th pass to TP0_PP0_Rank0_pid101005/Pass00151.pt
[2026-01-05 18:11:45] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:45] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
 30%|███       | 61/200 [02:48<02:54,  1.26s/it]
 31%|███       | 62/200 [02:52<03:54,  1.70s/it]
[2026-01-05 18:11:45] Dump 00152th pass to TP0_PP0_Rank0_pid101005/Pass00152.pt
[2026-01-05 18:11:46] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
 32%|███▏      | 63/200 [02:52<04:04,  1.78s/it]
[2026-01-05 18:11:46] Dump 00153th pass to TP0_PP0_Rank0_pid101005/Pass00153.pt
[2026-01-05 18:11:47] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:11:47] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
 32%|███▏      | 64/200 [02:53<03:42,  1.64s/it]
 32%|███▎      | 65/200 [02:54<03:28,  1.55s/it]
[2026-01-05 18:11:47] Dump 00154th pass to TP0_PP0_Rank0_pid101005/Pass00154.pt
[2026-01-05 18:11:48] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:11:48] Dump 00155th pass to TP0_PP0_Rank0_pid101005/Pass00155.pt
[2026-01-05 18:11:49] Dump 00156th pass to TP0_PP0_Rank0_pid101005/Pass00156.pt
[2026-01-05 18:11:50] Dump 00157th pass to TP0_PP0_Rank0_pid101005/Pass00157.pt
[2026-01-05 18:11:51] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:51] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
 33%|███▎      | 66/200 [02:54<02:39,  1.19s/it]
 34%|███▎      | 67/200 [02:57<03:47,  1.71s/it]
[2026-01-05 18:11:51] Dump 00158th pass to TP0_PP0_Rank0_pid101005/Pass00158.pt
[2026-01-05 18:11:52] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:52] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:11:52] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
 34%|███▍      | 68/200 [02:57<03:51,  1.76s/it]
 34%|███▍      | 69/200 [02:58<03:31,  1.62s/it]
[2026-01-05 18:11:52] Dump 00159th pass to TP0_PP0_Rank0_pid101005/Pass00159.pt
[2026-01-05 18:11:53] INFO:     127.0.0.1:33900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:11:53] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
 35%|███▌      | 70/200 [02:58<02:37,  1.21s/it]
 36%|███▌      | 71/200 [03:00<02:36,  1.21s/it]
[2026-01-05 18:11:53] Dump 00160th pass to TP0_PP0_Rank0_pid101005/Pass00160.pt
[2026-01-05 18:11:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-05 18:11:54] Dump 00161th pass to TP0_PP0_Rank0_pid101005/Pass00161.pt
[2026-01-05 18:11:55] Dump 00162th pass to TP0_PP0_Rank0_pid101005/Pass00162.pt
[2026-01-05 18:11:56] Dump 00163th pass to TP0_PP0_Rank0_pid101005/Pass00163.pt
[2026-01-05 18:11:57] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:57] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
 36%|███▌      | 72/200 [03:00<02:06,  1.01it/s]
 36%|███▋      | 73/200 [03:04<03:22,  1.59s/it]
[2026-01-05 18:11:57] Dump 00164th pass to TP0_PP0_Rank0_pid101005/Pass00164.pt
[2026-01-05 18:11:58] INFO:     127.0.0.1:33926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:58] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:11:58] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
 37%|███▋      | 74/200 [03:04<03:35,  1.71s/it]
 38%|███▊      | 75/200 [03:05<03:15,  1.56s/it]
 38%|███▊      | 77/200 [03:05<01:37,  1.26it/s]
[2026-01-05 18:11:58] Dump 00165th pass to TP0_PP0_Rank0_pid101005/Pass00165.pt
[2026-01-05 18:11:59] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
 38%|███▊      | 77/200 [03:05<01:37,  1.26it/s]
[2026-01-05 18:11:59] Dump 00166th pass to TP0_PP0_Rank0_pid101005/Pass00166.pt
[2026-01-05 18:12:00] Dump 00167th pass to TP0_PP0_Rank0_pid101005/Pass00167.pt
[2026-01-05 18:12:01] Decode batch, #running-req: 122, #token: 25344, token usage: 0.02, cpu graph: False, gen throughput (token/s): 66.44, #queue-req: 0,
[2026-01-05 18:12:01] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
 39%|███▉      | 78/200 [03:06<01:40,  1.21it/s]
[2026-01-05 18:12:01] Dump 00168th pass to TP0_PP0_Rank0_pid101005/Pass00168.pt
[2026-01-05 18:12:02] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:02] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
 40%|███▉      | 79/200 [03:08<02:08,  1.06s/it]
 40%|████      | 80/200 [03:09<02:07,  1.06s/it]
[2026-01-05 18:12:02] Dump 00169th pass to TP0_PP0_Rank0_pid101005/Pass00169.pt
[2026-01-05 18:12:03] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:03] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:03] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:03] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:03] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
 40%|████      | 81/200 [03:09<01:42,  1.17it/s]
 41%|████      | 82/200 [03:10<01:45,  1.11it/s]
 43%|████▎     | 86/200 [03:10<00:37,  3.06it/s]
 43%|████▎     | 86/200 [03:10<00:37,  3.06it/s]
 43%|████▎     | 86/200 [03:10<00:37,  3.06it/s]
[2026-01-05 18:12:03] Dump 00170th pass to TP0_PP0_Rank0_pid101005/Pass00170.pt
[2026-01-05 18:12:04] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:04] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:04] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:04] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:04] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
 43%|████▎     | 86/200 [03:10<00:37,  3.06it/s]
 44%|████▎     | 87/200 [03:11<00:45,  2.47it/s]
 46%|████▌     | 91/200 [03:11<00:30,  3.56it/s]
 46%|████▌     | 91/200 [03:11<00:30,  3.56it/s]
 46%|████▌     | 91/200 [03:11<00:30,  3.56it/s]
[2026-01-05 18:12:04] Dump 00171th pass to TP0_PP0_Rank0_pid101005/Pass00171.pt
[2026-01-05 18:12:05] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:05] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:05] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
 46%|████▌     | 91/200 [03:11<00:30,  3.56it/s]
 46%|████▌     | 92/200 [03:12<00:38,  2.79it/s]
 47%|████▋     | 94/200 [03:12<00:39,  2.67it/s]
[2026-01-05 18:12:05] Dump 00172th pass to TP0_PP0_Rank0_pid101005/Pass00172.pt
[2026-01-05 18:12:06] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
 47%|████▋     | 94/200 [03:12<00:39,  2.67it/s]
[2026-01-05 18:12:06] Dump 00173th pass to TP0_PP0_Rank0_pid101005/Pass00173.pt
[2026-01-05 18:12:07] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:07] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
 48%|████▊     | 95/200 [03:12<00:45,  2.31it/s]
 48%|████▊     | 96/200 [03:13<00:52,  2.00it/s]
[2026-01-05 18:12:07] Dump 00174th pass to TP0_PP0_Rank0_pid101005/Pass00174.pt
[2026-01-05 18:12:08] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:08] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
 48%|████▊     | 97/200 [03:13<00:49,  2.10it/s]
 49%|████▉     | 98/200 [03:14<00:55,  1.83it/s]
[2026-01-05 18:12:08] Dump 00175th pass to TP0_PP0_Rank0_pid101005/Pass00175.pt
[2026-01-05 18:12:09] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
 50%|████▉     | 99/200 [03:14<00:50,  1.99it/s]
[2026-01-05 18:12:09] Dump 00176th pass to TP0_PP0_Rank0_pid101005/Pass00176.pt
[2026-01-05 18:12:10] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
 50%|█████     | 100/200 [03:15<00:57,  1.74it/s]
[2026-01-05 18:12:10] Dump 00177th pass to TP0_PP0_Rank0_pid101005/Pass00177.pt
[2026-01-05 18:12:10] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
 50%|█████     | 101/200 [03:16<01:02,  1.57it/s]
[2026-01-05 18:12:10] Dump 00178th pass to TP0_PP0_Rank0_pid101005/Pass00178.pt
[2026-01-05 18:12:11] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:11] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
 51%|█████     | 102/200 [03:17<01:07,  1.46it/s]
 52%|█████▏    | 103/200 [03:18<01:09,  1.39it/s]
[2026-01-05 18:12:11] Dump 00179th pass to TP0_PP0_Rank0_pid101005/Pass00179.pt
[2026-01-05 18:12:12] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
 52%|█████▏    | 104/200 [03:18<00:56,  1.70it/s]
[2026-01-05 18:12:12] Dump 00180th pass to TP0_PP0_Rank0_pid101005/Pass00180.pt
[2026-01-05 18:12:13] Dump 00181th pass to TP0_PP0_Rank0_pid101005/Pass00181.pt
[2026-01-05 18:12:14] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:14] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:14] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:14] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
 52%|█████▎    | 105/200 [03:18<01:01,  1.55it/s]
 53%|█████▎    | 106/200 [03:20<01:24,  1.11it/s]
 55%|█████▍    | 109/200 [03:20<00:52,  1.73it/s]
 55%|█████▍    | 109/200 [03:20<00:52,  1.73it/s]
[2026-01-05 18:12:14] Dump 00182th pass to TP0_PP0_Rank0_pid101005/Pass00182.pt
[2026-01-05 18:12:15] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:15] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:15] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:15] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
 55%|█████▍    | 109/200 [03:20<00:52,  1.73it/s]
 55%|█████▌    | 110/200 [03:21<00:55,  1.63it/s]
 56%|█████▋    | 113/200 [03:21<00:29,  2.91it/s]
 56%|█████▋    | 113/200 [03:21<00:29,  2.91it/s]
[2026-01-05 18:12:15] Dump 00183th pass to TP0_PP0_Rank0_pid101005/Pass00183.pt
[2026-01-05 18:12:15] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
 56%|█████▋    | 113/200 [03:21<00:29,  2.91it/s]
[2026-01-05 18:12:15] Dump 00184th pass to TP0_PP0_Rank0_pid101005/Pass00184.pt
[2026-01-05 18:12:17] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
 57%|█████▋    | 114/200 [03:22<00:34,  2.46it/s]
[2026-01-05 18:12:17] Dump 00185th pass to TP0_PP0_Rank0_pid101005/Pass00185.pt
[2026-01-05 18:12:18] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:18] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
 57%|█████▊    | 115/200 [03:24<00:55,  1.54it/s]
 58%|█████▊    | 116/200 [03:24<00:56,  1.50it/s]
[2026-01-05 18:12:18] Dump 00186th pass to TP0_PP0_Rank0_pid101005/Pass00186.pt
[2026-01-05 18:12:19] Dump 00187th pass to TP0_PP0_Rank0_pid101005/Pass00187.pt
[2026-01-05 18:12:19] Dump 00188th pass to TP0_PP0_Rank0_pid101005/Pass00188.pt
[2026-01-05 18:12:20] Dump 00189th pass to TP0_PP0_Rank0_pid101005/Pass00189.pt
[2026-01-05 18:12:21] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:21] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
 58%|█████▊    | 117/200 [03:24<00:47,  1.76it/s]
 59%|█████▉    | 118/200 [03:27<01:20,  1.02it/s]
[2026-01-05 18:12:21] Dump 00190th pass to TP0_PP0_Rank0_pid101005/Pass00190.pt
[2026-01-05 18:12:21] Dump 00191th pass to TP0_PP0_Rank0_pid101005/Pass00191.pt
[2026-01-05 18:12:22] Dump 00192th pass to TP0_PP0_Rank0_pid101005/Pass00192.pt
[2026-01-05 18:12:23] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:23] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
 60%|█████▉    | 119/200 [03:27<01:30,  1.11s/it]
 60%|██████    | 120/200 [03:29<01:41,  1.27s/it]
[2026-01-05 18:12:23] Dump 00193th pass to TP0_PP0_Rank0_pid101005/Pass00193.pt
[2026-01-05 18:12:23] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
 60%|██████    | 121/200 [03:29<01:31,  1.15s/it]
[2026-01-05 18:12:23] Dump 00194th pass to TP0_PP0_Rank0_pid101005/Pass00194.pt
[2026-01-05 18:12:24] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
 61%|██████    | 122/200 [03:30<01:21,  1.05s/it]
[2026-01-05 18:12:24] Dump 00195th pass to TP0_PP0_Rank0_pid101005/Pass00195.pt
[2026-01-05 18:12:25] Dump 00196th pass to TP0_PP0_Rank0_pid101005/Pass00196.pt
[2026-01-05 18:12:25] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:25] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
 62%|██████▏   | 123/200 [03:30<01:13,  1.05it/s]
 62%|██████▏   | 124/200 [03:32<01:19,  1.04s/it]
[2026-01-05 18:12:25] Dump 00197th pass to TP0_PP0_Rank0_pid101005/Pass00197.pt
[2026-01-05 18:12:26] Dump 00198th pass to TP0_PP0_Rank0_pid101005/Pass00198.pt
[2026-01-05 18:12:27] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:27] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:27] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
 62%|██████▎   | 125/200 [03:32<01:06,  1.13it/s]
 63%|██████▎   | 126/200 [03:33<01:11,  1.03it/s]
 64%|██████▍   | 128/200 [03:33<00:46,  1.56it/s]
[2026-01-05 18:12:27] Dump 00199th pass to TP0_PP0_Rank0_pid101005/Pass00199.pt
[2026-01-05 18:12:27] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:27] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
 64%|██████▍   | 128/200 [03:33<00:46,  1.56it/s]
 64%|██████▍   | 129/200 [03:33<00:45,  1.57it/s]
[2026-01-05 18:12:27] Dump 00200th pass to TP0_PP0_Rank0_pid101005/Pass00200.pt
[2026-01-05 18:12:28] Dump 00201th pass to TP0_PP0_Rank0_pid101005/Pass00201.pt
[2026-01-05 18:12:28] INFO:     127.0.0.1:33980 - "POST /generate HTTP/1.1" 200 OK
 65%|██████▌   | 130/200 [03:33<00:36,  1.90it/s]
[2026-01-05 18:12:28] Dump 00202th pass to TP0_PP0_Rank0_pid101005/Pass00202.pt
[2026-01-05 18:12:29] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:29] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:29] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:29] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:29] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
 66%|██████▌   | 131/200 [03:35<00:44,  1.55it/s]
 66%|██████▌   | 132/200 [03:35<00:43,  1.57it/s]
 68%|██████▊   | 136/200 [03:35<00:12,  4.93it/s]
 68%|██████▊   | 136/200 [03:35<00:12,  4.93it/s]
 68%|██████▊   | 136/200 [03:35<00:12,  4.93it/s]
[2026-01-05 18:12:29] Dump 00203th pass to TP0_PP0_Rank0_pid101005/Pass00203.pt
[2026-01-05 18:12:29] Dump 00204th pass to TP0_PP0_Rank0_pid101005/Pass00204.pt
[2026-01-05 18:12:30] Dump 00205th pass to TP0_PP0_Rank0_pid101005/Pass00205.pt
[2026-01-05 18:12:30] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
 68%|██████▊   | 136/200 [03:35<00:12,  4.93it/s]
[2026-01-05 18:12:31] Dump 00206th pass to TP0_PP0_Rank0_pid101005/Pass00206.pt
[2026-01-05 18:12:31] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:31] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:31] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:31] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
 68%|██████▊   | 137/200 [03:37<00:22,  2.77it/s]
 69%|██████▉   | 138/200 [03:37<00:23,  2.64it/s]
 70%|███████   | 141/200 [03:37<00:12,  4.78it/s]
 70%|███████   | 141/200 [03:37<00:12,  4.78it/s]
[2026-01-05 18:12:31] Dump 00207th pass to TP0_PP0_Rank0_pid101005/Pass00207.pt
[2026-01-05 18:12:31] Decode batch, #running-req: 63, #token: 14336, token usage: 0.01, cpu graph: False, gen throughput (token/s): 115.95, #queue-req: 0,
[2026-01-05 18:12:31] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
 70%|███████   | 141/200 [03:37<00:12,  4.78it/s]
[2026-01-05 18:12:32] Dump 00208th pass to TP0_PP0_Rank0_pid101005/Pass00208.pt
[2026-01-05 18:12:32] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
 71%|███████   | 142/200 [03:38<00:14,  4.01it/s]
[2026-01-05 18:12:32] Dump 00209th pass to TP0_PP0_Rank0_pid101005/Pass00209.pt
[2026-01-05 18:12:32] Dump 00210th pass to TP0_PP0_Rank0_pid101005/Pass00210.pt
[2026-01-05 18:12:33] Dump 00211th pass to TP0_PP0_Rank0_pid101005/Pass00211.pt
[2026-01-05 18:12:33] Dump 00212th pass to TP0_PP0_Rank0_pid101005/Pass00212.pt
[2026-01-05 18:12:34] Dump 00213th pass to TP0_PP0_Rank0_pid101005/Pass00213.pt
[2026-01-05 18:12:35] Dump 00214th pass to TP0_PP0_Rank0_pid101005/Pass00214.pt
[2026-01-05 18:12:35] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▏  | 143/200 [03:38<00:16,  3.46it/s]
[2026-01-05 18:12:35] Dump 00215th pass to TP0_PP0_Rank0_pid101005/Pass00215.pt
[2026-01-05 18:12:35] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▏  | 144/200 [03:41<00:43,  1.28it/s]
[2026-01-05 18:12:35] Dump 00216th pass to TP0_PP0_Rank0_pid101005/Pass00216.pt
[2026-01-05 18:12:36] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▎  | 145/200 [03:42<00:39,  1.40it/s]
[2026-01-05 18:12:36] Dump 00217th pass to TP0_PP0_Rank0_pid101005/Pass00217.pt
[2026-01-05 18:12:36] Dump 00218th pass to TP0_PP0_Rank0_pid101005/Pass00218.pt
[2026-01-05 18:12:37] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:37] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:37] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
 73%|███████▎  | 146/200 [03:42<00:34,  1.54it/s]
 74%|███████▎  | 147/200 [03:43<00:37,  1.40it/s]
 74%|███████▍  | 149/200 [03:43<00:23,  2.15it/s]
[2026-01-05 18:12:37] Dump 00219th pass to TP0_PP0_Rank0_pid101005/Pass00219.pt
[2026-01-05 18:12:37] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:37] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
 74%|███████▍  | 149/200 [03:43<00:23,  2.15it/s]
 75%|███████▌  | 150/200 [03:44<00:23,  2.14it/s]
[2026-01-05 18:12:37] Dump 00220th pass to TP0_PP0_Rank0_pid101005/Pass00220.pt
[2026-01-05 18:12:38] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
 76%|███████▌  | 151/200 [03:44<00:19,  2.57it/s]
[2026-01-05 18:12:38] Dump 00221th pass to TP0_PP0_Rank0_pid101005/Pass00221.pt
[2026-01-05 18:12:38] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
 76%|███████▌  | 152/200 [03:44<00:18,  2.55it/s]
[2026-01-05 18:12:38] Dump 00222th pass to TP0_PP0_Rank0_pid101005/Pass00222.pt
[2026-01-05 18:12:38] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
 76%|███████▋  | 153/200 [03:44<00:18,  2.48it/s]
[2026-01-05 18:12:39] Dump 00223th pass to TP0_PP0_Rank0_pid101005/Pass00223.pt
[2026-01-05 18:12:39] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:39] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:39] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
 77%|███████▋  | 154/200 [03:45<00:18,  2.45it/s]
 78%|███████▊  | 155/200 [03:45<00:18,  2.40it/s]
 78%|███████▊  | 157/200 [03:45<00:10,  4.09it/s]
[2026-01-05 18:12:39] Dump 00224th pass to TP0_PP0_Rank0_pid101005/Pass00224.pt
[2026-01-05 18:12:39] Dump 00225th pass to TP0_PP0_Rank0_pid101005/Pass00225.pt
[2026-01-05 18:12:40] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
 78%|███████▊  | 157/200 [03:45<00:10,  4.09it/s]
[2026-01-05 18:12:40] Dump 00226th pass to TP0_PP0_Rank0_pid101005/Pass00226.pt
[2026-01-05 18:12:40] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
 79%|███████▉  | 158/200 [03:46<00:14,  2.93it/s]
[2026-01-05 18:12:40] Dump 00227th pass to TP0_PP0_Rank0_pid101005/Pass00227.pt
[2026-01-05 18:12:41] Dump 00228th pass to TP0_PP0_Rank0_pid101005/Pass00228.pt
[2026-01-05 18:12:41] Dump 00229th pass to TP0_PP0_Rank0_pid101005/Pass00229.pt
[2026-01-05 18:12:41] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:41] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
 80%|███████▉  | 159/200 [03:46<00:14,  2.84it/s]
 80%|████████  | 160/200 [03:47<00:20,  1.91it/s]
[2026-01-05 18:12:41] Dump 00230th pass to TP0_PP0_Rank0_pid101005/Pass00230.pt
[2026-01-05 18:12:42] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:42] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
 80%|████████  | 161/200 [03:47<00:20,  1.87it/s]
 81%|████████  | 162/200 [03:48<00:18,  2.03it/s]
[2026-01-05 18:12:42] Dump 00231th pass to TP0_PP0_Rank0_pid101005/Pass00231.pt
[2026-01-05 18:12:42] Dump 00232th pass to TP0_PP0_Rank0_pid101005/Pass00232.pt
[2026-01-05 18:12:42] Dump 00233th pass to TP0_PP0_Rank0_pid101005/Pass00233.pt
[2026-01-05 18:12:43] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▏ | 163/200 [03:48<00:13,  2.71it/s]
[2026-01-05 18:12:43] Dump 00234th pass to TP0_PP0_Rank0_pid101005/Pass00234.pt
[2026-01-05 18:12:43] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▏ | 164/200 [03:49<00:18,  1.96it/s]
[2026-01-05 18:12:43] Dump 00235th pass to TP0_PP0_Rank0_pid101005/Pass00235.pt
[2026-01-05 18:12:43] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▎ | 165/200 [03:49<00:16,  2.12it/s]
[2026-01-05 18:12:43] Dump 00236th pass to TP0_PP0_Rank0_pid101005/Pass00236.pt
[2026-01-05 18:12:44] Dump 00237th pass to TP0_PP0_Rank0_pid101005/Pass00237.pt
[2026-01-05 18:12:44] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:44] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:44] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
 83%|████████▎ | 166/200 [03:50<00:14,  2.27it/s]
 84%|████████▎ | 167/200 [03:50<00:16,  2.01it/s]
 84%|████████▍ | 169/200 [03:50<00:10,  3.06it/s]
[2026-01-05 18:12:44] Dump 00238th pass to TP0_PP0_Rank0_pid101005/Pass00238.pt
[2026-01-05 18:12:44] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
 84%|████████▍ | 169/200 [03:50<00:10,  3.06it/s]
[2026-01-05 18:12:44] Dump 00239th pass to TP0_PP0_Rank0_pid101005/Pass00239.pt
[2026-01-05 18:12:45] Dump 00240th pass to TP0_PP0_Rank0_pid101005/Pass00240.pt
[2026-01-05 18:12:45] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
 85%|████████▌ | 170/200 [03:51<00:09,  3.09it/s]
[2026-01-05 18:12:45] Dump 00241th pass to TP0_PP0_Rank0_pid101005/Pass00241.pt
[2026-01-05 18:12:45] Dump 00242th pass to TP0_PP0_Rank0_pid101005/Pass00242.pt
[2026-01-05 18:12:45] Dump 00243th pass to TP0_PP0_Rank0_pid101005/Pass00243.pt
[2026-01-05 18:12:46] Dump 00244th pass to TP0_PP0_Rank0_pid101005/Pass00244.pt
[2026-01-05 18:12:46] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
 86%|████████▌ | 171/200 [03:51<00:11,  2.60it/s]
[2026-01-05 18:12:46] Dump 00245th pass to TP0_PP0_Rank0_pid101005/Pass00245.pt
[2026-01-05 18:12:46] Dump 00246th pass to TP0_PP0_Rank0_pid101005/Pass00246.pt
[2026-01-05 18:12:47] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
 86%|████████▌ | 172/200 [03:52<00:15,  1.77it/s]
[2026-01-05 18:12:47] Dump 00247th pass to TP0_PP0_Rank0_pid101005/Pass00247.pt
[2026-01-05 18:12:47] Decode batch, #running-req: 28, #token: 8064, token usage: 0.01, cpu graph: False, gen throughput (token/s): 112.24, #queue-req: 0,
[2026-01-05 18:12:47] Dump 00248th pass to TP0_PP0_Rank0_pid101005/Pass00248.pt
[2026-01-05 18:12:47] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
 86%|████████▋ | 173/200 [03:53<00:15,  1.79it/s]
[2026-01-05 18:12:47] Dump 00249th pass to TP0_PP0_Rank0_pid101005/Pass00249.pt
[2026-01-05 18:12:47] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK
 87%|████████▋ | 174/200 [03:53<00:14,  1.81it/s]
[2026-01-05 18:12:47] Dump 00250th pass to TP0_PP0_Rank0_pid101005/Pass00250.pt
[2026-01-05 18:12:48] Dump 00251th pass to TP0_PP0_Rank0_pid101005/Pass00251.pt
[2026-01-05 18:12:48] Dump 00252th pass to TP0_PP0_Rank0_pid101005/Pass00252.pt
[2026-01-05 18:12:48] Dump 00253th pass to TP0_PP0_Rank0_pid101005/Pass00253.pt
[2026-01-05 18:12:48] Dump 00254th pass to TP0_PP0_Rank0_pid101005/Pass00254.pt
[2026-01-05 18:12:49] Dump 00255th pass to TP0_PP0_Rank0_pid101005/Pass00255.pt
[2026-01-05 18:12:49] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:49] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
 88%|████████▊ | 175/200 [03:54<00:11,  2.09it/s]
 88%|████████▊ | 176/200 [03:55<00:18,  1.27it/s]
[2026-01-05 18:12:49] Dump 00256th pass to TP0_PP0_Rank0_pid101005/Pass00256.pt
[2026-01-05 18:12:49] Dump 00257th pass to TP0_PP0_Rank0_pid101005/Pass00257.pt
[2026-01-05 18:12:49] Dump 00258th pass to TP0_PP0_Rank0_pid101005/Pass00258.pt
[2026-01-05 18:12:50] Dump 00259th pass to TP0_PP0_Rank0_pid101005/Pass00259.pt
[2026-01-05 18:12:50] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:50] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
 88%|████████▊ | 177/200 [03:55<00:18,  1.26it/s]
 89%|████████▉ | 178/200 [03:56<00:18,  1.20it/s]
[2026-01-05 18:12:50] Dump 00260th pass to TP0_PP0_Rank0_pid101005/Pass00260.pt
[2026-01-05 18:12:50] Dump 00261th pass to TP0_PP0_Rank0_pid101005/Pass00261.pt
[2026-01-05 18:12:50] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
 90%|████████▉ | 179/200 [03:56<00:14,  1.45it/s]
[2026-01-05 18:12:50] Dump 00262th pass to TP0_PP0_Rank0_pid101005/Pass00262.pt
[2026-01-05 18:12:51] Dump 00263th pass to TP0_PP0_Rank0_pid101005/Pass00263.pt
[2026-01-05 18:12:51] Dump 00264th pass to TP0_PP0_Rank0_pid101005/Pass00264.pt
[2026-01-05 18:12:51] Dump 00265th pass to TP0_PP0_Rank0_pid101005/Pass00265.pt
[2026-01-05 18:12:51] Dump 00266th pass to TP0_PP0_Rank0_pid101005/Pass00266.pt
[2026-01-05 18:12:52] Dump 00267th pass to TP0_PP0_Rank0_pid101005/Pass00267.pt
[2026-01-05 18:12:52] Dump 00268th pass to TP0_PP0_Rank0_pid101005/Pass00268.pt
[2026-01-05 18:12:52] Dump 00269th pass to TP0_PP0_Rank0_pid101005/Pass00269.pt
[2026-01-05 18:12:52] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
 90%|█████████ | 180/200 [03:57<00:12,  1.58it/s]
[2026-01-05 18:12:52] Dump 00270th pass to TP0_PP0_Rank0_pid101005/Pass00270.pt
[2026-01-05 18:12:52] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:12:52] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
 90%|█████████ | 181/200 [03:58<00:17,  1.09it/s]
 91%|█████████ | 182/200 [03:59<00:13,  1.36it/s]
[2026-01-05 18:12:52] Dump 00271th pass to TP0_PP0_Rank0_pid101005/Pass00271.pt
[2026-01-05 18:12:53] Dump 00272th pass to TP0_PP0_Rank0_pid101005/Pass00272.pt
[2026-01-05 18:12:53] Dump 00273th pass to TP0_PP0_Rank0_pid101005/Pass00273.pt
[2026-01-05 18:12:53] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▏| 183/200 [03:59<00:07,  2.14it/s]
[2026-01-05 18:12:53] Dump 00274th pass to TP0_PP0_Rank0_pid101005/Pass00274.pt
[2026-01-05 18:12:53] Dump 00275th pass to TP0_PP0_Rank0_pid101005/Pass00275.pt
[2026-01-05 18:12:53] Dump 00276th pass to TP0_PP0_Rank0_pid101005/Pass00276.pt
[2026-01-05 18:12:54] Dump 00277th pass to TP0_PP0_Rank0_pid101005/Pass00277.pt
[2026-01-05 18:12:54] Dump 00278th pass to TP0_PP0_Rank0_pid101005/Pass00278.pt
[2026-01-05 18:12:54] Dump 00279th pass to TP0_PP0_Rank0_pid101005/Pass00279.pt
[2026-01-05 18:12:54] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▏| 184/200 [03:59<00:08,  1.98it/s]
[2026-01-05 18:12:54] Dump 00280th pass to TP0_PP0_Rank0_pid101005/Pass00280.pt
[2026-01-05 18:12:54] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▎| 185/200 [04:00<00:09,  1.51it/s]
[2026-01-05 18:12:54] Dump 00281th pass to TP0_PP0_Rank0_pid101005/Pass00281.pt
[2026-01-05 18:12:55] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
 93%|█████████▎| 186/200 [04:01<00:07,  1.86it/s]
[2026-01-05 18:12:55] Dump 00282th pass to TP0_PP0_Rank0_pid101005/Pass00282.pt
[2026-01-05 18:12:55] Dump 00283th pass to TP0_PP0_Rank0_pid101005/Pass00283.pt
[2026-01-05 18:12:55] Dump 00284th pass to TP0_PP0_Rank0_pid101005/Pass00284.pt
[2026-01-05 18:12:55] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▎| 187/200 [04:01<00:05,  2.24it/s]
[2026-01-05 18:12:55] Dump 00285th pass to TP0_PP0_Rank0_pid101005/Pass00285.pt
[2026-01-05 18:12:55] Dump 00286th pass to TP0_PP0_Rank0_pid101005/Pass00286.pt
[2026-01-05 18:12:55] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▍| 188/200 [04:01<00:05,  2.15it/s]
[2026-01-05 18:12:55] Dump 00287th pass to TP0_PP0_Rank0_pid101005/Pass00287.pt
[2026-01-05 18:12:56] Decode batch, #running-req: 12, #token: 4224, token usage: 0.00, cpu graph: False, gen throughput (token/s): 89.35, #queue-req: 0,
[2026-01-05 18:12:56] Dump 00288th pass to TP0_PP0_Rank0_pid101005/Pass00288.pt
[2026-01-05 18:12:56] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▍| 189/200 [04:02<00:04,  2.30it/s]
[2026-01-05 18:12:56] Dump 00289th pass to TP0_PP0_Rank0_pid101005/Pass00289.pt
[2026-01-05 18:12:56] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
 95%|█████████▌| 190/200 [04:02<00:04,  2.45it/s]
[2026-01-05 18:12:56] Dump 00290th pass to TP0_PP0_Rank0_pid101005/Pass00290.pt
[2026-01-05 18:12:56] Dump 00291th pass to TP0_PP0_Rank0_pid101005/Pass00291.pt
[2026-01-05 18:12:56] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▌| 191/200 [04:02<00:03,  2.96it/s]
[2026-01-05 18:12:56] Dump 00292th pass to TP0_PP0_Rank0_pid101005/Pass00292.pt
[2026-01-05 18:12:56] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▌| 192/200 [04:03<00:02,  3.02it/s]
[2026-01-05 18:12:56] Dump 00293th pass to TP0_PP0_Rank0_pid101005/Pass00293.pt
[2026-01-05 18:12:57] Dump 00294th pass to TP0_PP0_Rank0_pid101005/Pass00294.pt
[2026-01-05 18:12:57] Dump 00295th pass to TP0_PP0_Rank0_pid101005/Pass00295.pt
[2026-01-05 18:12:57] Dump 00296th pass to TP0_PP0_Rank0_pid101005/Pass00296.pt
[2026-01-05 18:12:57] Dump 00297th pass to TP0_PP0_Rank0_pid101005/Pass00297.pt
[2026-01-05 18:12:57] Dump 00298th pass to TP0_PP0_Rank0_pid101005/Pass00298.pt
[2026-01-05 18:12:57] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▋| 193/200 [04:03<00:01,  3.60it/s]
[2026-01-05 18:12:57] Dump 00299th pass to TP0_PP0_Rank0_pid101005/Pass00299.pt
[2026-01-05 18:12:57] Dump 00300th pass to TP0_PP0_Rank0_pid101005/Pass00300.pt
[2026-01-05 18:12:57] Dump 00301th pass to TP0_PP0_Rank0_pid101005/Pass00301.pt
[2026-01-05 18:12:58] Dump 00302th pass to TP0_PP0_Rank0_pid101005/Pass00302.pt
[2026-01-05 18:12:58] Dump 00303th pass to TP0_PP0_Rank0_pid101005/Pass00303.pt
[2026-01-05 18:12:58] Dump 00304th pass to TP0_PP0_Rank0_pid101005/Pass00304.pt
[2026-01-05 18:12:58] Dump 00305th pass to TP0_PP0_Rank0_pid101005/Pass00305.pt
[2026-01-05 18:12:58] Dump 00306th pass to TP0_PP0_Rank0_pid101005/Pass00306.pt
[2026-01-05 18:12:58] Dump 00307th pass to TP0_PP0_Rank0_pid101005/Pass00307.pt
[2026-01-05 18:12:58] Dump 00308th pass to TP0_PP0_Rank0_pid101005/Pass00308.pt
[2026-01-05 18:12:58] Dump 00309th pass to TP0_PP0_Rank0_pid101005/Pass00309.pt
[2026-01-05 18:12:59] Dump 00310th pass to TP0_PP0_Rank0_pid101005/Pass00310.pt
[2026-01-05 18:12:59] Dump 00311th pass to TP0_PP0_Rank0_pid101005/Pass00311.pt
[2026-01-05 18:12:59] Dump 00312th pass to TP0_PP0_Rank0_pid101005/Pass00312.pt
[2026-01-05 18:12:59] Dump 00313th pass to TP0_PP0_Rank0_pid101005/Pass00313.pt
[2026-01-05 18:12:59] Dump 00314th pass to TP0_PP0_Rank0_pid101005/Pass00314.pt
[2026-01-05 18:12:59] Dump 00315th pass to TP0_PP0_Rank0_pid101005/Pass00315.pt
[2026-01-05 18:12:59] Dump 00316th pass to TP0_PP0_Rank0_pid101005/Pass00316.pt
[2026-01-05 18:13:00] Dump 00317th pass to TP0_PP0_Rank0_pid101005/Pass00317.pt
[2026-01-05 18:13:00] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
 97%|█████████▋| 194/200 [04:03<00:02,  2.30it/s]
[2026-01-05 18:13:00] Dump 00318th pass to TP0_PP0_Rank0_pid101005/Pass00318.pt
[2026-01-05 18:13:00] Dump 00319th pass to TP0_PP0_Rank0_pid101005/Pass00319.pt
[2026-01-05 18:13:00] Dump 00320th pass to TP0_PP0_Rank0_pid101005/Pass00320.pt
[2026-01-05 18:13:00] Dump 00321th pass to TP0_PP0_Rank0_pid101005/Pass00321.pt
[2026-01-05 18:13:00] Dump 00322th pass to TP0_PP0_Rank0_pid101005/Pass00322.pt
[2026-01-05 18:13:00] Dump 00323th pass to TP0_PP0_Rank0_pid101005/Pass00323.pt
[2026-01-05 18:13:00] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 195/200 [04:06<00:05,  1.03s/it]
[2026-01-05 18:13:00] Dump 00324th pass to TP0_PP0_Rank0_pid101005/Pass00324.pt
[2026-01-05 18:13:00] Dump 00325th pass to TP0_PP0_Rank0_pid101005/Pass00325.pt
[2026-01-05 18:13:01] Dump 00326th pass to TP0_PP0_Rank0_pid101005/Pass00326.pt
[2026-01-05 18:13:01] Dump 00327th pass to TP0_PP0_Rank0_pid101005/Pass00327.pt
[2026-01-05 18:13:01] Decode batch, #running-req: 4, #token: 1920, token usage: 0.00, cpu graph: False, gen throughput (token/s): 49.24, #queue-req: 0,
[2026-01-05 18:13:01] Dump 00328th pass to TP0_PP0_Rank0_pid101005/Pass00328.pt
[2026-01-05 18:13:01] Dump 00329th pass to TP0_PP0_Rank0_pid101005/Pass00329.pt
[2026-01-05 18:13:01] Dump 00330th pass to TP0_PP0_Rank0_pid101005/Pass00330.pt
[2026-01-05 18:13:01] Dump 00331th pass to TP0_PP0_Rank0_pid101005/Pass00331.pt
[2026-01-05 18:13:01] Dump 00332th pass to TP0_PP0_Rank0_pid101005/Pass00332.pt
[2026-01-05 18:13:01] Dump 00333th pass to TP0_PP0_Rank0_pid101005/Pass00333.pt
[2026-01-05 18:13:01] Dump 00334th pass to TP0_PP0_Rank0_pid101005/Pass00334.pt
[2026-01-05 18:13:02] Dump 00335th pass to TP0_PP0_Rank0_pid101005/Pass00335.pt
[2026-01-05 18:13:02] Dump 00336th pass to TP0_PP0_Rank0_pid101005/Pass00336.pt
[2026-01-05 18:13:02] Dump 00337th pass to TP0_PP0_Rank0_pid101005/Pass00337.pt
[2026-01-05 18:13:02] Dump 00338th pass to TP0_PP0_Rank0_pid101005/Pass00338.pt
[2026-01-05 18:13:02] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 196/200 [04:07<00:03,  1.07it/s]
[2026-01-05 18:13:02] Dump 00339th pass to TP0_PP0_Rank0_pid101005/Pass00339.pt
[2026-01-05 18:13:02] Dump 00340th pass to TP0_PP0_Rank0_pid101005/Pass00340.pt
[2026-01-05 18:13:02] Dump 00341th pass to TP0_PP0_Rank0_pid101005/Pass00341.pt
[2026-01-05 18:13:02] Dump 00342th pass to TP0_PP0_Rank0_pid101005/Pass00342.pt
[2026-01-05 18:13:02] Dump 00343th pass to TP0_PP0_Rank0_pid101005/Pass00343.pt
[2026-01-05 18:13:03] Dump 00344th pass to TP0_PP0_Rank0_pid101005/Pass00344.pt
[2026-01-05 18:13:03] Dump 00345th pass to TP0_PP0_Rank0_pid101005/Pass00345.pt
[2026-01-05 18:13:03] Dump 00346th pass to TP0_PP0_Rank0_pid101005/Pass00346.pt
[2026-01-05 18:13:03] Dump 00347th pass to TP0_PP0_Rank0_pid101005/Pass00347.pt
[2026-01-05 18:13:03] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 197/200 [04:08<00:03,  1.16s/it]
[2026-01-05 18:13:03] Dump 00348th pass to TP0_PP0_Rank0_pid101005/Pass00348.pt
[2026-01-05 18:13:03] Dump 00349th pass to TP0_PP0_Rank0_pid101005/Pass00349.pt
[2026-01-05 18:13:03] Dump 00350th pass to TP0_PP0_Rank0_pid101005/Pass00350.pt
[2026-01-05 18:13:03] Dump 00351th pass to TP0_PP0_Rank0_pid101005/Pass00351.pt
[2026-01-05 18:13:03] Dump 00352th pass to TP0_PP0_Rank0_pid101005/Pass00352.pt
[2026-01-05 18:13:04] Dump 00353th pass to TP0_PP0_Rank0_pid101005/Pass00353.pt
[2026-01-05 18:13:04] Dump 00354th pass to TP0_PP0_Rank0_pid101005/Pass00354.pt
[2026-01-05 18:13:04] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
 99%|█████████▉| 198/200 [04:09<00:02,  1.10s/it]
[2026-01-05 18:13:04] Dump 00355th pass to TP0_PP0_Rank0_pid101005/Pass00355.pt
[2026-01-05 18:13:04] Dump 00356th pass to TP0_PP0_Rank0_pid101005/Pass00356.pt
[2026-01-05 18:13:04] Dump 00357th pass to TP0_PP0_Rank0_pid101005/Pass00357.pt
[2026-01-05 18:13:04] Dump 00358th pass to TP0_PP0_Rank0_pid101005/Pass00358.pt
[2026-01-05 18:13:04] Dump 00359th pass to TP0_PP0_Rank0_pid101005/Pass00359.pt
[2026-01-05 18:13:04] Dump 00360th pass to TP0_PP0_Rank0_pid101005/Pass00360.pt
[2026-01-05 18:13:04] Dump 00361th pass to TP0_PP0_Rank0_pid101005/Pass00361.pt
[2026-01-05 18:13:04] Dump 00362th pass to TP0_PP0_Rank0_pid101005/Pass00362.pt
[2026-01-05 18:13:04] Dump 00363th pass to TP0_PP0_Rank0_pid101005/Pass00363.pt
[2026-01-05 18:13:04] Dump 00364th pass to TP0_PP0_Rank0_pid101005/Pass00364.pt
[2026-01-05 18:13:05] Dump 00365th pass to TP0_PP0_Rank0_pid101005/Pass00365.pt
[2026-01-05 18:13:05] Dump 00366th pass to TP0_PP0_Rank0_pid101005/Pass00366.pt
[2026-01-05 18:13:05] Dump 00367th pass to TP0_PP0_Rank0_pid101005/Pass00367.pt
[2026-01-05 18:13:05] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cpu graph: False, gen throughput (token/s): 25.11, #queue-req: 0,
[2026-01-05 18:13:05] Dump 00368th pass to TP0_PP0_Rank0_pid101005/Pass00368.pt
[2026-01-05 18:13:05] Dump 00369th pass to TP0_PP0_Rank0_pid101005/Pass00369.pt
[2026-01-05 18:13:05] Dump 00370th pass to TP0_PP0_Rank0_pid101005/Pass00370.pt
[2026-01-05 18:13:05] Dump 00371th pass to TP0_PP0_Rank0_pid101005/Pass00371.pt
[2026-01-05 18:13:05] Dump 00372th pass to TP0_PP0_Rank0_pid101005/Pass00372.pt
[2026-01-05 18:13:05] Dump 00373th pass to TP0_PP0_Rank0_pid101005/Pass00373.pt
[2026-01-05 18:13:05] Dump 00374th pass to TP0_PP0_Rank0_pid101005/Pass00374.pt
[2026-01-05 18:13:05] Dump 00375th pass to TP0_PP0_Rank0_pid101005/Pass00375.pt
[2026-01-05 18:13:06] Dump 00376th pass to TP0_PP0_Rank0_pid101005/Pass00376.pt
[2026-01-05 18:13:06] Dump 00377th pass to TP0_PP0_Rank0_pid101005/Pass00377.pt
[2026-01-05 18:13:06] Dump 00378th pass to TP0_PP0_Rank0_pid101005/Pass00378.pt
[2026-01-05 18:13:06] Dump 00379th pass to TP0_PP0_Rank0_pid101005/Pass00379.pt
[2026-01-05 18:13:06] Dump 00380th pass to TP0_PP0_Rank0_pid101005/Pass00380.pt
[2026-01-05 18:13:06] Dump 00381th pass to TP0_PP0_Rank0_pid101005/Pass00381.pt
[2026-01-05 18:13:06] Dump 00382th pass to TP0_PP0_Rank0_pid101005/Pass00382.pt
[2026-01-05 18:13:06] Dump 00383th pass to TP0_PP0_Rank0_pid101005/Pass00383.pt
[2026-01-05 18:13:06] Dump 00384th pass to TP0_PP0_Rank0_pid101005/Pass00384.pt
[2026-01-05 18:13:06] Dump 00385th pass to TP0_PP0_Rank0_pid101005/Pass00385.pt
[2026-01-05 18:13:06] Dump 00386th pass to TP0_PP0_Rank0_pid101005/Pass00386.pt
[2026-01-05 18:13:06] Dump 00387th pass to TP0_PP0_Rank0_pid101005/Pass00387.pt
[2026-01-05 18:13:07] Dump 00388th pass to TP0_PP0_Rank0_pid101005/Pass00388.pt
[2026-01-05 18:13:07] Dump 00389th pass to TP0_PP0_Rank0_pid101005/Pass00389.pt
[2026-01-05 18:13:07] Dump 00390th pass to TP0_PP0_Rank0_pid101005/Pass00390.pt
[2026-01-05 18:13:07] Dump 00391th pass to TP0_PP0_Rank0_pid101005/Pass00391.pt
[2026-01-05 18:13:07] Dump 00392th pass to TP0_PP0_Rank0_pid101005/Pass00392.pt
[2026-01-05 18:13:07] Dump 00393th pass to TP0_PP0_Rank0_pid101005/Pass00393.pt
[2026-01-05 18:13:07] Dump 00394th pass to TP0_PP0_Rank0_pid101005/Pass00394.pt
[2026-01-05 18:13:07] Dump 00395th pass to TP0_PP0_Rank0_pid101005/Pass00395.pt
[2026-01-05 18:13:07] Dump 00396th pass to TP0_PP0_Rank0_pid101005/Pass00396.pt
[2026-01-05 18:13:07] Dump 00397th pass to TP0_PP0_Rank0_pid101005/Pass00397.pt
[2026-01-05 18:13:07] Dump 00398th pass to TP0_PP0_Rank0_pid101005/Pass00398.pt
[2026-01-05 18:13:07] Dump 00399th pass to TP0_PP0_Rank0_pid101005/Pass00399.pt
[2026-01-05 18:13:08] Dump 00400th pass to TP0_PP0_Rank0_pid101005/Pass00400.pt
[2026-01-05 18:13:08] Dump 00401th pass to TP0_PP0_Rank0_pid101005/Pass00401.pt
[2026-01-05 18:13:08] Dump 00402th pass to TP0_PP0_Rank0_pid101005/Pass00402.pt
[2026-01-05 18:13:08] Dump 00403th pass to TP0_PP0_Rank0_pid101005/Pass00403.pt
[2026-01-05 18:13:08] Dump 00404th pass to TP0_PP0_Rank0_pid101005/Pass00404.pt
[2026-01-05 18:13:08] Dump 00405th pass to TP0_PP0_Rank0_pid101005/Pass00405.pt
[2026-01-05 18:13:08] Dump 00406th pass to TP0_PP0_Rank0_pid101005/Pass00406.pt
[2026-01-05 18:13:08] Dump 00407th pass to TP0_PP0_Rank0_pid101005/Pass00407.pt
[2026-01-05 18:13:08] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.66, #queue-req: 0,
[2026-01-05 18:13:08] Dump 00408th pass to TP0_PP0_Rank0_pid101005/Pass00408.pt
[2026-01-05 18:13:08] Dump 00409th pass to TP0_PP0_Rank0_pid101005/Pass00409.pt
[2026-01-05 18:13:08] Dump 00410th pass to TP0_PP0_Rank0_pid101005/Pass00410.pt
[2026-01-05 18:13:09] Dump 00411th pass to TP0_PP0_Rank0_pid101005/Pass00411.pt
[2026-01-05 18:13:09] Dump 00412th pass to TP0_PP0_Rank0_pid101005/Pass00412.pt
[2026-01-05 18:13:09] Dump 00413th pass to TP0_PP0_Rank0_pid101005/Pass00413.pt
[2026-01-05 18:13:09] Dump 00414th pass to TP0_PP0_Rank0_pid101005/Pass00414.pt
[2026-01-05 18:13:09] Dump 00415th pass to TP0_PP0_Rank0_pid101005/Pass00415.pt
[2026-01-05 18:13:09] Dump 00416th pass to TP0_PP0_Rank0_pid101005/Pass00416.pt
[2026-01-05 18:13:09] Dump 00417th pass to TP0_PP0_Rank0_pid101005/Pass00417.pt
[2026-01-05 18:13:09] Dump 00418th pass to TP0_PP0_Rank0_pid101005/Pass00418.pt
[2026-01-05 18:13:09] Dump 00419th pass to TP0_PP0_Rank0_pid101005/Pass00419.pt
[2026-01-05 18:13:09] Dump 00420th pass to TP0_PP0_Rank0_pid101005/Pass00420.pt
[2026-01-05 18:13:09] Dump 00421th pass to TP0_PP0_Rank0_pid101005/Pass00421.pt
[2026-01-05 18:13:09] Dump 00422th pass to TP0_PP0_Rank0_pid101005/Pass00422.pt
[2026-01-05 18:13:10] Dump 00423th pass to TP0_PP0_Rank0_pid101005/Pass00423.pt
[2026-01-05 18:13:10] Dump 00424th pass to TP0_PP0_Rank0_pid101005/Pass00424.pt
[2026-01-05 18:13:10] Dump 00425th pass to TP0_PP0_Rank0_pid101005/Pass00425.pt
[2026-01-05 18:13:10] Dump 00426th pass to TP0_PP0_Rank0_pid101005/Pass00426.pt
[2026-01-05 18:13:10] Dump 00427th pass to TP0_PP0_Rank0_pid101005/Pass00427.pt
[2026-01-05 18:13:10] Dump 00428th pass to TP0_PP0_Rank0_pid101005/Pass00428.pt
[2026-01-05 18:13:10] Dump 00429th pass to TP0_PP0_Rank0_pid101005/Pass00429.pt
[2026-01-05 18:13:10] Dump 00430th pass to TP0_PP0_Rank0_pid101005/Pass00430.pt
[2026-01-05 18:13:10] Dump 00431th pass to TP0_PP0_Rank0_pid101005/Pass00431.pt
[2026-01-05 18:13:10] Dump 00432th pass to TP0_PP0_Rank0_pid101005/Pass00432.pt
[2026-01-05 18:13:10] Dump 00433th pass to TP0_PP0_Rank0_pid101005/Pass00433.pt
[2026-01-05 18:13:11] Dump 00434th pass to TP0_PP0_Rank0_pid101005/Pass00434.pt
[2026-01-05 18:13:11] Dump 00435th pass to TP0_PP0_Rank0_pid101005/Pass00435.pt
[2026-01-05 18:13:11] Dump 00436th pass to TP0_PP0_Rank0_pid101005/Pass00436.pt
[2026-01-05 18:13:11] Dump 00437th pass to TP0_PP0_Rank0_pid101005/Pass00437.pt
[2026-01-05 18:13:11] Dump 00438th pass to TP0_PP0_Rank0_pid101005/Pass00438.pt
[2026-01-05 18:13:11] Dump 00439th pass to TP0_PP0_Rank0_pid101005/Pass00439.pt
[2026-01-05 18:13:11] Dump 00440th pass to TP0_PP0_Rank0_pid101005/Pass00440.pt
[2026-01-05 18:13:11] Dump 00441th pass to TP0_PP0_Rank0_pid101005/Pass00441.pt
[2026-01-05 18:13:11] Dump 00442th pass to TP0_PP0_Rank0_pid101005/Pass00442.pt
[2026-01-05 18:13:11] Dump 00443th pass to TP0_PP0_Rank0_pid101005/Pass00443.pt
[2026-01-05 18:13:11] Dump 00444th pass to TP0_PP0_Rank0_pid101005/Pass00444.pt
[2026-01-05 18:13:11] Dump 00445th pass to TP0_PP0_Rank0_pid101005/Pass00445.pt
[2026-01-05 18:13:12] Dump 00446th pass to TP0_PP0_Rank0_pid101005/Pass00446.pt
[2026-01-05 18:13:12] Dump 00447th pass to TP0_PP0_Rank0_pid101005/Pass00447.pt
[2026-01-05 18:13:12] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.71, #queue-req: 0,
[2026-01-05 18:13:12] Dump 00448th pass to TP0_PP0_Rank0_pid101005/Pass00448.pt
[2026-01-05 18:13:12] Dump 00449th pass to TP0_PP0_Rank0_pid101005/Pass00449.pt
[2026-01-05 18:13:12] Dump 00450th pass to TP0_PP0_Rank0_pid101005/Pass00450.pt
[2026-01-05 18:13:12] Dump 00451th pass to TP0_PP0_Rank0_pid101005/Pass00451.pt
[2026-01-05 18:13:12] Dump 00452th pass to TP0_PP0_Rank0_pid101005/Pass00452.pt
[2026-01-05 18:13:12] Dump 00453th pass to TP0_PP0_Rank0_pid101005/Pass00453.pt
[2026-01-05 18:13:12] Dump 00454th pass to TP0_PP0_Rank0_pid101005/Pass00454.pt
[2026-01-05 18:13:12] Dump 00455th pass to TP0_PP0_Rank0_pid101005/Pass00455.pt
[2026-01-05 18:13:12] Dump 00456th pass to TP0_PP0_Rank0_pid101005/Pass00456.pt
[2026-01-05 18:13:12] Dump 00457th pass to TP0_PP0_Rank0_pid101005/Pass00457.pt
[2026-01-05 18:13:13] Dump 00458th pass to TP0_PP0_Rank0_pid101005/Pass00458.pt
[2026-01-05 18:13:13] Dump 00459th pass to TP0_PP0_Rank0_pid101005/Pass00459.pt
[2026-01-05 18:13:13] Dump 00460th pass to TP0_PP0_Rank0_pid101005/Pass00460.pt
[2026-01-05 18:13:13] Dump 00461th pass to TP0_PP0_Rank0_pid101005/Pass00461.pt
[2026-01-05 18:13:13] Dump 00462th pass to TP0_PP0_Rank0_pid101005/Pass00462.pt
[2026-01-05 18:13:13] Dump 00463th pass to TP0_PP0_Rank0_pid101005/Pass00463.pt
[2026-01-05 18:13:13] Dump 00464th pass to TP0_PP0_Rank0_pid101005/Pass00464.pt
[2026-01-05 18:13:13] Dump 00465th pass to TP0_PP0_Rank0_pid101005/Pass00465.pt
[2026-01-05 18:13:13] Dump 00466th pass to TP0_PP0_Rank0_pid101005/Pass00466.pt
[2026-01-05 18:13:13] Dump 00467th pass to TP0_PP0_Rank0_pid101005/Pass00467.pt
[2026-01-05 18:13:13] Dump 00468th pass to TP0_PP0_Rank0_pid101005/Pass00468.pt
[2026-01-05 18:13:13] Dump 00469th pass to TP0_PP0_Rank0_pid101005/Pass00469.pt
[2026-01-05 18:13:14] Dump 00470th pass to TP0_PP0_Rank0_pid101005/Pass00470.pt
[2026-01-05 18:13:14] Dump 00471th pass to TP0_PP0_Rank0_pid101005/Pass00471.pt
[2026-01-05 18:13:14] Dump 00472th pass to TP0_PP0_Rank0_pid101005/Pass00472.pt
[2026-01-05 18:13:14] Dump 00473th pass to TP0_PP0_Rank0_pid101005/Pass00473.pt
[2026-01-05 18:13:14] Dump 00474th pass to TP0_PP0_Rank0_pid101005/Pass00474.pt
[2026-01-05 18:13:14] Dump 00475th pass to TP0_PP0_Rank0_pid101005/Pass00475.pt
[2026-01-05 18:13:14] Dump 00476th pass to TP0_PP0_Rank0_pid101005/Pass00476.pt
[2026-01-05 18:13:14] Dump 00477th pass to TP0_PP0_Rank0_pid101005/Pass00477.pt
[2026-01-05 18:13:14] Dump 00478th pass to TP0_PP0_Rank0_pid101005/Pass00478.pt
[2026-01-05 18:13:14] Dump 00479th pass to TP0_PP0_Rank0_pid101005/Pass00479.pt
[2026-01-05 18:13:14] Dump 00480th pass to TP0_PP0_Rank0_pid101005/Pass00480.pt
[2026-01-05 18:13:15] Dump 00481th pass to TP0_PP0_Rank0_pid101005/Pass00481.pt
[2026-01-05 18:13:15] Dump 00482th pass to TP0_PP0_Rank0_pid101005/Pass00482.pt
[2026-01-05 18:13:15] Dump 00483th pass to TP0_PP0_Rank0_pid101005/Pass00483.pt
[2026-01-05 18:13:15] Dump 00484th pass to TP0_PP0_Rank0_pid101005/Pass00484.pt
[2026-01-05 18:13:15] Dump 00485th pass to TP0_PP0_Rank0_pid101005/Pass00485.pt
[2026-01-05 18:13:15] Dump 00486th pass to TP0_PP0_Rank0_pid101005/Pass00486.pt
[2026-01-05 18:13:15] Dump 00487th pass to TP0_PP0_Rank0_pid101005/Pass00487.pt
[2026-01-05 18:13:15] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.64, #queue-req: 0,
[2026-01-05 18:13:15] Dump 00488th pass to TP0_PP0_Rank0_pid101005/Pass00488.pt
[2026-01-05 18:13:15] Dump 00489th pass to TP0_PP0_Rank0_pid101005/Pass00489.pt
[2026-01-05 18:13:15] Dump 00490th pass to TP0_PP0_Rank0_pid101005/Pass00490.pt
[2026-01-05 18:13:15] Dump 00491th pass to TP0_PP0_Rank0_pid101005/Pass00491.pt
[2026-01-05 18:13:15] Dump 00492th pass to TP0_PP0_Rank0_pid101005/Pass00492.pt
[2026-01-05 18:13:16] Dump 00493th pass to TP0_PP0_Rank0_pid101005/Pass00493.pt
[2026-01-05 18:13:16] Dump 00494th pass to TP0_PP0_Rank0_pid101005/Pass00494.pt
[2026-01-05 18:13:16] Dump 00495th pass to TP0_PP0_Rank0_pid101005/Pass00495.pt
[2026-01-05 18:13:16] Dump 00496th pass to TP0_PP0_Rank0_pid101005/Pass00496.pt
[2026-01-05 18:13:16] Dump 00497th pass to TP0_PP0_Rank0_pid101005/Pass00497.pt
[2026-01-05 18:13:16] Dump 00498th pass to TP0_PP0_Rank0_pid101005/Pass00498.pt
[2026-01-05 18:13:16] Dump 00499th pass to TP0_PP0_Rank0_pid101005/Pass00499.pt
[2026-01-05 18:13:16] Dump 00500th pass to TP0_PP0_Rank0_pid101005/Pass00500.pt
[2026-01-05 18:13:16] Dump 00501th pass to TP0_PP0_Rank0_pid101005/Pass00501.pt
[2026-01-05 18:13:16] Dump 00502th pass to TP0_PP0_Rank0_pid101005/Pass00502.pt
[2026-01-05 18:13:16] Dump 00503th pass to TP0_PP0_Rank0_pid101005/Pass00503.pt
[2026-01-05 18:13:16] Dump 00504th pass to TP0_PP0_Rank0_pid101005/Pass00504.pt
[2026-01-05 18:13:17] Dump 00505th pass to TP0_PP0_Rank0_pid101005/Pass00505.pt
[2026-01-05 18:13:17] Dump 00506th pass to TP0_PP0_Rank0_pid101005/Pass00506.pt
[2026-01-05 18:13:17] Dump 00507th pass to TP0_PP0_Rank0_pid101005/Pass00507.pt
[2026-01-05 18:13:17] Dump 00508th pass to TP0_PP0_Rank0_pid101005/Pass00508.pt
[2026-01-05 18:13:17] Dump 00509th pass to TP0_PP0_Rank0_pid101005/Pass00509.pt
[2026-01-05 18:13:17] Dump 00510th pass to TP0_PP0_Rank0_pid101005/Pass00510.pt
[2026-01-05 18:13:17] Dump 00511th pass to TP0_PP0_Rank0_pid101005/Pass00511.pt
[2026-01-05 18:13:17] Dump 00512th pass to TP0_PP0_Rank0_pid101005/Pass00512.pt
[2026-01-05 18:13:17] Dump 00513th pass to TP0_PP0_Rank0_pid101005/Pass00513.pt
[2026-01-05 18:13:17] Dump 00514th pass to TP0_PP0_Rank0_pid101005/Pass00514.pt
[2026-01-05 18:13:17] Dump 00515th pass to TP0_PP0_Rank0_pid101005/Pass00515.pt
[2026-01-05 18:13:17] Dump 00516th pass to TP0_PP0_Rank0_pid101005/Pass00516.pt
[2026-01-05 18:13:18] Dump 00517th pass to TP0_PP0_Rank0_pid101005/Pass00517.pt
[2026-01-05 18:13:18] Dump 00518th pass to TP0_PP0_Rank0_pid101005/Pass00518.pt
[2026-01-05 18:13:18] Dump 00519th pass to TP0_PP0_Rank0_pid101005/Pass00519.pt
[2026-01-05 18:13:18] Dump 00520th pass to TP0_PP0_Rank0_pid101005/Pass00520.pt
[2026-01-05 18:13:18] Dump 00521th pass to TP0_PP0_Rank0_pid101005/Pass00521.pt
[2026-01-05 18:13:18] Dump 00522th pass to TP0_PP0_Rank0_pid101005/Pass00522.pt
[2026-01-05 18:13:18] Dump 00523th pass to TP0_PP0_Rank0_pid101005/Pass00523.pt
[2026-01-05 18:13:18] Dump 00524th pass to TP0_PP0_Rank0_pid101005/Pass00524.pt
[2026-01-05 18:13:18] Dump 00525th pass to TP0_PP0_Rank0_pid101005/Pass00525.pt
[2026-01-05 18:13:18] Dump 00526th pass to TP0_PP0_Rank0_pid101005/Pass00526.pt
[2026-01-05 18:13:18] Dump 00527th pass to TP0_PP0_Rank0_pid101005/Pass00527.pt
[2026-01-05 18:13:18] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.79, #queue-req: 0,
[2026-01-05 18:13:19] Dump 00528th pass to TP0_PP0_Rank0_pid101005/Pass00528.pt
[2026-01-05 18:13:19] Dump 00529th pass to TP0_PP0_Rank0_pid101005/Pass00529.pt
[2026-01-05 18:13:19] Dump 00530th pass to TP0_PP0_Rank0_pid101005/Pass00530.pt
[2026-01-05 18:13:19] Dump 00531th pass to TP0_PP0_Rank0_pid101005/Pass00531.pt
[2026-01-05 18:13:19] Dump 00532th pass to TP0_PP0_Rank0_pid101005/Pass00532.pt
[2026-01-05 18:13:19] Dump 00533th pass to TP0_PP0_Rank0_pid101005/Pass00533.pt
[2026-01-05 18:13:19] Dump 00534th pass to TP0_PP0_Rank0_pid101005/Pass00534.pt
[2026-01-05 18:13:19] Dump 00535th pass to TP0_PP0_Rank0_pid101005/Pass00535.pt
[2026-01-05 18:13:19] Dump 00536th pass to TP0_PP0_Rank0_pid101005/Pass00536.pt
[2026-01-05 18:13:19] Dump 00537th pass to TP0_PP0_Rank0_pid101005/Pass00537.pt
[2026-01-05 18:13:19] Dump 00538th pass to TP0_PP0_Rank0_pid101005/Pass00538.pt
[2026-01-05 18:13:19] Dump 00539th pass to TP0_PP0_Rank0_pid101005/Pass00539.pt
[2026-01-05 18:13:20] Dump 00540th pass to TP0_PP0_Rank0_pid101005/Pass00540.pt
[2026-01-05 18:13:20] Dump 00541th pass to TP0_PP0_Rank0_pid101005/Pass00541.pt
[2026-01-05 18:13:20] Dump 00542th pass to TP0_PP0_Rank0_pid101005/Pass00542.pt
[2026-01-05 18:13:20] Dump 00543th pass to TP0_PP0_Rank0_pid101005/Pass00543.pt
[2026-01-05 18:13:20] Dump 00544th pass to TP0_PP0_Rank0_pid101005/Pass00544.pt
[2026-01-05 18:13:20] Dump 00545th pass to TP0_PP0_Rank0_pid101005/Pass00545.pt
[2026-01-05 18:13:20] Dump 00546th pass to TP0_PP0_Rank0_pid101005/Pass00546.pt
[2026-01-05 18:13:20] Dump 00547th pass to TP0_PP0_Rank0_pid101005/Pass00547.pt
[2026-01-05 18:13:20] Dump 00548th pass to TP0_PP0_Rank0_pid101005/Pass00548.pt
[2026-01-05 18:13:20] Dump 00549th pass to TP0_PP0_Rank0_pid101005/Pass00549.pt
[2026-01-05 18:13:20] Dump 00550th pass to TP0_PP0_Rank0_pid101005/Pass00550.pt
[2026-01-05 18:13:20] Dump 00551th pass to TP0_PP0_Rank0_pid101005/Pass00551.pt
[2026-01-05 18:13:21] Dump 00552th pass to TP0_PP0_Rank0_pid101005/Pass00552.pt
[2026-01-05 18:13:21] Dump 00553th pass to TP0_PP0_Rank0_pid101005/Pass00553.pt
[2026-01-05 18:13:21] Dump 00554th pass to TP0_PP0_Rank0_pid101005/Pass00554.pt
[2026-01-05 18:13:21] Dump 00555th pass to TP0_PP0_Rank0_pid101005/Pass00555.pt
[2026-01-05 18:13:21] Dump 00556th pass to TP0_PP0_Rank0_pid101005/Pass00556.pt
[2026-01-05 18:13:21] Dump 00557th pass to TP0_PP0_Rank0_pid101005/Pass00557.pt
[2026-01-05 18:13:21] Dump 00558th pass to TP0_PP0_Rank0_pid101005/Pass00558.pt
[2026-01-05 18:13:21] Dump 00559th pass to TP0_PP0_Rank0_pid101005/Pass00559.pt
[2026-01-05 18:13:21] Dump 00560th pass to TP0_PP0_Rank0_pid101005/Pass00560.pt
[2026-01-05 18:13:21] Dump 00561th pass to TP0_PP0_Rank0_pid101005/Pass00561.pt
[2026-01-05 18:13:21] Dump 00562th pass to TP0_PP0_Rank0_pid101005/Pass00562.pt
[2026-01-05 18:13:21] Dump 00563th pass to TP0_PP0_Rank0_pid101005/Pass00563.pt
[2026-01-05 18:13:22] Dump 00564th pass to TP0_PP0_Rank0_pid101005/Pass00564.pt
[2026-01-05 18:13:22] Dump 00565th pass to TP0_PP0_Rank0_pid101005/Pass00565.pt
[2026-01-05 18:13:22] Dump 00566th pass to TP0_PP0_Rank0_pid101005/Pass00566.pt
[2026-01-05 18:13:22] Dump 00567th pass to TP0_PP0_Rank0_pid101005/Pass00567.pt
[2026-01-05 18:13:22] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.87, #queue-req: 0,
[2026-01-05 18:13:22] Dump 00568th pass to TP0_PP0_Rank0_pid101005/Pass00568.pt
[2026-01-05 18:13:22] Dump 00569th pass to TP0_PP0_Rank0_pid101005/Pass00569.pt
[2026-01-05 18:13:22] Dump 00570th pass to TP0_PP0_Rank0_pid101005/Pass00570.pt
[2026-01-05 18:13:22] Dump 00571th pass to TP0_PP0_Rank0_pid101005/Pass00571.pt
[2026-01-05 18:13:22] Dump 00572th pass to TP0_PP0_Rank0_pid101005/Pass00572.pt
[2026-01-05 18:13:22] Dump 00573th pass to TP0_PP0_Rank0_pid101005/Pass00573.pt
[2026-01-05 18:13:22] Dump 00574th pass to TP0_PP0_Rank0_pid101005/Pass00574.pt
[2026-01-05 18:13:22] Dump 00575th pass to TP0_PP0_Rank0_pid101005/Pass00575.pt
[2026-01-05 18:13:23] Dump 00576th pass to TP0_PP0_Rank0_pid101005/Pass00576.pt
[2026-01-05 18:13:23] Dump 00577th pass to TP0_PP0_Rank0_pid101005/Pass00577.pt
[2026-01-05 18:13:23] Dump 00578th pass to TP0_PP0_Rank0_pid101005/Pass00578.pt
[2026-01-05 18:13:23] Dump 00579th pass to TP0_PP0_Rank0_pid101005/Pass00579.pt
[2026-01-05 18:13:23] Dump 00580th pass to TP0_PP0_Rank0_pid101005/Pass00580.pt
[2026-01-05 18:13:23] Dump 00581th pass to TP0_PP0_Rank0_pid101005/Pass00581.pt
[2026-01-05 18:13:23] Dump 00582th pass to TP0_PP0_Rank0_pid101005/Pass00582.pt
[2026-01-05 18:13:23] Dump 00583th pass to TP0_PP0_Rank0_pid101005/Pass00583.pt
[2026-01-05 18:13:23] Dump 00584th pass to TP0_PP0_Rank0_pid101005/Pass00584.pt
[2026-01-05 18:13:23] Dump 00585th pass to TP0_PP0_Rank0_pid101005/Pass00585.pt
[2026-01-05 18:13:23] Dump 00586th pass to TP0_PP0_Rank0_pid101005/Pass00586.pt
[2026-01-05 18:13:23] Dump 00587th pass to TP0_PP0_Rank0_pid101005/Pass00587.pt
[2026-01-05 18:13:24] Dump 00588th pass to TP0_PP0_Rank0_pid101005/Pass00588.pt
[2026-01-05 18:13:24] Dump 00589th pass to TP0_PP0_Rank0_pid101005/Pass00589.pt
[2026-01-05 18:13:24] Dump 00590th pass to TP0_PP0_Rank0_pid101005/Pass00590.pt
[2026-01-05 18:13:24] Dump 00591th pass to TP0_PP0_Rank0_pid101005/Pass00591.pt
[2026-01-05 18:13:24] Dump 00592th pass to TP0_PP0_Rank0_pid101005/Pass00592.pt
[2026-01-05 18:13:24] Dump 00593th pass to TP0_PP0_Rank0_pid101005/Pass00593.pt
[2026-01-05 18:13:24] Dump 00594th pass to TP0_PP0_Rank0_pid101005/Pass00594.pt
[2026-01-05 18:13:24] Dump 00595th pass to TP0_PP0_Rank0_pid101005/Pass00595.pt
[2026-01-05 18:13:24] Dump 00596th pass to TP0_PP0_Rank0_pid101005/Pass00596.pt
[2026-01-05 18:13:24] Dump 00597th pass to TP0_PP0_Rank0_pid101005/Pass00597.pt
[2026-01-05 18:13:24] Dump 00598th pass to TP0_PP0_Rank0_pid101005/Pass00598.pt
[2026-01-05 18:13:24] Dump 00599th pass to TP0_PP0_Rank0_pid101005/Pass00599.pt
[2026-01-05 18:13:25] Dump 00600th pass to TP0_PP0_Rank0_pid101005/Pass00600.pt
[2026-01-05 18:13:25] Dump 00601th pass to TP0_PP0_Rank0_pid101005/Pass00601.pt
[2026-01-05 18:13:25] Dump 00602th pass to TP0_PP0_Rank0_pid101005/Pass00602.pt
[2026-01-05 18:13:25] Dump 00603th pass to TP0_PP0_Rank0_pid101005/Pass00603.pt
[2026-01-05 18:13:25] Dump 00604th pass to TP0_PP0_Rank0_pid101005/Pass00604.pt
[2026-01-05 18:13:25] Dump 00605th pass to TP0_PP0_Rank0_pid101005/Pass00605.pt
[2026-01-05 18:13:25] Dump 00606th pass to TP0_PP0_Rank0_pid101005/Pass00606.pt
[2026-01-05 18:13:25] Dump 00607th pass to TP0_PP0_Rank0_pid101005/Pass00607.pt
[2026-01-05 18:13:25] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.88, #queue-req: 0,
[2026-01-05 18:13:25] Dump 00608th pass to TP0_PP0_Rank0_pid101005/Pass00608.pt
[2026-01-05 18:13:25] Dump 00609th pass to TP0_PP0_Rank0_pid101005/Pass00609.pt
[2026-01-05 18:13:25] Dump 00610th pass to TP0_PP0_Rank0_pid101005/Pass00610.pt
[2026-01-05 18:13:25] Dump 00611th pass to TP0_PP0_Rank0_pid101005/Pass00611.pt
[2026-01-05 18:13:26] Dump 00612th pass to TP0_PP0_Rank0_pid101005/Pass00612.pt
[2026-01-05 18:13:26] Dump 00613th pass to TP0_PP0_Rank0_pid101005/Pass00613.pt
[2026-01-05 18:13:26] Dump 00614th pass to TP0_PP0_Rank0_pid101005/Pass00614.pt
[2026-01-05 18:13:26] Dump 00615th pass to TP0_PP0_Rank0_pid101005/Pass00615.pt
[2026-01-05 18:13:26] Dump 00616th pass to TP0_PP0_Rank0_pid101005/Pass00616.pt
[2026-01-05 18:13:26] Dump 00617th pass to TP0_PP0_Rank0_pid101005/Pass00617.pt
[2026-01-05 18:13:26] Dump 00618th pass to TP0_PP0_Rank0_pid101005/Pass00618.pt
[2026-01-05 18:13:26] Dump 00619th pass to TP0_PP0_Rank0_pid101005/Pass00619.pt
[2026-01-05 18:13:26] Dump 00620th pass to TP0_PP0_Rank0_pid101005/Pass00620.pt
[2026-01-05 18:13:26] Dump 00621th pass to TP0_PP0_Rank0_pid101005/Pass00621.pt
[2026-01-05 18:13:26] Dump 00622th pass to TP0_PP0_Rank0_pid101005/Pass00622.pt
[2026-01-05 18:13:26] Dump 00623th pass to TP0_PP0_Rank0_pid101005/Pass00623.pt
[2026-01-05 18:13:27] Dump 00624th pass to TP0_PP0_Rank0_pid101005/Pass00624.pt
[2026-01-05 18:13:27] Dump 00625th pass to TP0_PP0_Rank0_pid101005/Pass00625.pt
[2026-01-05 18:13:27] Dump 00626th pass to TP0_PP0_Rank0_pid101005/Pass00626.pt
[2026-01-05 18:13:27] Dump 00627th pass to TP0_PP0_Rank0_pid101005/Pass00627.pt
[2026-01-05 18:13:27] Dump 00628th pass to TP0_PP0_Rank0_pid101005/Pass00628.pt
[2026-01-05 18:13:27] Dump 00629th pass to TP0_PP0_Rank0_pid101005/Pass00629.pt
[2026-01-05 18:13:27] Dump 00630th pass to TP0_PP0_Rank0_pid101005/Pass00630.pt
[2026-01-05 18:13:27] Dump 00631th pass to TP0_PP0_Rank0_pid101005/Pass00631.pt
[2026-01-05 18:13:27] Dump 00632th pass to TP0_PP0_Rank0_pid101005/Pass00632.pt
[2026-01-05 18:13:27] Dump 00633th pass to TP0_PP0_Rank0_pid101005/Pass00633.pt
[2026-01-05 18:13:27] Dump 00634th pass to TP0_PP0_Rank0_pid101005/Pass00634.pt
[2026-01-05 18:13:28] Dump 00635th pass to TP0_PP0_Rank0_pid101005/Pass00635.pt
[2026-01-05 18:13:28] Dump 00636th pass to TP0_PP0_Rank0_pid101005/Pass00636.pt
[2026-01-05 18:13:28] Dump 00637th pass to TP0_PP0_Rank0_pid101005/Pass00637.pt
[2026-01-05 18:13:28] Dump 00638th pass to TP0_PP0_Rank0_pid101005/Pass00638.pt
[2026-01-05 18:13:28] Dump 00639th pass to TP0_PP0_Rank0_pid101005/Pass00639.pt
[2026-01-05 18:13:28] Dump 00640th pass to TP0_PP0_Rank0_pid101005/Pass00640.pt
[2026-01-05 18:13:28] Dump 00641th pass to TP0_PP0_Rank0_pid101005/Pass00641.pt
[2026-01-05 18:13:28] Dump 00642th pass to TP0_PP0_Rank0_pid101005/Pass00642.pt
[2026-01-05 18:13:28] Dump 00643th pass to TP0_PP0_Rank0_pid101005/Pass00643.pt
[2026-01-05 18:13:28] Dump 00644th pass to TP0_PP0_Rank0_pid101005/Pass00644.pt
[2026-01-05 18:13:28] Dump 00645th pass to TP0_PP0_Rank0_pid101005/Pass00645.pt
[2026-01-05 18:13:28] Dump 00646th pass to TP0_PP0_Rank0_pid101005/Pass00646.pt
[2026-01-05 18:13:29] Dump 00647th pass to TP0_PP0_Rank0_pid101005/Pass00647.pt
[2026-01-05 18:13:29] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.89, #queue-req: 0,
[2026-01-05 18:13:29] Dump 00648th pass to TP0_PP0_Rank0_pid101005/Pass00648.pt
[2026-01-05 18:13:29] Dump 00649th pass to TP0_PP0_Rank0_pid101005/Pass00649.pt
[2026-01-05 18:13:29] Dump 00650th pass to TP0_PP0_Rank0_pid101005/Pass00650.pt
[2026-01-05 18:13:29] Dump 00651th pass to TP0_PP0_Rank0_pid101005/Pass00651.pt
[2026-01-05 18:13:29] Dump 00652th pass to TP0_PP0_Rank0_pid101005/Pass00652.pt
[2026-01-05 18:13:29] Dump 00653th pass to TP0_PP0_Rank0_pid101005/Pass00653.pt
[2026-01-05 18:13:29] Dump 00654th pass to TP0_PP0_Rank0_pid101005/Pass00654.pt
[2026-01-05 18:13:29] Dump 00655th pass to TP0_PP0_Rank0_pid101005/Pass00655.pt
[2026-01-05 18:13:29] Dump 00656th pass to TP0_PP0_Rank0_pid101005/Pass00656.pt
[2026-01-05 18:13:29] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
100%|█████████▉| 199/200 [04:10<00:00,  1.02it/s]
100%|██████████| 200/200 [04:36<00:00,  8.38s/it]
100%|██████████| 200/200 [04:36<00:00,  1.38s/it]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_ascend_pp_single_node.py", line 50, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.335) not greater than 0.38 : Accuracy of /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B is 0.335, is lower than 0.38
E
======================================================================
ERROR: test_gsm8k (__main__.TestQwenPPTieWeightsAccuracy.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.335) not greater than 0.38 : Accuracy of /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B is 0.335, is lower than 0.38

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 280.295s

FAILED (errors=1)
Accuracy: 0.335
Invalid: 0.005
Latency: 279.216 s
Output throughput: 62.246 token/s
.
.
End (16/106):
filename='ascend/function/test_ascend_pp_single_node.py', elapsed=299, estimated_time=400
.
.


[CI Retry] ascend/function/test_ascend_pp_single_node.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/function/test_ascend_pp_single_node.py

.
.
Begin (16/106):
python3 /data/l30079981/260105/ascend/function/test_ascend_pp_single_node.py
.
.

[2026-01-05 18:14:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:14:48] Dump 00657th pass to TP0_PP0_Rank0_pid101005/Pass00657.pt
[2026-01-05 18:14:48] Dump 00658th pass to TP0_PP0_Rank0_pid101005/Pass00658.pt
[2026-01-05 18:14:49] INFO:     127.0.0.1:33290 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 18:14:49] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 18:14:49] INFO:     127.0.0.1:33292 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 18:14:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:14:49] Dump 00659th pass to TP0_PP0_Rank0_pid101005/Pass00659.pt
[2026-01-05 18:14:49] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B --chunked-prefill-size 256 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwenPPTieWeightsAccuracy.test_gsm8k

[2026-01-05 18:14:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:14:49] Dump 00660th pass to TP0_PP0_Rank0_pid101005/Pass00660.pt
[2026-01-05 18:14:50] Prefill batch, #new-seq: 64, #new-token: 8192, #cached-token: 41472, token usage: 0.00, #running-req: 1, #queue-req: 63,
[2026-01-05 18:14:51] Dump 00661th pass to TP0_PP0_Rank0_pid101005/Pass00661.pt
  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-05-18:15:07 (PID:104802, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
[2026-01-05 18:15:10] Prefill batch, #new-seq: 63, #new-token: 8064, #cached-token: 40576, token usage: 0.01, #running-req: 65, #queue-req: 0,
[2026-01-05 18:15:12] Dump 00662th pass to TP0_PP0_Rank0_pid101005/Pass00662.pt
[2026-01-05 18:15:34] Dump 00663th pass to TP0_PP0_Rank0_pid101005/Pass00663.pt
[2026-01-05 18:15:35] Dump 00664th pass to TP0_PP0_Rank0_pid101005/Pass00664.pt
[2026-01-05 18:15:36] Dump 00665th pass to TP0_PP0_Rank0_pid101005/Pass00665.pt
[2026-01-05 18:15:37] Dump 00666th pass to TP0_PP0_Rank0_pid101005/Pass00666.pt
[2026-01-05 18:15:38] Dump 00667th pass to TP0_PP0_Rank0_pid101005/Pass00667.pt
[2026-01-05 18:15:40] Dump 00668th pass to TP0_PP0_Rank0_pid101005/Pass00668.pt
[2026-01-05 18:15:41] Dump 00669th pass to TP0_PP0_Rank0_pid101005/Pass00669.pt
[2026-01-05 18:15:42] Dump 00670th pass to TP0_PP0_Rank0_pid101005/Pass00670.pt
[2026-01-05 18:15:43] Dump 00671th pass to TP0_PP0_Rank0_pid101005/Pass00671.pt
[2026-01-05 18:15:44] Dump 00672th pass to TP0_PP0_Rank0_pid101005/Pass00672.pt
[2026-01-05 18:15:45] Dump 00673th pass to TP0_PP0_Rank0_pid101005/Pass00673.pt
[2026-01-05 18:15:46] Dump 00674th pass to TP0_PP0_Rank0_pid101005/Pass00674.pt
[2026-01-05 18:15:47] Dump 00675th pass to TP0_PP0_Rank0_pid101005/Pass00675.pt
[2026-01-05 18:15:48] Dump 00676th pass to TP0_PP0_Rank0_pid101005/Pass00676.pt
[2026-01-05 18:15:49] Dump 00677th pass to TP0_PP0_Rank0_pid101005/Pass00677.pt
[2026-01-05 18:15:50] Dump 00678th pass to TP0_PP0_Rank0_pid101005/Pass00678.pt
[2026-01-05 18:15:52] Dump 00679th pass to TP0_PP0_Rank0_pid101005/Pass00679.pt
[2026-01-05 18:15:53] Dump 00680th pass to TP0_PP0_Rank0_pid101005/Pass00680.pt
[2026-01-05 18:15:54] Dump 00681th pass to TP0_PP0_Rank0_pid101005/Pass00681.pt
[2026-01-05 18:15:55] Dump 00682th pass to TP0_PP0_Rank0_pid101005/Pass00682.pt
[2026-01-05 18:15:56] Dump 00683th pass to TP0_PP0_Rank0_pid101005/Pass00683.pt
[2026-01-05 18:15:57] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:15:57] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [01:07<3:43:55, 67.51s/it]
[2026-01-05 18:15:57] Dump 00684th pass to TP0_PP0_Rank0_pid101005/Pass00684.pt
[2026-01-05 18:15:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.01, #running-req: 126, #queue-req: 0,
[2026-01-05 18:15:58] Dump 00685th pass to TP0_PP0_Rank0_pid101005/Pass00685.pt
[2026-01-05 18:16:00] Dump 00686th pass to TP0_PP0_Rank0_pid101005/Pass00686.pt
[2026-01-05 18:16:00] Dump 00687th pass to TP0_PP0_Rank0_pid101005/Pass00687.pt
[2026-01-05 18:16:01] Dump 00688th pass to TP0_PP0_Rank0_pid101005/Pass00688.pt
[2026-01-05 18:16:02] Dump 00689th pass to TP0_PP0_Rank0_pid101005/Pass00689.pt
[2026-01-05 18:16:03] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
  1%|          | 2/200 [01:07<2:20:16, 42.51s/it]
[2026-01-05 18:16:03] Dump 00690th pass to TP0_PP0_Rank0_pid101005/Pass00690.pt
[2026-01-05 18:16:04] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
  2%|▏         | 3/200 [01:13<1:38:38, 30.04s/it]
[2026-01-05 18:16:05] Dump 00691th pass to TP0_PP0_Rank0_pid101005/Pass00691.pt
[2026-01-05 18:16:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 128, #queue-req: 0,
[2026-01-05 18:16:05] Dump 00692th pass to TP0_PP0_Rank0_pid101005/Pass00692.pt
[2026-01-05 18:16:06] Dump 00693th pass to TP0_PP0_Rank0_pid101005/Pass00693.pt
[2026-01-05 18:16:06] Dump 00694th pass to TP0_PP0_Rank0_pid101005/Pass00694.pt
[2026-01-05 18:16:08] Dump 00695th pass to TP0_PP0_Rank0_pid101005/Pass00695.pt
[2026-01-05 18:16:09] Decode batch, #running-req: 128, #token: 21376, token usage: 0.01, cpu graph: False, gen throughput (token/s): 23.25, #queue-req: 0,
[2026-01-05 18:16:09] Dump 00696th pass to TP0_PP0_Rank0_pid101005/Pass00696.pt
[2026-01-05 18:16:10] Dump 00697th pass to TP0_PP0_Rank0_pid101005/Pass00697.pt
[2026-01-05 18:16:11] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
  2%|▏         | 4/200 [01:15<1:06:49, 20.46s/it]
[2026-01-05 18:16:11] Dump 00698th pass to TP0_PP0_Rank0_pid101005/Pass00698.pt
[2026-01-05 18:16:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
[2026-01-05 18:16:12] Dump 00699th pass to TP0_PP0_Rank0_pid101005/Pass00699.pt
[2026-01-05 18:16:13] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
  2%|▎         | 5/200 [01:21<51:43, 15.92s/it]
[2026-01-05 18:16:13] Dump 00700th pass to TP0_PP0_Rank0_pid101005/Pass00700.pt
[2026-01-05 18:16:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 127, #queue-req: 0,
[2026-01-05 18:16:14] Dump 00701th pass to TP0_PP0_Rank0_pid101005/Pass00701.pt
[2026-01-05 18:16:14] Dump 00702th pass to TP0_PP0_Rank0_pid101005/Pass00702.pt
[2026-01-05 18:16:15] Dump 00703th pass to TP0_PP0_Rank0_pid101005/Pass00703.pt
[2026-01-05 18:16:16] Dump 00704th pass to TP0_PP0_Rank0_pid101005/Pass00704.pt
[2026-01-05 18:16:17] Dump 00705th pass to TP0_PP0_Rank0_pid101005/Pass00705.pt
[2026-01-05 18:16:18] Dump 00706th pass to TP0_PP0_Rank0_pid101005/Pass00706.pt
[2026-01-05 18:16:19] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
  3%|▎         | 6/200 [01:23<37:20, 11.55s/it]
[2026-01-05 18:16:19] Dump 00707th pass to TP0_PP0_Rank0_pid101005/Pass00707.pt
[2026-01-05 18:16:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:16:20] Dump 00708th pass to TP0_PP0_Rank0_pid101005/Pass00708.pt
[2026-01-05 18:16:21] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
  4%|▎         | 7/200 [01:29<32:11, 10.01s/it]
[2026-01-05 18:16:21] Dump 00709th pass to TP0_PP0_Rank0_pid101005/Pass00709.pt
[2026-01-05 18:16:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:16:22] Dump 00710th pass to TP0_PP0_Rank0_pid101005/Pass00710.pt
[2026-01-05 18:16:23] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:23] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
  4%|▍         | 8/200 [01:31<23:55,  7.48s/it]
  4%|▍         | 9/200 [01:33<18:04,  5.68s/it]
[2026-01-05 18:16:23] Dump 00711th pass to TP0_PP0_Rank0_pid101005/Pass00711.pt
[2026-01-05 18:16:23] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:16:24] Dump 00712th pass to TP0_PP0_Rank0_pid101005/Pass00712.pt
[2026-01-05 18:16:25] Dump 00713th pass to TP0_PP0_Rank0_pid101005/Pass00713.pt
[2026-01-05 18:16:26] Dump 00714th pass to TP0_PP0_Rank0_pid101005/Pass00714.pt
[2026-01-05 18:16:27] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:27] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:27] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
  5%|▌         | 10/200 [01:33<10:46,  3.40s/it]
  6%|▌         | 11/200 [01:37<11:09,  3.54s/it]
  6%|▋         | 13/200 [01:37<06:41,  2.15s/it]
[2026-01-05 18:16:27] Dump 00715th pass to TP0_PP0_Rank0_pid101005/Pass00715.pt
[2026-01-05 18:16:28] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
  6%|▋         | 13/200 [01:37<06:41,  2.15s/it]
[2026-01-05 18:16:28] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 18:16:28] Dump 00716th pass to TP0_PP0_Rank0_pid101005/Pass00716.pt
[2026-01-05 18:16:29] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
  7%|▋         | 14/200 [01:38<06:02,  1.95s/it]
[2026-01-05 18:16:29] Dump 00717th pass to TP0_PP0_Rank0_pid101005/Pass00717.pt
[2026-01-05 18:16:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:16:30] Dump 00718th pass to TP0_PP0_Rank0_pid101005/Pass00718.pt
[2026-01-05 18:16:30] Dump 00719th pass to TP0_PP0_Rank0_pid101005/Pass00719.pt
[2026-01-05 18:16:31] Dump 00720th pass to TP0_PP0_Rank0_pid101005/Pass00720.pt
[2026-01-05 18:16:32] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:32] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:32] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
  8%|▊         | 15/200 [01:40<05:51,  1.90s/it]
  8%|▊         | 16/200 [01:42<06:25,  2.09s/it]
  9%|▉         | 18/200 [01:42<04:15,  1.40s/it]
[2026-01-05 18:16:32] Dump 00721th pass to TP0_PP0_Rank0_pid101005/Pass00721.pt
[2026-01-05 18:16:33] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:33] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2048, token usage: 0.02, #running-req: 125, #queue-req: 0,
  9%|▉         | 18/200 [01:42<04:15,  1.40s/it]
[2026-01-05 18:16:33] Dump 00722th pass to TP0_PP0_Rank0_pid101005/Pass00722.pt
[2026-01-05 18:16:35] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:16:35] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:35] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
 10%|▉         | 19/200 [01:43<04:00,  1.33s/it]
 10%|█         | 20/200 [01:45<04:05,  1.37s/it]
 11%|█         | 22/200 [01:45<02:33,  1.16it/s]
[2026-01-05 18:16:35] Dump 00723th pass to TP0_PP0_Rank0_pid101005/Pass00723.pt
[2026-01-05 18:16:35] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2048, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:16:35] Dump 00724th pass to TP0_PP0_Rank0_pid101005/Pass00724.pt
[2026-01-05 18:16:36] Dump 00725th pass to TP0_PP0_Rank0_pid101005/Pass00725.pt
[2026-01-05 18:16:37] Dump 00726th pass to TP0_PP0_Rank0_pid101005/Pass00726.pt
[2026-01-05 18:16:38] Dump 00727th pass to TP0_PP0_Rank0_pid101005/Pass00727.pt
[2026-01-05 18:16:39] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:39] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
 11%|█         | 22/200 [01:45<02:33,  1.16it/s]
 12%|█▏        | 23/200 [01:49<04:28,  1.52s/it]
[2026-01-05 18:16:39] Dump 00728th pass to TP0_PP0_Rank0_pid101005/Pass00728.pt
[2026-01-05 18:16:40] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:40] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
 12%|█▏        | 24/200 [01:49<05:13,  1.78s/it]
[2026-01-05 18:16:40] Dump 00729th pass to TP0_PP0_Rank0_pid101005/Pass00729.pt
[2026-01-05 18:16:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:16:42] Dump 00730th pass to TP0_PP0_Rank0_pid101005/Pass00730.pt
[2026-01-05 18:16:43] Dump 00731th pass to TP0_PP0_Rank0_pid101005/Pass00731.pt
[2026-01-05 18:16:43] Dump 00732th pass to TP0_PP0_Rank0_pid101005/Pass00732.pt
[2026-01-05 18:16:44] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
 12%|█▎        | 25/200 [01:50<04:48,  1.65s/it]
[2026-01-05 18:16:44] Dump 00733th pass to TP0_PP0_Rank0_pid101005/Pass00733.pt
[2026-01-05 18:16:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:16:46] Dump 00734th pass to TP0_PP0_Rank0_pid101005/Pass00734.pt
[2026-01-05 18:16:46] Dump 00735th pass to TP0_PP0_Rank0_pid101005/Pass00735.pt
[2026-01-05 18:16:47] Dump 00736th pass to TP0_PP0_Rank0_pid101005/Pass00736.pt
[2026-01-05 18:16:48] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:48] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:48] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:48] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
 13%|█▎        | 26/200 [01:54<06:13,  2.15s/it]
 14%|█▎        | 27/200 [01:58<07:08,  2.48s/it]
 15%|█▌        | 30/200 [01:58<03:46,  1.33s/it]
 15%|█▌        | 30/200 [01:58<03:46,  1.33s/it]
[2026-01-05 18:16:48] Dump 00737th pass to TP0_PP0_Rank0_pid101005/Pass00737.pt
[2026-01-05 18:16:49] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
 15%|█▌        | 30/200 [01:58<03:46,  1.33s/it]
[2026-01-05 18:16:49] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-05 18:16:49] Dump 00738th pass to TP0_PP0_Rank0_pid101005/Pass00738.pt
[2026-01-05 18:16:52] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:16:52] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:52] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
 16%|█▌        | 31/200 [01:59<03:37,  1.29s/it]
 16%|█▌        | 32/200 [02:02<04:21,  1.56s/it]
 17%|█▋        | 34/200 [02:02<03:33,  1.29s/it]
[2026-01-05 18:16:52] Dump 00739th pass to TP0_PP0_Rank0_pid101005/Pass00739.pt
[2026-01-05 18:16:53] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:16:53] Dump 00740th pass to TP0_PP0_Rank0_pid101005/Pass00740.pt
[2026-01-05 18:16:54] Dump 00741th pass to TP0_PP0_Rank0_pid101005/Pass00741.pt
[2026-01-05 18:16:55] Dump 00742th pass to TP0_PP0_Rank0_pid101005/Pass00742.pt
[2026-01-05 18:16:56] Dump 00743th pass to TP0_PP0_Rank0_pid101005/Pass00743.pt
[2026-01-05 18:16:57] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:57] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
 17%|█▋        | 34/200 [02:02<03:33,  1.29s/it]
 18%|█▊        | 35/200 [02:07<05:22,  1.96s/it]
[2026-01-05 18:16:57] Dump 00744th pass to TP0_PP0_Rank0_pid101005/Pass00744.pt
[2026-01-05 18:16:58] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:16:58] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:58] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
 18%|█▊        | 36/200 [02:07<05:59,  2.19s/it]
 18%|█▊        | 37/200 [02:08<05:24,  1.99s/it]
 20%|█▉        | 39/200 [02:08<02:43,  1.01s/it]
[2026-01-05 18:16:58] Dump 00745th pass to TP0_PP0_Rank0_pid101005/Pass00745.pt
[2026-01-05 18:16:59] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:59] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:16:59] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2048, token usage: 0.02, #running-req: 128, #queue-req: 0,
 20%|█▉        | 39/200 [02:08<02:43,  1.01s/it]
 20%|██        | 40/200 [02:10<02:46,  1.04s/it]
[2026-01-05 18:17:00] Dump 00746th pass to TP0_PP0_Rank0_pid101005/Pass00746.pt
[2026-01-05 18:17:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 131, #queue-req: 0,
[2026-01-05 18:17:01] Dump 00747th pass to TP0_PP0_Rank0_pid101005/Pass00747.pt
[2026-01-05 18:17:01] Dump 00748th pass to TP0_PP0_Rank0_pid101005/Pass00748.pt
[2026-01-05 18:17:02] Dump 00749th pass to TP0_PP0_Rank0_pid101005/Pass00749.pt
[2026-01-05 18:17:03] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:03] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:03] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
 20%|██        | 41/200 [02:10<02:22,  1.12it/s]
 21%|██        | 42/200 [02:13<03:41,  1.40s/it]
 22%|██▏       | 44/200 [02:13<03:37,  1.39s/it]
[2026-01-05 18:17:03] Dump 00750th pass to TP0_PP0_Rank0_pid101005/Pass00750.pt
[2026-01-05 18:17:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 18:17:04] Dump 00751th pass to TP0_PP0_Rank0_pid101005/Pass00751.pt
[2026-01-05 18:17:06] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
 22%|██▏       | 44/200 [02:13<03:37,  1.39s/it]
[2026-01-05 18:17:06] Dump 00752th pass to TP0_PP0_Rank0_pid101005/Pass00752.pt
[2026-01-05 18:17:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 18:17:07] Dump 00753th pass to TP0_PP0_Rank0_pid101005/Pass00753.pt
[2026-01-05 18:17:08] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:08] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
 22%|██▎       | 45/200 [02:16<04:15,  1.65s/it]
 23%|██▎       | 46/200 [02:18<04:16,  1.66s/it]
[2026-01-05 18:17:08] Dump 00754th pass to TP0_PP0_Rank0_pid101005/Pass00754.pt
[2026-01-05 18:17:08] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1408, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:17:08] Dump 00755th pass to TP0_PP0_Rank0_pid101005/Pass00755.pt
[2026-01-05 18:17:09] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:09] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:09] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
 24%|██▎       | 47/200 [02:18<03:30,  1.38s/it]
 24%|██▍       | 48/200 [02:19<03:38,  1.44s/it]
 25%|██▌       | 50/200 [02:19<02:19,  1.08it/s]
[2026-01-05 18:17:09] Dump 00756th pass to TP0_PP0_Rank0_pid101005/Pass00756.pt
[2026-01-05 18:17:10] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 18:17:10] Dump 00757th pass to TP0_PP0_Rank0_pid101005/Pass00757.pt
[2026-01-05 18:17:12] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:12] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:12] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:12] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
 25%|██▌       | 50/200 [02:19<02:19,  1.08it/s]
 26%|██▌       | 51/200 [02:22<03:04,  1.24s/it]
 27%|██▋       | 54/200 [02:22<02:15,  1.08it/s]
 27%|██▋       | 54/200 [02:22<02:15,  1.08it/s]
[2026-01-05 18:17:12] Dump 00758th pass to TP0_PP0_Rank0_pid101005/Pass00758.pt
[2026-01-05 18:17:13] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-05 18:17:13] Dump 00759th pass to TP0_PP0_Rank0_pid101005/Pass00759.pt
[2026-01-05 18:17:15] Decode batch, #running-req: 124, #token: 27776, token usage: 0.02, cpu graph: False, gen throughput (token/s): 76.16, #queue-req: 0,
[2026-01-05 18:17:15] Dump 00760th pass to TP0_PP0_Rank0_pid101005/Pass00760.pt
[2026-01-05 18:17:16] Dump 00761th pass to TP0_PP0_Rank0_pid101005/Pass00761.pt
[2026-01-05 18:17:17] Dump 00762th pass to TP0_PP0_Rank0_pid101005/Pass00762.pt
[2026-01-05 18:17:18] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:18] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:18] INFO:     127.0.0.1:33488 - "POST /generate HTTP/1.1" 200 OK
 27%|██▋       | 54/200 [02:22<02:15,  1.08it/s]
 28%|██▊       | 55/200 [02:28<03:58,  1.64s/it]
 28%|██▊       | 57/200 [02:28<04:53,  2.05s/it]
[2026-01-05 18:17:18] Dump 00763th pass to TP0_PP0_Rank0_pid101005/Pass00763.pt
[2026-01-05 18:17:19] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:19] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2048, token usage: 0.02, #running-req: 125, #queue-req: 0,
 28%|██▊       | 57/200 [02:28<04:53,  2.05s/it]
[2026-01-05 18:17:20] Dump 00764th pass to TP0_PP0_Rank0_pid101005/Pass00764.pt
[2026-01-05 18:17:21] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:17:21] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:21] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
 29%|██▉       | 58/200 [02:30<04:33,  1.92s/it]
 30%|██▉       | 59/200 [02:31<04:15,  1.81s/it]
 30%|███       | 61/200 [02:31<02:19,  1.00s/it]
[2026-01-05 18:17:21] Dump 00765th pass to TP0_PP0_Rank0_pid101005/Pass00765.pt
[2026-01-05 18:17:21] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2048, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:17:21] Dump 00766th pass to TP0_PP0_Rank0_pid101005/Pass00766.pt
[2026-01-05 18:17:22] Dump 00767th pass to TP0_PP0_Rank0_pid101005/Pass00767.pt
[2026-01-05 18:17:23] Dump 00768th pass to TP0_PP0_Rank0_pid101005/Pass00768.pt
[2026-01-05 18:17:24] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:24] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
 30%|███       | 61/200 [02:31<02:19,  1.00s/it]
 31%|███       | 62/200 [02:34<03:13,  1.40s/it]
[2026-01-05 18:17:24] Dump 00769th pass to TP0_PP0_Rank0_pid101005/Pass00769.pt
[2026-01-05 18:17:25] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:25] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
 32%|███▏      | 63/200 [02:34<03:26,  1.50s/it]
[2026-01-05 18:17:25] Dump 00770th pass to TP0_PP0_Rank0_pid101005/Pass00770.pt
[2026-01-05 18:17:27] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:17:27] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
 32%|███▏      | 64/200 [02:35<03:14,  1.43s/it]
 32%|███▎      | 65/200 [02:37<03:09,  1.40s/it]
[2026-01-05 18:17:27] Dump 00771th pass to TP0_PP0_Rank0_pid101005/Pass00771.pt
[2026-01-05 18:17:27] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1408, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 18:17:27] Dump 00772th pass to TP0_PP0_Rank0_pid101005/Pass00772.pt
[2026-01-05 18:17:28] Dump 00773th pass to TP0_PP0_Rank0_pid101005/Pass00773.pt
[2026-01-05 18:17:28] Dump 00774th pass to TP0_PP0_Rank0_pid101005/Pass00774.pt
[2026-01-05 18:17:29] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:29] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
 33%|███▎      | 66/200 [02:37<02:30,  1.12s/it]
 34%|███▎      | 67/200 [02:40<03:18,  1.49s/it]
[2026-01-05 18:17:30] Dump 00775th pass to TP0_PP0_Rank0_pid101005/Pass00775.pt
[2026-01-05 18:17:31] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 18:17:31] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
 34%|███▍      | 68/200 [02:40<03:14,  1.47s/it]
 34%|███▍      | 69/200 [02:41<03:01,  1.39s/it]
[2026-01-05 18:17:31] Dump 00776th pass to TP0_PP0_Rank0_pid101005/Pass00776.pt
[2026-01-05 18:17:32] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 18:17:32] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
 35%|███▌      | 70/200 [02:41<02:18,  1.07s/it]
 36%|███▌      | 71/200 [02:42<02:19,  1.08s/it]
[2026-01-05 18:17:32] Dump 00777th pass to TP0_PP0_Rank0_pid101005/Pass00777.pt
[2026-01-05 18:17:33] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-05 18:17:33] Dump 00778th pass to TP0_PP0_Rank0_pid101005/Pass00778.pt
[2026-01-05 18:17:34] Dump 00779th pass to TP0_PP0_Rank0_pid101005/Pass00779.pt
[2026-01-05 18:17:35] Dump 00780th pass to TP0_PP0_Rank0_pid101005/Pass00780.pt
[2026-01-05 18:17:36] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:36] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
 36%|███▌      | 72/200 [02:42<01:53,  1.13it/s]
 36%|███▋      | 73/200 [02:46<03:15,  1.54s/it]
[2026-01-05 18:17:36] Dump 00781th pass to TP0_PP0_Rank0_pid101005/Pass00781.pt
[2026-01-05 18:17:37] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:37] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:37] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
 37%|███▋      | 74/200 [02:46<03:34,  1.70s/it]
 38%|███▊      | 75/200 [02:47<03:16,  1.57s/it]
 38%|███▊      | 77/200 [02:47<01:40,  1.22it/s]
[2026-01-05 18:17:37] Dump 00782th pass to TP0_PP0_Rank0_pid101005/Pass00782.pt
[2026-01-05 18:17:38] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
 38%|███▊      | 77/200 [02:47<01:40,  1.22it/s]
[2026-01-05 18:17:38] Dump 00783th pass to TP0_PP0_Rank0_pid101005/Pass00783.pt
[2026-01-05 18:17:39] Dump 00784th pass to TP0_PP0_Rank0_pid101005/Pass00784.pt
[2026-01-05 18:17:40] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
 39%|███▉      | 78/200 [02:48<01:45,  1.15it/s]
[2026-01-05 18:17:40] Dump 00785th pass to TP0_PP0_Rank0_pid101005/Pass00785.pt
[2026-01-05 18:17:41] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:41] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
 40%|███▉      | 79/200 [02:50<02:13,  1.10s/it]
 40%|████      | 80/200 [02:51<02:10,  1.09s/it]
[2026-01-05 18:17:41] Dump 00786th pass to TP0_PP0_Rank0_pid101005/Pass00786.pt
[2026-01-05 18:17:42] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:42] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:42] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:42] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:42] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
 40%|████      | 81/200 [02:51<01:43,  1.15it/s]
 41%|████      | 82/200 [02:52<01:46,  1.11it/s]
 43%|████▎     | 86/200 [02:52<00:36,  3.08it/s]
 43%|████▎     | 86/200 [02:52<00:36,  3.08it/s]
 43%|████▎     | 86/200 [02:52<00:36,  3.08it/s]
[2026-01-05 18:17:42] Dump 00787th pass to TP0_PP0_Rank0_pid101005/Pass00787.pt
[2026-01-05 18:17:43] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:43] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:43] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:43] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:43] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
 43%|████▎     | 86/200 [02:52<00:36,  3.08it/s]
 44%|████▎     | 87/200 [02:53<00:45,  2.50it/s]
 46%|████▌     | 91/200 [02:53<00:30,  3.60it/s]
 46%|████▌     | 91/200 [02:53<00:30,  3.60it/s]
 46%|████▌     | 91/200 [02:53<00:30,  3.60it/s]
[2026-01-05 18:17:43] Dump 00788th pass to TP0_PP0_Rank0_pid101005/Pass00788.pt
[2026-01-05 18:17:44] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:44] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:44] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
 46%|████▌     | 91/200 [02:53<00:30,  3.60it/s]
 46%|████▌     | 92/200 [02:54<00:37,  2.85it/s]
 47%|████▋     | 94/200 [02:54<00:38,  2.76it/s]
[2026-01-05 18:17:44] Dump 00789th pass to TP0_PP0_Rank0_pid101005/Pass00789.pt
[2026-01-05 18:17:45] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
 47%|████▋     | 94/200 [02:54<00:38,  2.76it/s]
[2026-01-05 18:17:45] Dump 00790th pass to TP0_PP0_Rank0_pid101005/Pass00790.pt
[2026-01-05 18:17:46] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:46] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
 48%|████▊     | 95/200 [02:55<00:46,  2.27it/s]
 48%|████▊     | 96/200 [02:56<00:53,  1.94it/s]
[2026-01-05 18:17:46] Dump 00791th pass to TP0_PP0_Rank0_pid101005/Pass00791.pt
[2026-01-05 18:17:47] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:47] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
 48%|████▊     | 97/200 [02:56<00:51,  2.02it/s]
 49%|████▉     | 98/200 [02:57<00:58,  1.76it/s]
[2026-01-05 18:17:47] Dump 00792th pass to TP0_PP0_Rank0_pid101005/Pass00792.pt
[2026-01-05 18:17:48] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
 50%|████▉     | 99/200 [02:57<00:53,  1.90it/s]
[2026-01-05 18:17:48] Dump 00793th pass to TP0_PP0_Rank0_pid101005/Pass00793.pt
[2026-01-05 18:17:48] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
 50%|█████     | 100/200 [02:58<01:00,  1.66it/s]
[2026-01-05 18:17:48] Dump 00794th pass to TP0_PP0_Rank0_pid101005/Pass00794.pt
[2026-01-05 18:17:49] INFO:     127.0.0.1:33540 - "POST /generate HTTP/1.1" 200 OK
 50%|█████     | 101/200 [02:59<01:05,  1.52it/s]
[2026-01-05 18:17:49] Dump 00795th pass to TP0_PP0_Rank0_pid101005/Pass00795.pt
[2026-01-05 18:17:50] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:50] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
 51%|█████     | 102/200 [02:59<01:10,  1.39it/s]
 52%|█████▏    | 103/200 [03:00<01:14,  1.31it/s]
[2026-01-05 18:17:50] Dump 00796th pass to TP0_PP0_Rank0_pid101005/Pass00796.pt
[2026-01-05 18:17:51] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
 52%|█████▏    | 104/200 [03:00<01:00,  1.59it/s]
[2026-01-05 18:17:51] Dump 00797th pass to TP0_PP0_Rank0_pid101005/Pass00797.pt
[2026-01-05 18:17:52] Dump 00798th pass to TP0_PP0_Rank0_pid101005/Pass00798.pt
[2026-01-05 18:17:53] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
 52%|█████▎    | 105/200 [03:01<01:03,  1.49it/s]
 53%|█████▎    | 106/200 [03:03<01:25,  1.10it/s]
 55%|█████▍    | 109/200 [03:03<00:51,  1.75it/s]
 55%|█████▍    | 109/200 [03:03<00:51,  1.75it/s]
[2026-01-05 18:17:53] Dump 00799th pass to TP0_PP0_Rank0_pid101005/Pass00799.pt
[2026-01-05 18:17:53] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:53] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
 55%|█████▍    | 109/200 [03:03<00:51,  1.75it/s]
 55%|█████▌    | 110/200 [03:04<00:54,  1.64it/s]
 56%|█████▋    | 113/200 [03:04<00:30,  2.89it/s]
 56%|█████▋    | 113/200 [03:04<00:30,  2.89it/s]
[2026-01-05 18:17:54] Dump 00800th pass to TP0_PP0_Rank0_pid101005/Pass00800.pt
[2026-01-05 18:17:54] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
 56%|█████▋    | 113/200 [03:04<00:30,  2.89it/s]
[2026-01-05 18:17:54] Dump 00801th pass to TP0_PP0_Rank0_pid101005/Pass00801.pt
[2026-01-05 18:17:55] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
 57%|█████▋    | 114/200 [03:04<00:34,  2.50it/s]
[2026-01-05 18:17:55] Dump 00802th pass to TP0_PP0_Rank0_pid101005/Pass00802.pt
[2026-01-05 18:17:56] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:56] INFO:     127.0.0.1:41286 - "POST /generate HTTP/1.1" 200 OK
 57%|█████▊    | 115/200 [03:05<00:38,  2.20it/s]
 58%|█████▊    | 116/200 [03:06<00:43,  1.95it/s]
[2026-01-05 18:17:56] Dump 00803th pass to TP0_PP0_Rank0_pid101005/Pass00803.pt
[2026-01-05 18:17:57] Dump 00804th pass to TP0_PP0_Rank0_pid101005/Pass00804.pt
[2026-01-05 18:17:57] Dump 00805th pass to TP0_PP0_Rank0_pid101005/Pass00805.pt
[2026-01-05 18:17:58] Dump 00806th pass to TP0_PP0_Rank0_pid101005/Pass00806.pt
[2026-01-05 18:17:59] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:17:59] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
 58%|█████▊    | 117/200 [03:06<00:39,  2.12it/s]
 59%|█████▉    | 118/200 [03:09<01:17,  1.06it/s]
[2026-01-05 18:17:59] Dump 00807th pass to TP0_PP0_Rank0_pid101005/Pass00807.pt
[2026-01-05 18:17:59] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
 60%|█████▉    | 119/200 [03:09<01:30,  1.12s/it]
[2026-01-05 18:17:59] Dump 00808th pass to TP0_PP0_Rank0_pid101005/Pass00808.pt
[2026-01-05 18:18:00] Dump 00809th pass to TP0_PP0_Rank0_pid101005/Pass00809.pt
[2026-01-05 18:18:01] Decode batch, #running-req: 80, #token: 18688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 95.37, #queue-req: 0,
[2026-01-05 18:18:01] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:01] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
 60%|██████    | 120/200 [03:09<01:22,  1.03s/it]
 60%|██████    | 121/200 [03:11<01:27,  1.10s/it]
[2026-01-05 18:18:01] Dump 00810th pass to TP0_PP0_Rank0_pid101005/Pass00810.pt
[2026-01-05 18:18:01] Dump 00811th pass to TP0_PP0_Rank0_pid101005/Pass00811.pt
[2026-01-05 18:18:02] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
 61%|██████    | 122/200 [03:11<01:13,  1.07it/s]
[2026-01-05 18:18:02] Dump 00812th pass to TP0_PP0_Rank0_pid101005/Pass00812.pt
[2026-01-05 18:18:03] Dump 00813th pass to TP0_PP0_Rank0_pid101005/Pass00813.pt
[2026-01-05 18:18:03] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:03] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
 62%|██████▏   | 123/200 [03:12<01:19,  1.04s/it]
 62%|██████▏   | 124/200 [03:14<01:24,  1.11s/it]
[2026-01-05 18:18:03] Dump 00814th pass to TP0_PP0_Rank0_pid101005/Pass00814.pt
[2026-01-05 18:18:04] Dump 00815th pass to TP0_PP0_Rank0_pid101005/Pass00815.pt
[2026-01-05 18:18:05] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:05] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:05] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
 62%|██████▎   | 125/200 [03:14<01:10,  1.07it/s]
 63%|██████▎   | 126/200 [03:15<01:15,  1.02s/it]
 64%|██████▍   | 128/200 [03:15<00:49,  1.47it/s]
[2026-01-05 18:18:05] Dump 00816th pass to TP0_PP0_Rank0_pid101005/Pass00816.pt
[2026-01-05 18:18:05] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:05] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
 64%|██████▍   | 128/200 [03:15<00:49,  1.47it/s]
 64%|██████▍   | 129/200 [03:15<00:47,  1.48it/s]
[2026-01-05 18:18:05] Dump 00817th pass to TP0_PP0_Rank0_pid101005/Pass00817.pt
[2026-01-05 18:18:06] Dump 00818th pass to TP0_PP0_Rank0_pid101005/Pass00818.pt
[2026-01-05 18:18:07] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
 65%|██████▌   | 130/200 [03:15<00:38,  1.80it/s]
[2026-01-05 18:18:07] Dump 00819th pass to TP0_PP0_Rank0_pid101005/Pass00819.pt
[2026-01-05 18:18:07] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:07] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:07] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:07] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:07] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
 66%|██████▌   | 131/200 [03:17<00:47,  1.44it/s]
 66%|██████▌   | 132/200 [03:17<00:46,  1.46it/s]
 68%|██████▊   | 136/200 [03:17<00:13,  4.62it/s]
 68%|██████▊   | 136/200 [03:17<00:13,  4.62it/s]
 68%|██████▊   | 136/200 [03:17<00:13,  4.62it/s]
[2026-01-05 18:18:07] Dump 00820th pass to TP0_PP0_Rank0_pid101005/Pass00820.pt
[2026-01-05 18:18:08] Dump 00821th pass to TP0_PP0_Rank0_pid101005/Pass00821.pt
[2026-01-05 18:18:08] Dump 00822th pass to TP0_PP0_Rank0_pid101005/Pass00822.pt
[2026-01-05 18:18:09] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
 68%|██████▊   | 136/200 [03:17<00:13,  4.62it/s]
[2026-01-05 18:18:09] Dump 00823th pass to TP0_PP0_Rank0_pid101005/Pass00823.pt
[2026-01-05 18:18:10] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:10] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:10] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:10] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
 68%|██████▊   | 137/200 [03:19<00:24,  2.61it/s]
 69%|██████▉   | 138/200 [03:20<00:25,  2.45it/s]
 70%|███████   | 141/200 [03:20<00:13,  4.31it/s]
 70%|███████   | 141/200 [03:20<00:13,  4.31it/s]
[2026-01-05 18:18:10] Dump 00824th pass to TP0_PP0_Rank0_pid101005/Pass00824.pt
[2026-01-05 18:18:10] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
 70%|███████   | 141/200 [03:20<00:13,  4.31it/s]
[2026-01-05 18:18:10] Dump 00825th pass to TP0_PP0_Rank0_pid101005/Pass00825.pt
[2026-01-05 18:18:11] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
 71%|███████   | 142/200 [03:20<00:15,  3.67it/s]
[2026-01-05 18:18:11] Dump 00826th pass to TP0_PP0_Rank0_pid101005/Pass00826.pt
[2026-01-05 18:18:11] Dump 00827th pass to TP0_PP0_Rank0_pid101005/Pass00827.pt
[2026-01-05 18:18:12] Dump 00828th pass to TP0_PP0_Rank0_pid101005/Pass00828.pt
[2026-01-05 18:18:12] Dump 00829th pass to TP0_PP0_Rank0_pid101005/Pass00829.pt
[2026-01-05 18:18:13] Dump 00830th pass to TP0_PP0_Rank0_pid101005/Pass00830.pt
[2026-01-05 18:18:13] Dump 00831th pass to TP0_PP0_Rank0_pid101005/Pass00831.pt
[2026-01-05 18:18:14] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▏  | 143/200 [03:21<00:17,  3.24it/s]
[2026-01-05 18:18:14] Dump 00832th pass to TP0_PP0_Rank0_pid101005/Pass00832.pt
[2026-01-05 18:18:14] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▏  | 144/200 [03:24<00:45,  1.23it/s]
[2026-01-05 18:18:14] Dump 00833th pass to TP0_PP0_Rank0_pid101005/Pass00833.pt
[2026-01-05 18:18:15] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
 72%|███████▎  | 145/200 [03:24<00:41,  1.34it/s]
[2026-01-05 18:18:15] Dump 00834th pass to TP0_PP0_Rank0_pid101005/Pass00834.pt
[2026-01-05 18:18:15] Dump 00835th pass to TP0_PP0_Rank0_pid101005/Pass00835.pt
[2026-01-05 18:18:16] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:16] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:16] INFO:     127.0.0.1:41304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:16] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
 73%|███████▎  | 146/200 [03:25<00:37,  1.43it/s]
 74%|███████▎  | 147/200 [03:26<00:40,  1.30it/s]
 75%|███████▌  | 150/200 [03:26<00:18,  2.64it/s]
 75%|███████▌  | 150/200 [03:26<00:18,  2.64it/s]
[2026-01-05 18:18:16] Dump 00836th pass to TP0_PP0_Rank0_pid101005/Pass00836.pt
[2026-01-05 18:18:16] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:16] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
 75%|███████▌  | 150/200 [03:26<00:18,  2.64it/s]
 76%|███████▌  | 151/200 [03:26<00:19,  2.57it/s]
[2026-01-05 18:18:16] Dump 00837th pass to TP0_PP0_Rank0_pid101005/Pass00837.pt
[2026-01-05 18:18:17] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
 76%|███████▌  | 152/200 [03:26<00:16,  2.94it/s]
[2026-01-05 18:18:17] Dump 00838th pass to TP0_PP0_Rank0_pid101005/Pass00838.pt
[2026-01-05 18:18:17] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
 76%|███████▋  | 153/200 [03:27<00:16,  2.77it/s]
[2026-01-05 18:18:17] Dump 00839th pass to TP0_PP0_Rank0_pid101005/Pass00839.pt
[2026-01-05 18:18:17] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
 77%|███████▋  | 154/200 [03:27<00:17,  2.65it/s]
[2026-01-05 18:18:17] Dump 00840th pass to TP0_PP0_Rank0_pid101005/Pass00840.pt
[2026-01-05 18:18:18] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:18] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:18] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
 78%|███████▊  | 155/200 [03:28<00:17,  2.54it/s]
 78%|███████▊  | 156/200 [03:28<00:17,  2.55it/s]
 79%|███████▉  | 158/200 [03:28<00:09,  4.40it/s]
[2026-01-05 18:18:18] Dump 00841th pass to TP0_PP0_Rank0_pid101005/Pass00841.pt
[2026-01-05 18:18:18] Dump 00842th pass to TP0_PP0_Rank0_pid101005/Pass00842.pt
[2026-01-05 18:18:19] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:19] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
 79%|███████▉  | 158/200 [03:28<00:09,  4.40it/s]
 80%|███████▉  | 159/200 [03:29<00:13,  3.05it/s]
[2026-01-05 18:18:19] Dump 00843th pass to TP0_PP0_Rank0_pid101005/Pass00843.pt
[2026-01-05 18:18:19] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
 80%|████████  | 160/200 [03:29<00:14,  2.85it/s]
[2026-01-05 18:18:19] Dump 00844th pass to TP0_PP0_Rank0_pid101005/Pass00844.pt
[2026-01-05 18:18:19] Dump 00845th pass to TP0_PP0_Rank0_pid101005/Pass00845.pt
[2026-01-05 18:18:20] Dump 00846th pass to TP0_PP0_Rank0_pid101005/Pass00846.pt
[2026-01-05 18:18:20] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:20] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
 80%|████████  | 161/200 [03:29<00:13,  2.79it/s]
 81%|████████  | 162/200 [03:30<00:20,  1.89it/s]
[2026-01-05 18:18:20] Dump 00847th pass to TP0_PP0_Rank0_pid101005/Pass00847.pt
[2026-01-05 18:18:20] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▏ | 163/200 [03:30<00:20,  1.84it/s]
[2026-01-05 18:18:21] Dump 00848th pass to TP0_PP0_Rank0_pid101005/Pass00848.pt
[2026-01-05 18:18:21] Dump 00849th pass to TP0_PP0_Rank0_pid101005/Pass00849.pt
[2026-01-05 18:18:21] Decode batch, #running-req: 36, #token: 10752, token usage: 0.01, cpu graph: False, gen throughput (token/s): 112.44, #queue-req: 0,
[2026-01-05 18:18:21] Dump 00850th pass to TP0_PP0_Rank0_pid101005/Pass00850.pt
[2026-01-05 18:18:22] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▏ | 164/200 [03:31<00:17,  2.00it/s]
[2026-01-05 18:18:22] Dump 00851th pass to TP0_PP0_Rank0_pid101005/Pass00851.pt
[2026-01-05 18:18:22] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
 82%|████████▎ | 165/200 [03:32<00:22,  1.58it/s]
[2026-01-05 18:18:22] Dump 00852th pass to TP0_PP0_Rank0_pid101005/Pass00852.pt
[2026-01-05 18:18:22] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
 83%|████████▎ | 166/200 [03:32<00:19,  1.79it/s]
[2026-01-05 18:18:22] Dump 00853th pass to TP0_PP0_Rank0_pid101005/Pass00853.pt
[2026-01-05 18:18:23] Dump 00854th pass to TP0_PP0_Rank0_pid101005/Pass00854.pt
[2026-01-05 18:18:23] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:23] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:23] INFO:     127.0.0.1:41190 - "POST /generate HTTP/1.1" 200 OK
 84%|████████▎ | 167/200 [03:32<00:16,  2.00it/s]
 84%|████████▍ | 168/200 [03:33<00:17,  1.84it/s]
 85%|████████▌ | 170/200 [03:33<00:10,  2.96it/s]
[2026-01-05 18:18:23] Dump 00855th pass to TP0_PP0_Rank0_pid101005/Pass00855.pt
[2026-01-05 18:18:23] Dump 00856th pass to TP0_PP0_Rank0_pid101005/Pass00856.pt
[2026-01-05 18:18:24] Dump 00857th pass to TP0_PP0_Rank0_pid101005/Pass00857.pt
[2026-01-05 18:18:24] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
 85%|████████▌ | 170/200 [03:33<00:10,  2.96it/s]
[2026-01-05 18:18:24] Dump 00858th pass to TP0_PP0_Rank0_pid101005/Pass00858.pt
[2026-01-05 18:18:24] Dump 00859th pass to TP0_PP0_Rank0_pid101005/Pass00859.pt
[2026-01-05 18:18:24] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
 86%|████████▌ | 171/200 [03:34<00:13,  2.22it/s]
[2026-01-05 18:18:24] Dump 00860th pass to TP0_PP0_Rank0_pid101005/Pass00860.pt
[2026-01-05 18:18:25] Dump 00861th pass to TP0_PP0_Rank0_pid101005/Pass00861.pt
[2026-01-05 18:18:25] Dump 00862th pass to TP0_PP0_Rank0_pid101005/Pass00862.pt
[2026-01-05 18:18:25] Dump 00863th pass to TP0_PP0_Rank0_pid101005/Pass00863.pt
[2026-01-05 18:18:26] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:26] INFO:     127.0.0.1:41302 - "POST /generate HTTP/1.1" 200 OK
 86%|████████▌ | 172/200 [03:35<00:13,  2.08it/s]
 86%|████████▋ | 173/200 [03:36<00:17,  1.57it/s]
[2026-01-05 18:18:26] Dump 00864th pass to TP0_PP0_Rank0_pid101005/Pass00864.pt
[2026-01-05 18:18:26] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
 87%|████████▋ | 174/200 [03:36<00:15,  1.63it/s]
[2026-01-05 18:18:26] Dump 00865th pass to TP0_PP0_Rank0_pid101005/Pass00865.pt
[2026-01-05 18:18:26] Dump 00866th pass to TP0_PP0_Rank0_pid101005/Pass00866.pt
[2026-01-05 18:18:26] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
 88%|████████▊ | 175/200 [03:36<00:13,  1.85it/s]
[2026-01-05 18:18:26] Dump 00867th pass to TP0_PP0_Rank0_pid101005/Pass00867.pt
[2026-01-05 18:18:27] Dump 00868th pass to TP0_PP0_Rank0_pid101005/Pass00868.pt
[2026-01-05 18:18:27] Dump 00869th pass to TP0_PP0_Rank0_pid101005/Pass00869.pt
[2026-01-05 18:18:27] Dump 00870th pass to TP0_PP0_Rank0_pid101005/Pass00870.pt
[2026-01-05 18:18:28] Dump 00871th pass to TP0_PP0_Rank0_pid101005/Pass00871.pt
[2026-01-05 18:18:28] Dump 00872th pass to TP0_PP0_Rank0_pid101005/Pass00872.pt
[2026-01-05 18:18:28] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:28] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
 88%|████████▊ | 176/200 [03:37<00:13,  1.81it/s]
 88%|████████▊ | 177/200 [03:38<00:19,  1.18it/s]
[2026-01-05 18:18:28] Dump 00873th pass to TP0_PP0_Rank0_pid101005/Pass00873.pt
[2026-01-05 18:18:28] Dump 00874th pass to TP0_PP0_Rank0_pid101005/Pass00874.pt
[2026-01-05 18:18:29] Dump 00875th pass to TP0_PP0_Rank0_pid101005/Pass00875.pt
[2026-01-05 18:18:29] Dump 00876th pass to TP0_PP0_Rank0_pid101005/Pass00876.pt
[2026-01-05 18:18:29] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:29] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
 89%|████████▉ | 178/200 [03:38<00:18,  1.17it/s]
 90%|████████▉ | 179/200 [03:39<00:18,  1.11it/s]
[2026-01-05 18:18:29] Dump 00877th pass to TP0_PP0_Rank0_pid101005/Pass00877.pt
[2026-01-05 18:18:29] Dump 00878th pass to TP0_PP0_Rank0_pid101005/Pass00878.pt
[2026-01-05 18:18:30] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
 90%|█████████ | 180/200 [03:39<00:14,  1.33it/s]
[2026-01-05 18:18:30] Dump 00879th pass to TP0_PP0_Rank0_pid101005/Pass00879.pt
[2026-01-05 18:18:30] Dump 00880th pass to TP0_PP0_Rank0_pid101005/Pass00880.pt
[2026-01-05 18:18:30] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
 90%|█████████ | 181/200 [03:40<00:13,  1.44it/s]
[2026-01-05 18:18:30] Dump 00881th pass to TP0_PP0_Rank0_pid101005/Pass00881.pt
[2026-01-05 18:18:30] Dump 00882th pass to TP0_PP0_Rank0_pid101005/Pass00882.pt
[2026-01-05 18:18:31] Dump 00883th pass to TP0_PP0_Rank0_pid101005/Pass00883.pt
[2026-01-05 18:18:31] Dump 00884th pass to TP0_PP0_Rank0_pid101005/Pass00884.pt
[2026-01-05 18:18:31] Dump 00885th pass to TP0_PP0_Rank0_pid101005/Pass00885.pt
[2026-01-05 18:18:31] INFO:     127.0.0.1:37798 - "POST /generate HTTP/1.1" 200 OK
 91%|█████████ | 182/200 [03:40<00:11,  1.58it/s]
[2026-01-05 18:18:31] Dump 00886th pass to TP0_PP0_Rank0_pid101005/Pass00886.pt
[2026-01-05 18:18:31] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▏| 183/200 [03:41<00:12,  1.34it/s]
[2026-01-05 18:18:31] Dump 00887th pass to TP0_PP0_Rank0_pid101005/Pass00887.pt
[2026-01-05 18:18:32] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▏| 184/200 [03:42<00:09,  1.65it/s]
[2026-01-05 18:18:32] Dump 00888th pass to TP0_PP0_Rank0_pid101005/Pass00888.pt
[2026-01-05 18:18:32] Dump 00889th pass to TP0_PP0_Rank0_pid101005/Pass00889.pt
[2026-01-05 18:18:32] Decode batch, #running-req: 15, #token: 5248, token usage: 0.00, cpu graph: False, gen throughput (token/s): 91.07, #queue-req: 0,
[2026-01-05 18:18:32] Dump 00890th pass to TP0_PP0_Rank0_pid101005/Pass00890.pt
[2026-01-05 18:18:32] Dump 00891th pass to TP0_PP0_Rank0_pid101005/Pass00891.pt
[2026-01-05 18:18:32] Dump 00892th pass to TP0_PP0_Rank0_pid101005/Pass00892.pt
[2026-01-05 18:18:33] Dump 00893th pass to TP0_PP0_Rank0_pid101005/Pass00893.pt
[2026-01-05 18:18:33] Dump 00894th pass to TP0_PP0_Rank0_pid101005/Pass00894.pt
[2026-01-05 18:18:33] Dump 00895th pass to TP0_PP0_Rank0_pid101005/Pass00895.pt
[2026-01-05 18:18:33] Dump 00896th pass to TP0_PP0_Rank0_pid101005/Pass00896.pt
[2026-01-05 18:18:33] Dump 00897th pass to TP0_PP0_Rank0_pid101005/Pass00897.pt
[2026-01-05 18:18:34] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:18:34] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
 92%|█████████▎| 185/200 [03:42<00:07,  2.01it/s]
 93%|█████████▎| 186/200 [03:44<00:12,  1.11it/s]
[2026-01-05 18:18:34] Dump 00898th pass to TP0_PP0_Rank0_pid101005/Pass00898.pt
[2026-01-05 18:18:34] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▎| 187/200 [03:44<00:12,  1.08it/s]
[2026-01-05 18:18:34] Dump 00899th pass to TP0_PP0_Rank0_pid101005/Pass00899.pt
[2026-01-05 18:18:34] Dump 00900th pass to TP0_PP0_Rank0_pid101005/Pass00900.pt
[2026-01-05 18:18:34] Dump 00901th pass to TP0_PP0_Rank0_pid101005/Pass00901.pt
[2026-01-05 18:18:34] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▍| 188/200 [03:44<00:08,  1.34it/s]
[2026-01-05 18:18:34] Dump 00902th pass to TP0_PP0_Rank0_pid101005/Pass00902.pt
[2026-01-05 18:18:34] Dump 00903th pass to TP0_PP0_Rank0_pid101005/Pass00903.pt
[2026-01-05 18:18:35] Dump 00904th pass to TP0_PP0_Rank0_pid101005/Pass00904.pt
[2026-01-05 18:18:35] Dump 00905th pass to TP0_PP0_Rank0_pid101005/Pass00905.pt
[2026-01-05 18:18:35] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
 94%|█████████▍| 189/200 [03:44<00:07,  1.47it/s]
[2026-01-05 18:18:35] Dump 00906th pass to TP0_PP0_Rank0_pid101005/Pass00906.pt
[2026-01-05 18:18:35] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
 95%|█████████▌| 190/200 [03:45<00:06,  1.49it/s]
[2026-01-05 18:18:35] Dump 00907th pass to TP0_PP0_Rank0_pid101005/Pass00907.pt
[2026-01-05 18:18:35] Dump 00908th pass to TP0_PP0_Rank0_pid101005/Pass00908.pt
[2026-01-05 18:18:35] INFO:     127.0.0.1:37800 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▌| 191/200 [03:45<00:04,  1.90it/s]
[2026-01-05 18:18:35] Dump 00909th pass to TP0_PP0_Rank0_pid101005/Pass00909.pt
[2026-01-05 18:18:35] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▌| 192/200 [03:45<00:03,  2.18it/s]
[2026-01-05 18:18:36] Dump 00910th pass to TP0_PP0_Rank0_pid101005/Pass00910.pt
[2026-01-05 18:18:36] Dump 00911th pass to TP0_PP0_Rank0_pid101005/Pass00911.pt
[2026-01-05 18:18:36] Dump 00912th pass to TP0_PP0_Rank0_pid101005/Pass00912.pt
[2026-01-05 18:18:36] Dump 00913th pass to TP0_PP0_Rank0_pid101005/Pass00913.pt
[2026-01-05 18:18:36] Dump 00914th pass to TP0_PP0_Rank0_pid101005/Pass00914.pt
[2026-01-05 18:18:36] Dump 00915th pass to TP0_PP0_Rank0_pid101005/Pass00915.pt
[2026-01-05 18:18:36] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
 96%|█████████▋| 193/200 [03:46<00:02,  2.72it/s]
[2026-01-05 18:18:36] Dump 00916th pass to TP0_PP0_Rank0_pid101005/Pass00916.pt
[2026-01-05 18:18:36] Dump 00917th pass to TP0_PP0_Rank0_pid101005/Pass00917.pt
[2026-01-05 18:18:37] Dump 00918th pass to TP0_PP0_Rank0_pid101005/Pass00918.pt
[2026-01-05 18:18:37] Dump 00919th pass to TP0_PP0_Rank0_pid101005/Pass00919.pt
[2026-01-05 18:18:37] Dump 00920th pass to TP0_PP0_Rank0_pid101005/Pass00920.pt
[2026-01-05 18:18:37] Dump 00921th pass to TP0_PP0_Rank0_pid101005/Pass00921.pt
[2026-01-05 18:18:37] Dump 00922th pass to TP0_PP0_Rank0_pid101005/Pass00922.pt
[2026-01-05 18:18:37] Dump 00923th pass to TP0_PP0_Rank0_pid101005/Pass00923.pt
[2026-01-05 18:18:37] Dump 00924th pass to TP0_PP0_Rank0_pid101005/Pass00924.pt
[2026-01-05 18:18:37] Dump 00925th pass to TP0_PP0_Rank0_pid101005/Pass00925.pt
[2026-01-05 18:18:37] Dump 00926th pass to TP0_PP0_Rank0_pid101005/Pass00926.pt
[2026-01-05 18:18:38] Dump 00927th pass to TP0_PP0_Rank0_pid101005/Pass00927.pt
[2026-01-05 18:18:38] Dump 00928th pass to TP0_PP0_Rank0_pid101005/Pass00928.pt
[2026-01-05 18:18:38] Dump 00929th pass to TP0_PP0_Rank0_pid101005/Pass00929.pt
[2026-01-05 18:18:38] Decode batch, #running-req: 6, #token: 2688, token usage: 0.00, cpu graph: False, gen throughput (token/s): 65.16, #queue-req: 0,
[2026-01-05 18:18:38] Dump 00930th pass to TP0_PP0_Rank0_pid101005/Pass00930.pt
[2026-01-05 18:18:38] Dump 00931th pass to TP0_PP0_Rank0_pid101005/Pass00931.pt
[2026-01-05 18:18:38] Dump 00932th pass to TP0_PP0_Rank0_pid101005/Pass00932.pt
[2026-01-05 18:18:38] Dump 00933th pass to TP0_PP0_Rank0_pid101005/Pass00933.pt
[2026-01-05 18:18:38] Dump 00934th pass to TP0_PP0_Rank0_pid101005/Pass00934.pt
[2026-01-05 18:18:39] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
 97%|█████████▋| 194/200 [03:46<00:02,  2.07it/s]
[2026-01-05 18:18:39] Dump 00935th pass to TP0_PP0_Rank0_pid101005/Pass00935.pt
[2026-01-05 18:18:39] Dump 00936th pass to TP0_PP0_Rank0_pid101005/Pass00936.pt
[2026-01-05 18:18:39] Dump 00937th pass to TP0_PP0_Rank0_pid101005/Pass00937.pt
[2026-01-05 18:18:39] Dump 00938th pass to TP0_PP0_Rank0_pid101005/Pass00938.pt
[2026-01-05 18:18:39] Dump 00939th pass to TP0_PP0_Rank0_pid101005/Pass00939.pt
[2026-01-05 18:18:39] Dump 00940th pass to TP0_PP0_Rank0_pid101005/Pass00940.pt
[2026-01-05 18:18:39] Dump 00941th pass to TP0_PP0_Rank0_pid101005/Pass00941.pt
[2026-01-05 18:18:39] Dump 00942th pass to TP0_PP0_Rank0_pid101005/Pass00942.pt
[2026-01-05 18:18:40] Dump 00943th pass to TP0_PP0_Rank0_pid101005/Pass00943.pt
[2026-01-05 18:18:40] Dump 00944th pass to TP0_PP0_Rank0_pid101005/Pass00944.pt
[2026-01-05 18:18:40] Dump 00945th pass to TP0_PP0_Rank0_pid101005/Pass00945.pt
[2026-01-05 18:18:40] Dump 00946th pass to TP0_PP0_Rank0_pid101005/Pass00946.pt
[2026-01-05 18:18:40] Dump 00947th pass to TP0_PP0_Rank0_pid101005/Pass00947.pt
[2026-01-05 18:18:40] Dump 00948th pass to TP0_PP0_Rank0_pid101005/Pass00948.pt
[2026-01-05 18:18:40] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 195/200 [03:49<00:05,  1.02s/it]
[2026-01-05 18:18:40] Dump 00949th pass to TP0_PP0_Rank0_pid101005/Pass00949.pt
[2026-01-05 18:18:40] Dump 00950th pass to TP0_PP0_Rank0_pid101005/Pass00950.pt
[2026-01-05 18:18:40] Dump 00951th pass to TP0_PP0_Rank0_pid101005/Pass00951.pt
[2026-01-05 18:18:41] Dump 00952th pass to TP0_PP0_Rank0_pid101005/Pass00952.pt
[2026-01-05 18:18:41] Dump 00953th pass to TP0_PP0_Rank0_pid101005/Pass00953.pt
[2026-01-05 18:18:41] Dump 00954th pass to TP0_PP0_Rank0_pid101005/Pass00954.pt
[2026-01-05 18:18:41] Dump 00955th pass to TP0_PP0_Rank0_pid101005/Pass00955.pt
[2026-01-05 18:18:41] Dump 00956th pass to TP0_PP0_Rank0_pid101005/Pass00956.pt
[2026-01-05 18:18:41] Dump 00957th pass to TP0_PP0_Rank0_pid101005/Pass00957.pt
[2026-01-05 18:18:41] Dump 00958th pass to TP0_PP0_Rank0_pid101005/Pass00958.pt
[2026-01-05 18:18:41] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 196/200 [03:50<00:04,  1.19s/it]
[2026-01-05 18:18:41] Dump 00959th pass to TP0_PP0_Rank0_pid101005/Pass00959.pt
[2026-01-05 18:18:41] Dump 00960th pass to TP0_PP0_Rank0_pid101005/Pass00960.pt
[2026-01-05 18:18:41] Dump 00961th pass to TP0_PP0_Rank0_pid101005/Pass00961.pt
[2026-01-05 18:18:41] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
 98%|█████████▊| 197/200 [03:51<00:03,  1.15s/it]
[2026-01-05 18:18:42] Dump 00962th pass to TP0_PP0_Rank0_pid101005/Pass00962.pt
[2026-01-05 18:18:42] Dump 00963th pass to TP0_PP0_Rank0_pid101005/Pass00963.pt
[2026-01-05 18:18:42] Dump 00964th pass to TP0_PP0_Rank0_pid101005/Pass00964.pt
[2026-01-05 18:18:42] Dump 00965th pass to TP0_PP0_Rank0_pid101005/Pass00965.pt
[2026-01-05 18:18:42] Dump 00966th pass to TP0_PP0_Rank0_pid101005/Pass00966.pt
[2026-01-05 18:18:42] Dump 00967th pass to TP0_PP0_Rank0_pid101005/Pass00967.pt
[2026-01-05 18:18:42] Dump 00968th pass to TP0_PP0_Rank0_pid101005/Pass00968.pt
[2026-01-05 18:18:42] Dump 00969th pass to TP0_PP0_Rank0_pid101005/Pass00969.pt
[2026-01-05 18:18:42] Decode batch, #running-req: 2, #token: 1408, token usage: 0.00, cpu graph: False, gen throughput (token/s): 39.24, #queue-req: 0,
[2026-01-05 18:18:42] Dump 00970th pass to TP0_PP0_Rank0_pid101005/Pass00970.pt
[2026-01-05 18:18:42] Dump 00971th pass to TP0_PP0_Rank0_pid101005/Pass00971.pt
[2026-01-05 18:18:42] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
 99%|█████████▉| 198/200 [03:52<00:01,  1.11it/s]
[2026-01-05 18:18:42] Dump 00972th pass to TP0_PP0_Rank0_pid101005/Pass00972.pt
[2026-01-05 18:18:43] Dump 00973th pass to TP0_PP0_Rank0_pid101005/Pass00973.pt
[2026-01-05 18:18:43] Dump 00974th pass to TP0_PP0_Rank0_pid101005/Pass00974.pt
[2026-01-05 18:18:43] Dump 00975th pass to TP0_PP0_Rank0_pid101005/Pass00975.pt
[2026-01-05 18:18:43] Dump 00976th pass to TP0_PP0_Rank0_pid101005/Pass00976.pt
[2026-01-05 18:18:43] Dump 00977th pass to TP0_PP0_Rank0_pid101005/Pass00977.pt
[2026-01-05 18:18:43] Dump 00978th pass to TP0_PP0_Rank0_pid101005/Pass00978.pt
[2026-01-05 18:18:43] Dump 00979th pass to TP0_PP0_Rank0_pid101005/Pass00979.pt
[2026-01-05 18:18:43] Dump 00980th pass to TP0_PP0_Rank0_pid101005/Pass00980.pt
[2026-01-05 18:18:43] Dump 00981th pass to TP0_PP0_Rank0_pid101005/Pass00981.pt
[2026-01-05 18:18:43] Dump 00982th pass to TP0_PP0_Rank0_pid101005/Pass00982.pt
[2026-01-05 18:18:43] Dump 00983th pass to TP0_PP0_Rank0_pid101005/Pass00983.pt
[2026-01-05 18:18:43] Dump 00984th pass to TP0_PP0_Rank0_pid101005/Pass00984.pt
[2026-01-05 18:18:44] Dump 00985th pass to TP0_PP0_Rank0_pid101005/Pass00985.pt
[2026-01-05 18:18:44] Dump 00986th pass to TP0_PP0_Rank0_pid101005/Pass00986.pt
[2026-01-05 18:18:44] Dump 00987th pass to TP0_PP0_Rank0_pid101005/Pass00987.pt
[2026-01-05 18:18:44] Dump 00988th pass to TP0_PP0_Rank0_pid101005/Pass00988.pt
[2026-01-05 18:18:44] Dump 00989th pass to TP0_PP0_Rank0_pid101005/Pass00989.pt
[2026-01-05 18:18:44] Dump 00990th pass to TP0_PP0_Rank0_pid101005/Pass00990.pt
[2026-01-05 18:18:44] Dump 00991th pass to TP0_PP0_Rank0_pid101005/Pass00991.pt
[2026-01-05 18:18:44] Dump 00992th pass to TP0_PP0_Rank0_pid101005/Pass00992.pt
[2026-01-05 18:18:44] Dump 00993th pass to TP0_PP0_Rank0_pid101005/Pass00993.pt
[2026-01-05 18:18:44] Dump 00994th pass to TP0_PP0_Rank0_pid101005/Pass00994.pt
[2026-01-05 18:18:44] Dump 00995th pass to TP0_PP0_Rank0_pid101005/Pass00995.pt
[2026-01-05 18:18:45] Dump 00996th pass to TP0_PP0_Rank0_pid101005/Pass00996.pt
[2026-01-05 18:18:45] Dump 00997th pass to TP0_PP0_Rank0_pid101005/Pass00997.pt
[2026-01-05 18:18:45] Dump 00998th pass to TP0_PP0_Rank0_pid101005/Pass00998.pt
[2026-01-05 18:18:45] Dump 00999th pass to TP0_PP0_Rank0_pid101005/Pass00999.pt
[2026-01-05 18:18:45] Dump 01000th pass to TP0_PP0_Rank0_pid101005/Pass01000.pt
[2026-01-05 18:18:45] Dump 01001th pass to TP0_PP0_Rank0_pid101005/Pass01001.pt
[2026-01-05 18:18:45] Dump 01002th pass to TP0_PP0_Rank0_pid101005/Pass01002.pt
[2026-01-05 18:18:45] Dump 01003th pass to TP0_PP0_Rank0_pid101005/Pass01003.pt
[2026-01-05 18:18:45] Dump 01004th pass to TP0_PP0_Rank0_pid101005/Pass01004.pt
[2026-01-05 18:18:45] Dump 01005th pass to TP0_PP0_Rank0_pid101005/Pass01005.pt
[2026-01-05 18:18:45] Dump 01006th pass to TP0_PP0_Rank0_pid101005/Pass01006.pt
[2026-01-05 18:18:45] Dump 01007th pass to TP0_PP0_Rank0_pid101005/Pass01007.pt
[2026-01-05 18:18:46] Dump 01008th pass to TP0_PP0_Rank0_pid101005/Pass01008.pt
[2026-01-05 18:18:46] Dump 01009th pass to TP0_PP0_Rank0_pid101005/Pass01009.pt
[2026-01-05 18:18:46] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cpu graph: False, gen throughput (token/s): 12.64, #queue-req: 0,
[2026-01-05 18:18:46] Dump 01010th pass to TP0_PP0_Rank0_pid101005/Pass01010.pt
[2026-01-05 18:18:46] Dump 01011th pass to TP0_PP0_Rank0_pid101005/Pass01011.pt
[2026-01-05 18:18:46] Dump 01012th pass to TP0_PP0_Rank0_pid101005/Pass01012.pt
[2026-01-05 18:18:46] Dump 01013th pass to TP0_PP0_Rank0_pid101005/Pass01013.pt
[2026-01-05 18:18:46] Dump 01014th pass to TP0_PP0_Rank0_pid101005/Pass01014.pt
[2026-01-05 18:18:46] Dump 01015th pass to TP0_PP0_Rank0_pid101005/Pass01015.pt
[2026-01-05 18:18:46] Dump 01016th pass to TP0_PP0_Rank0_pid101005/Pass01016.pt
[2026-01-05 18:18:46] Dump 01017th pass to TP0_PP0_Rank0_pid101005/Pass01017.pt
[2026-01-05 18:18:46] Dump 01018th pass to TP0_PP0_Rank0_pid101005/Pass01018.pt
[2026-01-05 18:18:46] Dump 01019th pass to TP0_PP0_Rank0_pid101005/Pass01019.pt
[2026-01-05 18:18:47] Dump 01020th pass to TP0_PP0_Rank0_pid101005/Pass01020.pt
[2026-01-05 18:18:47] Dump 01021th pass to TP0_PP0_Rank0_pid101005/Pass01021.pt
[2026-01-05 18:18:47] Dump 01022th pass to TP0_PP0_Rank0_pid101005/Pass01022.pt
[2026-01-05 18:18:47] Dump 01023th pass to TP0_PP0_Rank0_pid101005/Pass01023.pt
[2026-01-05 18:18:47] Dump 01024th pass to TP0_PP0_Rank0_pid101005/Pass01024.pt
[2026-01-05 18:18:47] Dump 01025th pass to TP0_PP0_Rank0_pid101005/Pass01025.pt
[2026-01-05 18:18:47] Dump 01026th pass to TP0_PP0_Rank0_pid101005/Pass01026.pt
[2026-01-05 18:18:47] Dump 01027th pass to TP0_PP0_Rank0_pid101005/Pass01027.pt
[2026-01-05 18:18:47] Dump 01028th pass to TP0_PP0_Rank0_pid101005/Pass01028.pt
[2026-01-05 18:18:47] Dump 01029th pass to TP0_PP0_Rank0_pid101005/Pass01029.pt
[2026-01-05 18:18:47] Dump 01030th pass to TP0_PP0_Rank0_pid101005/Pass01030.pt
[2026-01-05 18:18:47] Dump 01031th pass to TP0_PP0_Rank0_pid101005/Pass01031.pt
[2026-01-05 18:18:47] Dump 01032th pass to TP0_PP0_Rank0_pid101005/Pass01032.pt
[2026-01-05 18:18:48] Dump 01033th pass to TP0_PP0_Rank0_pid101005/Pass01033.pt
[2026-01-05 18:18:48] Dump 01034th pass to TP0_PP0_Rank0_pid101005/Pass01034.pt
[2026-01-05 18:18:48] Dump 01035th pass to TP0_PP0_Rank0_pid101005/Pass01035.pt
[2026-01-05 18:18:48] Dump 01036th pass to TP0_PP0_Rank0_pid101005/Pass01036.pt
[2026-01-05 18:18:48] Dump 01037th pass to TP0_PP0_Rank0_pid101005/Pass01037.pt
[2026-01-05 18:18:48] Dump 01038th pass to TP0_PP0_Rank0_pid101005/Pass01038.pt
[2026-01-05 18:18:48] Dump 01039th pass to TP0_PP0_Rank0_pid101005/Pass01039.pt
[2026-01-05 18:18:48] Dump 01040th pass to TP0_PP0_Rank0_pid101005/Pass01040.pt
[2026-01-05 18:18:48] Dump 01041th pass to TP0_PP0_Rank0_pid101005/Pass01041.pt
[2026-01-05 18:18:48] Dump 01042th pass to TP0_PP0_Rank0_pid101005/Pass01042.pt
[2026-01-05 18:18:48] Dump 01043th pass to TP0_PP0_Rank0_pid101005/Pass01043.pt
[2026-01-05 18:18:48] Dump 01044th pass to TP0_PP0_Rank0_pid101005/Pass01044.pt
[2026-01-05 18:18:49] Dump 01045th pass to TP0_PP0_Rank0_pid101005/Pass01045.pt
[2026-01-05 18:18:49] Dump 01046th pass to TP0_PP0_Rank0_pid101005/Pass01046.pt
[2026-01-05 18:18:49] Dump 01047th pass to TP0_PP0_Rank0_pid101005/Pass01047.pt
[2026-01-05 18:18:49] Dump 01048th pass to TP0_PP0_Rank0_pid101005/Pass01048.pt
[2026-01-05 18:18:49] Dump 01049th pass to TP0_PP0_Rank0_pid101005/Pass01049.pt
[2026-01-05 18:18:49] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cpu graph: False, gen throughput (token/s): 12.22, #queue-req: 0,
[2026-01-05 18:18:49] Dump 01050th pass to TP0_PP0_Rank0_pid101005/Pass01050.pt
[2026-01-05 18:18:49] Dump 01051th pass to TP0_PP0_Rank0_pid101005/Pass01051.pt
[2026-01-05 18:18:49] Dump 01052th pass to TP0_PP0_Rank0_pid101005/Pass01052.pt
[2026-01-05 18:18:49] Dump 01053th pass to TP0_PP0_Rank0_pid101005/Pass01053.pt
[2026-01-05 18:18:49] Dump 01054th pass to TP0_PP0_Rank0_pid101005/Pass01054.pt
[2026-01-05 18:18:49] Dump 01055th pass to TP0_PP0_Rank0_pid101005/Pass01055.pt
[2026-01-05 18:18:49] Dump 01056th pass to TP0_PP0_Rank0_pid101005/Pass01056.pt
[2026-01-05 18:18:50] Dump 01057th pass to TP0_PP0_Rank0_pid101005/Pass01057.pt
[2026-01-05 18:18:50] Dump 01058th pass to TP0_PP0_Rank0_pid101005/Pass01058.pt
[2026-01-05 18:18:50] Dump 01059th pass to TP0_PP0_Rank0_pid101005/Pass01059.pt
[2026-01-05 18:18:50] Dump 01060th pass to TP0_PP0_Rank0_pid101005/Pass01060.pt
[2026-01-05 18:18:50] Dump 01061th pass to TP0_PP0_Rank0_pid101005/Pass01061.pt
[2026-01-05 18:18:50] Dump 01062th pass to TP0_PP0_Rank0_pid101005/Pass01062.pt
[2026-01-05 18:18:50] Dump 01063th pass to TP0_PP0_Rank0_pid101005/Pass01063.pt
[2026-01-05 18:18:50] Dump 01064th pass to TP0_PP0_Rank0_pid101005/Pass01064.pt
[2026-01-05 18:18:50] Dump 01065th pass to TP0_PP0_Rank0_pid101005/Pass01065.pt
[2026-01-05 18:18:50] Dump 01066th pass to TP0_PP0_Rank0_pid101005/Pass01066.pt
[2026-01-05 18:18:50] Dump 01067th pass to TP0_PP0_Rank0_pid101005/Pass01067.pt
[2026-01-05 18:18:50] Dump 01068th pass to TP0_PP0_Rank0_pid101005/Pass01068.pt
[2026-01-05 18:18:50] Dump 01069th pass to TP0_PP0_Rank0_pid101005/Pass01069.pt
[2026-01-05 18:18:51] Dump 01070th pass to TP0_PP0_Rank0_pid101005/Pass01070.pt
[2026-01-05 18:18:51] Dump 01071th pass to TP0_PP0_Rank0_pid101005/Pass01071.pt
[2026-01-05 18:18:51] Dump 01072th pass to TP0_PP0_Rank0_pid101005/Pass01072.pt
[2026-01-05 18:18:51] Dump 01073th pass to TP0_PP0_Rank0_pid101005/Pass01073.pt
[2026-01-05 18:18:51] Dump 01074th pass to TP0_PP0_Rank0_pid101005/Pass01074.pt
[2026-01-05 18:18:51] Dump 01075th pass to TP0_PP0_Rank0_pid101005/Pass01075.pt
[2026-01-05 18:18:51] Dump 01076th pass to TP0_PP0_Rank0_pid101005/Pass01076.pt
[2026-01-05 18:18:51] Dump 01077th pass to TP0_PP0_Rank0_pid101005/Pass01077.pt
[2026-01-05 18:18:51] Dump 01078th pass to TP0_PP0_Rank0_pid101005/Pass01078.pt
[2026-01-05 18:18:51] Dump 01079th pass to TP0_PP0_Rank0_pid101005/Pass01079.pt
[2026-01-05 18:18:51] Dump 01080th pass to TP0_PP0_Rank0_pid101005/Pass01080.pt
[2026-01-05 18:18:51] Dump 01081th pass to TP0_PP0_Rank0_pid101005/Pass01081.pt
[2026-01-05 18:18:52] Dump 01082th pass to TP0_PP0_Rank0_pid101005/Pass01082.pt
[2026-01-05 18:18:52] Dump 01083th pass to TP0_PP0_Rank0_pid101005/Pass01083.pt
[2026-01-05 18:18:52] Dump 01084th pass to TP0_PP0_Rank0_pid101005/Pass01084.pt
[2026-01-05 18:18:52] Dump 01085th pass to TP0_PP0_Rank0_pid101005/Pass01085.pt
[2026-01-05 18:18:52] Dump 01086th pass to TP0_PP0_Rank0_pid101005/Pass01086.pt
[2026-01-05 18:18:52] Dump 01087th pass to TP0_PP0_Rank0_pid101005/Pass01087.pt
[2026-01-05 18:18:52] Dump 01088th pass to TP0_PP0_Rank0_pid101005/Pass01088.pt
[2026-01-05 18:18:52] Dump 01089th pass to TP0_PP0_Rank0_pid101005/Pass01089.pt
[2026-01-05 18:18:52] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 12.12, #queue-req: 0,
[2026-01-05 18:18:52] Dump 01090th pass to TP0_PP0_Rank0_pid101005/Pass01090.pt
[2026-01-05 18:18:52] Dump 01091th pass to TP0_PP0_Rank0_pid101005/Pass01091.pt
[2026-01-05 18:18:52] Dump 01092th pass to TP0_PP0_Rank0_pid101005/Pass01092.pt
[2026-01-05 18:18:53] Dump 01093th pass to TP0_PP0_Rank0_pid101005/Pass01093.pt
[2026-01-05 18:18:53] Dump 01094th pass to TP0_PP0_Rank0_pid101005/Pass01094.pt
[2026-01-05 18:18:53] Dump 01095th pass to TP0_PP0_Rank0_pid101005/Pass01095.pt
[2026-01-05 18:18:53] Dump 01096th pass to TP0_PP0_Rank0_pid101005/Pass01096.pt
[2026-01-05 18:18:53] Dump 01097th pass to TP0_PP0_Rank0_pid101005/Pass01097.pt
[2026-01-05 18:18:53] Dump 01098th pass to TP0_PP0_Rank0_pid101005/Pass01098.pt
[2026-01-05 18:18:53] Dump 01099th pass to TP0_PP0_Rank0_pid101005/Pass01099.pt
[2026-01-05 18:18:53] Dump 01100th pass to TP0_PP0_Rank0_pid101005/Pass01100.pt
[2026-01-05 18:18:53] Dump 01101th pass to TP0_PP0_Rank0_pid101005/Pass01101.pt
[2026-01-05 18:18:53] Dump 01102th pass to TP0_PP0_Rank0_pid101005/Pass01102.pt
[2026-01-05 18:18:53] Dump 01103th pass to TP0_PP0_Rank0_pid101005/Pass01103.pt
[2026-01-05 18:18:53] Dump 01104th pass to TP0_PP0_Rank0_pid101005/Pass01104.pt
[2026-01-05 18:18:54] Dump 01105th pass to TP0_PP0_Rank0_pid101005/Pass01105.pt
[2026-01-05 18:18:54] Dump 01106th pass to TP0_PP0_Rank0_pid101005/Pass01106.pt
[2026-01-05 18:18:54] Dump 01107th pass to TP0_PP0_Rank0_pid101005/Pass01107.pt
[2026-01-05 18:18:54] Dump 01108th pass to TP0_PP0_Rank0_pid101005/Pass01108.pt
[2026-01-05 18:18:54] Dump 01109th pass to TP0_PP0_Rank0_pid101005/Pass01109.pt
[2026-01-05 18:18:54] Dump 01110th pass to TP0_PP0_Rank0_pid101005/Pass01110.pt
[2026-01-05 18:18:54] Dump 01111th pass to TP0_PP0_Rank0_pid101005/Pass01111.pt
[2026-01-05 18:18:54] Dump 01112th pass to TP0_PP0_Rank0_pid101005/Pass01112.pt
[2026-01-05 18:18:54] Dump 01113th pass to TP0_PP0_Rank0_pid101005/Pass01113.pt
[2026-01-05 18:18:54] Dump 01114th pass to TP0_PP0_Rank0_pid101005/Pass01114.pt
[2026-01-05 18:18:54] Dump 01115th pass to TP0_PP0_Rank0_pid101005/Pass01115.pt
[2026-01-05 18:18:54] Dump 01116th pass to TP0_PP0_Rank0_pid101005/Pass01116.pt
[2026-01-05 18:18:55] Dump 01117th pass to TP0_PP0_Rank0_pid101005/Pass01117.pt
[2026-01-05 18:18:55] Dump 01118th pass to TP0_PP0_Rank0_pid101005/Pass01118.pt
[2026-01-05 18:18:55] Dump 01119th pass to TP0_PP0_Rank0_pid101005/Pass01119.pt
[2026-01-05 18:18:55] Dump 01120th pass to TP0_PP0_Rank0_pid101005/Pass01120.pt
[2026-01-05 18:18:55] Dump 01121th pass to TP0_PP0_Rank0_pid101005/Pass01121.pt
[2026-01-05 18:18:55] Dump 01122th pass to TP0_PP0_Rank0_pid101005/Pass01122.pt
[2026-01-05 18:18:55] Dump 01123th pass to TP0_PP0_Rank0_pid101005/Pass01123.pt
[2026-01-05 18:18:55] Dump 01124th pass to TP0_PP0_Rank0_pid101005/Pass01124.pt
[2026-01-05 18:18:55] Dump 01125th pass to TP0_PP0_Rank0_pid101005/Pass01125.pt
[2026-01-05 18:18:55] Dump 01126th pass to TP0_PP0_Rank0_pid101005/Pass01126.pt
[2026-01-05 18:18:55] Dump 01127th pass to TP0_PP0_Rank0_pid101005/Pass01127.pt
[2026-01-05 18:18:55] Dump 01128th pass to TP0_PP0_Rank0_pid101005/Pass01128.pt
[2026-01-05 18:18:56] Dump 01129th pass to TP0_PP0_Rank0_pid101005/Pass01129.pt
[2026-01-05 18:18:56] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.96, #queue-req: 0,
[2026-01-05 18:18:56] Dump 01130th pass to TP0_PP0_Rank0_pid101005/Pass01130.pt
[2026-01-05 18:18:56] Dump 01131th pass to TP0_PP0_Rank0_pid101005/Pass01131.pt
[2026-01-05 18:18:56] Dump 01132th pass to TP0_PP0_Rank0_pid101005/Pass01132.pt
[2026-01-05 18:18:56] Dump 01133th pass to TP0_PP0_Rank0_pid101005/Pass01133.pt
[2026-01-05 18:18:56] Dump 01134th pass to TP0_PP0_Rank0_pid101005/Pass01134.pt
[2026-01-05 18:18:56] Dump 01135th pass to TP0_PP0_Rank0_pid101005/Pass01135.pt
[2026-01-05 18:18:56] Dump 01136th pass to TP0_PP0_Rank0_pid101005/Pass01136.pt
[2026-01-05 18:18:56] Dump 01137th pass to TP0_PP0_Rank0_pid101005/Pass01137.pt
[2026-01-05 18:18:56] Dump 01138th pass to TP0_PP0_Rank0_pid101005/Pass01138.pt
[2026-01-05 18:18:56] Dump 01139th pass to TP0_PP0_Rank0_pid101005/Pass01139.pt
[2026-01-05 18:18:56] Dump 01140th pass to TP0_PP0_Rank0_pid101005/Pass01140.pt
[2026-01-05 18:18:57] Dump 01141th pass to TP0_PP0_Rank0_pid101005/Pass01141.pt
[2026-01-05 18:18:57] Dump 01142th pass to TP0_PP0_Rank0_pid101005/Pass01142.pt
[2026-01-05 18:18:57] Dump 01143th pass to TP0_PP0_Rank0_pid101005/Pass01143.pt
[2026-01-05 18:18:57] Dump 01144th pass to TP0_PP0_Rank0_pid101005/Pass01144.pt
[2026-01-05 18:18:57] Dump 01145th pass to TP0_PP0_Rank0_pid101005/Pass01145.pt
[2026-01-05 18:18:57] Dump 01146th pass to TP0_PP0_Rank0_pid101005/Pass01146.pt
[2026-01-05 18:18:57] Dump 01147th pass to TP0_PP0_Rank0_pid101005/Pass01147.pt
[2026-01-05 18:18:57] Dump 01148th pass to TP0_PP0_Rank0_pid101005/Pass01148.pt
[2026-01-05 18:18:57] Dump 01149th pass to TP0_PP0_Rank0_pid101005/Pass01149.pt
[2026-01-05 18:18:57] Dump 01150th pass to TP0_PP0_Rank0_pid101005/Pass01150.pt
[2026-01-05 18:18:57] Dump 01151th pass to TP0_PP0_Rank0_pid101005/Pass01151.pt
[2026-01-05 18:18:57] Dump 01152th pass to TP0_PP0_Rank0_pid101005/Pass01152.pt
[2026-01-05 18:18:58] Dump 01153th pass to TP0_PP0_Rank0_pid101005/Pass01153.pt
[2026-01-05 18:18:58] Dump 01154th pass to TP0_PP0_Rank0_pid101005/Pass01154.pt
[2026-01-05 18:18:58] Dump 01155th pass to TP0_PP0_Rank0_pid101005/Pass01155.pt
[2026-01-05 18:18:58] Dump 01156th pass to TP0_PP0_Rank0_pid101005/Pass01156.pt
[2026-01-05 18:18:58] Dump 01157th pass to TP0_PP0_Rank0_pid101005/Pass01157.pt
[2026-01-05 18:18:58] Dump 01158th pass to TP0_PP0_Rank0_pid101005/Pass01158.pt
[2026-01-05 18:18:58] Dump 01159th pass to TP0_PP0_Rank0_pid101005/Pass01159.pt
[2026-01-05 18:18:58] Dump 01160th pass to TP0_PP0_Rank0_pid101005/Pass01160.pt
[2026-01-05 18:18:58] Dump 01161th pass to TP0_PP0_Rank0_pid101005/Pass01161.pt
[2026-01-05 18:18:58] Dump 01162th pass to TP0_PP0_Rank0_pid101005/Pass01162.pt
[2026-01-05 18:18:58] Dump 01163th pass to TP0_PP0_Rank0_pid101005/Pass01163.pt
[2026-01-05 18:18:58] Dump 01164th pass to TP0_PP0_Rank0_pid101005/Pass01164.pt
[2026-01-05 18:18:59] Dump 01165th pass to TP0_PP0_Rank0_pid101005/Pass01165.pt
[2026-01-05 18:18:59] Dump 01166th pass to TP0_PP0_Rank0_pid101005/Pass01166.pt
[2026-01-05 18:18:59] Dump 01167th pass to TP0_PP0_Rank0_pid101005/Pass01167.pt
[2026-01-05 18:18:59] Dump 01168th pass to TP0_PP0_Rank0_pid101005/Pass01168.pt
[2026-01-05 18:18:59] Dump 01169th pass to TP0_PP0_Rank0_pid101005/Pass01169.pt
[2026-01-05 18:18:59] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 12.07, #queue-req: 0,
[2026-01-05 18:18:59] Dump 01170th pass to TP0_PP0_Rank0_pid101005/Pass01170.pt
[2026-01-05 18:18:59] Dump 01171th pass to TP0_PP0_Rank0_pid101005/Pass01171.pt
[2026-01-05 18:18:59] Dump 01172th pass to TP0_PP0_Rank0_pid101005/Pass01172.pt
[2026-01-05 18:18:59] Dump 01173th pass to TP0_PP0_Rank0_pid101005/Pass01173.pt
[2026-01-05 18:18:59] Dump 01174th pass to TP0_PP0_Rank0_pid101005/Pass01174.pt
[2026-01-05 18:18:59] Dump 01175th pass to TP0_PP0_Rank0_pid101005/Pass01175.pt
[2026-01-05 18:18:59] Dump 01176th pass to TP0_PP0_Rank0_pid101005/Pass01176.pt
[2026-01-05 18:18:59] Dump 01177th pass to TP0_PP0_Rank0_pid101005/Pass01177.pt
[2026-01-05 18:19:00] Dump 01178th pass to TP0_PP0_Rank0_pid101005/Pass01178.pt
[2026-01-05 18:19:00] Dump 01179th pass to TP0_PP0_Rank0_pid101005/Pass01179.pt
[2026-01-05 18:19:00] Dump 01180th pass to TP0_PP0_Rank0_pid101005/Pass01180.pt
[2026-01-05 18:19:00] Dump 01181th pass to TP0_PP0_Rank0_pid101005/Pass01181.pt
[2026-01-05 18:19:00] Dump 01182th pass to TP0_PP0_Rank0_pid101005/Pass01182.pt
[2026-01-05 18:19:00] Dump 01183th pass to TP0_PP0_Rank0_pid101005/Pass01183.pt
[2026-01-05 18:19:00] Dump 01184th pass to TP0_PP0_Rank0_pid101005/Pass01184.pt
[2026-01-05 18:19:00] Dump 01185th pass to TP0_PP0_Rank0_pid101005/Pass01185.pt
[2026-01-05 18:19:00] Dump 01186th pass to TP0_PP0_Rank0_pid101005/Pass01186.pt
[2026-01-05 18:19:00] Dump 01187th pass to TP0_PP0_Rank0_pid101005/Pass01187.pt
[2026-01-05 18:19:00] Dump 01188th pass to TP0_PP0_Rank0_pid101005/Pass01188.pt
[2026-01-05 18:19:00] Dump 01189th pass to TP0_PP0_Rank0_pid101005/Pass01189.pt
[2026-01-05 18:19:01] Dump 01190th pass to TP0_PP0_Rank0_pid101005/Pass01190.pt
[2026-01-05 18:19:01] Dump 01191th pass to TP0_PP0_Rank0_pid101005/Pass01191.pt
[2026-01-05 18:19:01] Dump 01192th pass to TP0_PP0_Rank0_pid101005/Pass01192.pt
[2026-01-05 18:19:01] Dump 01193th pass to TP0_PP0_Rank0_pid101005/Pass01193.pt
[2026-01-05 18:19:01] Dump 01194th pass to TP0_PP0_Rank0_pid101005/Pass01194.pt
[2026-01-05 18:19:01] Dump 01195th pass to TP0_PP0_Rank0_pid101005/Pass01195.pt
[2026-01-05 18:19:01] Dump 01196th pass to TP0_PP0_Rank0_pid101005/Pass01196.pt
[2026-01-05 18:19:01] Dump 01197th pass to TP0_PP0_Rank0_pid101005/Pass01197.pt
[2026-01-05 18:19:01] Dump 01198th pass to TP0_PP0_Rank0_pid101005/Pass01198.pt
[2026-01-05 18:19:01] Dump 01199th pass to TP0_PP0_Rank0_pid101005/Pass01199.pt
[2026-01-05 18:19:01] Dump 01200th pass to TP0_PP0_Rank0_pid101005/Pass01200.pt
[2026-01-05 18:19:02] Dump 01201th pass to TP0_PP0_Rank0_pid101005/Pass01201.pt
[2026-01-05 18:19:02] Dump 01202th pass to TP0_PP0_Rank0_pid101005/Pass01202.pt
[2026-01-05 18:19:02] Dump 01203th pass to TP0_PP0_Rank0_pid101005/Pass01203.pt
[2026-01-05 18:19:02] Dump 01204th pass to TP0_PP0_Rank0_pid101005/Pass01204.pt
[2026-01-05 18:19:02] Dump 01205th pass to TP0_PP0_Rank0_pid101005/Pass01205.pt
[2026-01-05 18:19:02] Dump 01206th pass to TP0_PP0_Rank0_pid101005/Pass01206.pt
[2026-01-05 18:19:02] Dump 01207th pass to TP0_PP0_Rank0_pid101005/Pass01207.pt
[2026-01-05 18:19:02] Dump 01208th pass to TP0_PP0_Rank0_pid101005/Pass01208.pt
[2026-01-05 18:19:02] Dump 01209th pass to TP0_PP0_Rank0_pid101005/Pass01209.pt
[2026-01-05 18:19:02] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.91, #queue-req: 0,
[2026-01-05 18:19:02] Dump 01210th pass to TP0_PP0_Rank0_pid101005/Pass01210.pt
[2026-01-05 18:19:02] Dump 01211th pass to TP0_PP0_Rank0_pid101005/Pass01211.pt
[2026-01-05 18:19:02] Dump 01212th pass to TP0_PP0_Rank0_pid101005/Pass01212.pt
[2026-01-05 18:19:03] Dump 01213th pass to TP0_PP0_Rank0_pid101005/Pass01213.pt
[2026-01-05 18:19:03] Dump 01214th pass to TP0_PP0_Rank0_pid101005/Pass01214.pt
[2026-01-05 18:19:03] Dump 01215th pass to TP0_PP0_Rank0_pid101005/Pass01215.pt
[2026-01-05 18:19:03] Dump 01216th pass to TP0_PP0_Rank0_pid101005/Pass01216.pt
[2026-01-05 18:19:03] Dump 01217th pass to TP0_PP0_Rank0_pid101005/Pass01217.pt
[2026-01-05 18:19:03] Dump 01218th pass to TP0_PP0_Rank0_pid101005/Pass01218.pt
[2026-01-05 18:19:03] Dump 01219th pass to TP0_PP0_Rank0_pid101005/Pass01219.pt
[2026-01-05 18:19:03] Dump 01220th pass to TP0_PP0_Rank0_pid101005/Pass01220.pt
[2026-01-05 18:19:03] Dump 01221th pass to TP0_PP0_Rank0_pid101005/Pass01221.pt
[2026-01-05 18:19:03] Dump 01222th pass to TP0_PP0_Rank0_pid101005/Pass01222.pt
[2026-01-05 18:19:03] Dump 01223th pass to TP0_PP0_Rank0_pid101005/Pass01223.pt
[2026-01-05 18:19:03] Dump 01224th pass to TP0_PP0_Rank0_pid101005/Pass01224.pt
[2026-01-05 18:19:04] Dump 01225th pass to TP0_PP0_Rank0_pid101005/Pass01225.pt
[2026-01-05 18:19:04] Dump 01226th pass to TP0_PP0_Rank0_pid101005/Pass01226.pt
[2026-01-05 18:19:04] Dump 01227th pass to TP0_PP0_Rank0_pid101005/Pass01227.pt
[2026-01-05 18:19:04] Dump 01228th pass to TP0_PP0_Rank0_pid101005/Pass01228.pt
[2026-01-05 18:19:04] Dump 01229th pass to TP0_PP0_Rank0_pid101005/Pass01229.pt
[2026-01-05 18:19:04] Dump 01230th pass to TP0_PP0_Rank0_pid101005/Pass01230.pt
[2026-01-05 18:19:04] Dump 01231th pass to TP0_PP0_Rank0_pid101005/Pass01231.pt
[2026-01-05 18:19:04] Dump 01232th pass to TP0_PP0_Rank0_pid101005/Pass01232.pt
[2026-01-05 18:19:04] Dump 01233th pass to TP0_PP0_Rank0_pid101005/Pass01233.pt
[2026-01-05 18:19:04] Dump 01234th pass to TP0_PP0_Rank0_pid101005/Pass01234.pt
[2026-01-05 18:19:04] Dump 01235th pass to TP0_PP0_Rank0_pid101005/Pass01235.pt
[2026-01-05 18:19:05] Dump 01236th pass to TP0_PP0_Rank0_pid101005/Pass01236.pt
[2026-01-05 18:19:05] Dump 01237th pass to TP0_PP0_Rank0_pid101005/Pass01237.pt
[2026-01-05 18:19:05] Dump 01238th pass to TP0_PP0_Rank0_pid101005/Pass01238.pt
[2026-01-05 18:19:05] Dump 01239th pass to TP0_PP0_Rank0_pid101005/Pass01239.pt
[2026-01-05 18:19:05] Dump 01240th pass to TP0_PP0_Rank0_pid101005/Pass01240.pt
[2026-01-05 18:19:05] Dump 01241th pass to TP0_PP0_Rank0_pid101005/Pass01241.pt
[2026-01-05 18:19:05] Dump 01242th pass to TP0_PP0_Rank0_pid101005/Pass01242.pt
[2026-01-05 18:19:05] Dump 01243th pass to TP0_PP0_Rank0_pid101005/Pass01243.pt
[2026-01-05 18:19:05] Dump 01244th pass to TP0_PP0_Rank0_pid101005/Pass01244.pt
[2026-01-05 18:19:05] Dump 01245th pass to TP0_PP0_Rank0_pid101005/Pass01245.pt
[2026-01-05 18:19:05] Dump 01246th pass to TP0_PP0_Rank0_pid101005/Pass01246.pt
[2026-01-05 18:19:05] Dump 01247th pass to TP0_PP0_Rank0_pid101005/Pass01247.pt
[2026-01-05 18:19:06] Dump 01248th pass to TP0_PP0_Rank0_pid101005/Pass01248.pt
[2026-01-05 18:19:06] Dump 01249th pass to TP0_PP0_Rank0_pid101005/Pass01249.pt
[2026-01-05 18:19:06] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 11.61, #queue-req: 0,
[2026-01-05 18:19:06] Dump 01250th pass to TP0_PP0_Rank0_pid101005/Pass01250.pt
[2026-01-05 18:19:06] Dump 01251th pass to TP0_PP0_Rank0_pid101005/Pass01251.pt
[2026-01-05 18:19:06] Dump 01252th pass to TP0_PP0_Rank0_pid101005/Pass01252.pt
[2026-01-05 18:19:06] Dump 01253th pass to TP0_PP0_Rank0_pid101005/Pass01253.pt
[2026-01-05 18:19:06] Dump 01254th pass to TP0_PP0_Rank0_pid101005/Pass01254.pt
[2026-01-05 18:19:06] Dump 01255th pass to TP0_PP0_Rank0_pid101005/Pass01255.pt
[2026-01-05 18:19:06] Dump 01256th pass to TP0_PP0_Rank0_pid101005/Pass01256.pt
[2026-01-05 18:19:06] Dump 01257th pass to TP0_PP0_Rank0_pid101005/Pass01257.pt
[2026-01-05 18:19:06] Dump 01258th pass to TP0_PP0_Rank0_pid101005/Pass01258.pt
[2026-01-05 18:19:06] Dump 01259th pass to TP0_PP0_Rank0_pid101005/Pass01259.pt
[2026-01-05 18:19:07] Dump 01260th pass to TP0_PP0_Rank0_pid101005/Pass01260.pt
[2026-01-05 18:19:07] Dump 01261th pass to TP0_PP0_Rank0_pid101005/Pass01261.pt
[2026-01-05 18:19:07] Dump 01262th pass to TP0_PP0_Rank0_pid101005/Pass01262.pt
[2026-01-05 18:19:07] Dump 01263th pass to TP0_PP0_Rank0_pid101005/Pass01263.pt
[2026-01-05 18:19:07] Dump 01264th pass to TP0_PP0_Rank0_pid101005/Pass01264.pt
[2026-01-05 18:19:07] Dump 01265th pass to TP0_PP0_Rank0_pid101005/Pass01265.pt
[2026-01-05 18:19:07] Dump 01266th pass to TP0_PP0_Rank0_pid101005/Pass01266.pt
[2026-01-05 18:19:07] Dump 01267th pass to TP0_PP0_Rank0_pid101005/Pass01267.pt
[2026-01-05 18:19:07] Dump 01268th pass to TP0_PP0_Rank0_pid101005/Pass01268.pt
[2026-01-05 18:19:07] Dump 01269th pass to TP0_PP0_Rank0_pid101005/Pass01269.pt
[2026-01-05 18:19:07] Dump 01270th pass to TP0_PP0_Rank0_pid101005/Pass01270.pt
[2026-01-05 18:19:08] Dump 01271th pass to TP0_PP0_Rank0_pid101005/Pass01271.pt
[2026-01-05 18:19:08] Dump 01272th pass to TP0_PP0_Rank0_pid101005/Pass01272.pt
[2026-01-05 18:19:08] Dump 01273th pass to TP0_PP0_Rank0_pid101005/Pass01273.pt
[2026-01-05 18:19:08] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
100%|█████████▉| 199/200 [03:53<00:00,  1.10it/s]
100%|██████████| 200/200 [04:18<00:00,  8.20s/it]
100%|██████████| 200/200 [04:18<00:00,  1.29s/it]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/260105/ascend/function/test_ascend_pp_single_node.py", line 50, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.33) not greater than 0.38 : Accuracy of /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B is 0.33, is lower than 0.38
E
======================================================================
ERROR: test_gsm8k (__main__.TestQwenPPTieWeightsAccuracy.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.33) not greater than 0.38 : Accuracy of /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B is 0.33, is lower than 0.38

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 259.696s

FAILED (errors=1)
Accuracy: 0.330
Invalid: 0.005
Latency: 258.617 s
Output throughput: 66.918 token/s
.
.
End (16/106):
filename='ascend/function/test_ascend_pp_single_node.py', elapsed=279, estimated_time=400
.
.


✗ FAILED: ascend/function/test_ascend_pp_single_node.py returned exit code 1

.
.
Begin (17/106):
python3 /data/l30079981/260105/ascend/function/test_radix_attention.py
.
.

[2026-01-05 18:19:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:19:27] Dump 01274th pass to TP0_PP0_Rank0_pid101005/Pass01274.pt
[2026-01-05 18:19:27] Dump 01275th pass to TP0_PP0_Rank0_pid101005/Pass01275.pt
[2026-01-05 18:19:28] INFO:     127.0.0.1:51142 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 18:19:29] Prefill batch, #new-seq: 14, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 387,
[2026-01-05 18:19:30] Dump 01276th pass to TP0_PP0_Rank0_pid101005/Pass01276.pt
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b9042-7b6773fc227469cd4e241ddb;97a744a8-79e7-4151-9ed0-c0d5052f2cc2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b9042-7b6773fc227469cd4e241ddb;97a744a8-79e7-4151-9ed0-c0d5052f2cc2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-18:19:47 (PID:105775, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
[2026-01-05 18:19:57] Prefill batch, #new-seq: 17, #new-token: 8192, #cached-token: 4096, token usage: 0.01, #running-req: 13, #queue-req: 371,
[2026-01-05 18:19:58] Dump 01277th pass to TP0_PP0_Rank0_pid101005/Pass01277.pt
[2026-01-05 18:20:38] Prefill batch, #new-seq: 18, #new-token: 8192, #cached-token: 4992, token usage: 0.01, #running-req: 29, #queue-req: 354,
[2026-01-05 18:20:42] Dump 01278th pass to TP0_PP0_Rank0_pid101005/Pass01278.pt
[2026-01-05 18:21:17] Prefill batch, #new-seq: 22, #new-token: 8192, #cached-token: 7936, token usage: 0.01, #running-req: 46, #queue-req: 333,
[2026-01-05 18:21:19] Dump 01279th pass to TP0_PP0_Rank0_pid101005/Pass01279.pt
[2026-01-05 18:21:56] Prefill batch, #new-seq: 28, #new-token: 8192, #cached-token: 13184, token usage: 0.02, #running-req: 68, #queue-req: 305,
[2026-01-05 18:21:59] Dump 01280th pass to TP0_PP0_Rank0_pid101005/Pass01280.pt
[2026-01-05 18:22:32] Prefill batch, #new-seq: 28, #new-token: 8192, #cached-token: 17792, token usage: 0.02, #running-req: 96, #queue-req: 277,
[2026-01-05 18:22:36] Dump 01281th pass to TP0_PP0_Rank0_pid101005/Pass01281.pt
[2026-01-05 18:23:08] Prefill batch, #new-seq: 29, #new-token: 8192, #cached-token: 16256, token usage: 0.03, #running-req: 124, #queue-req: 248,
[2026-01-05 18:23:11] Dump 01282th pass to TP0_PP0_Rank0_pid101005/Pass01282.pt
[2026-01-05 18:23:46] Prefill batch, #new-seq: 35, #new-token: 8192, #cached-token: 20352, token usage: 0.03, #running-req: 153, #queue-req: 213,
[2026-01-05 18:23:48] Dump 01283th pass to TP0_PP0_Rank0_pid101005/Pass01283.pt
[2026-01-05 18:24:22] Prefill batch, #new-seq: 35, #new-token: 8192, #cached-token: 19584, token usage: 0.04, #running-req: 188, #queue-req: 178,
[2026-01-05 18:24:25] Dump 01284th pass to TP0_PP0_Rank0_pid101005/Pass01284.pt
[2026-01-05 18:24:57] Prefill batch, #new-seq: 35, #new-token: 8192, #cached-token: 20608, token usage: 0.04, #running-req: 223, #queue-req: 143,
[2026-01-05 18:25:01] Dump 01285th pass to TP0_PP0_Rank0_pid101005/Pass01285.pt
[2026-01-05 18:25:35] Prefill batch, #new-seq: 34, #new-token: 8192, #cached-token: 17280, token usage: 0.05, #running-req: 258, #queue-req: 109,
[2026-01-05 18:25:38] Dump 01286th pass to TP0_PP0_Rank0_pid101005/Pass01286.pt
[2026-01-05 18:26:13] Prefill batch, #new-seq: 33, #new-token: 8192, #cached-token: 18816, token usage: 0.05, #running-req: 292, #queue-req: 76,
[2026-01-05 18:26:16] Dump 01287th pass to TP0_PP0_Rank0_pid101005/Pass01287.pt
[2026-01-05 18:26:48] Prefill batch, #new-seq: 38, #new-token: 8192, #cached-token: 19328, token usage: 0.06, #running-req: 324, #queue-req: 39,
[2026-01-05 18:26:52] Dump 01288th pass to TP0_PP0_Rank0_pid101005/Pass01288.pt
[2026-01-05 18:27:24] Prefill batch, #new-seq: 34, #new-token: 8192, #cached-token: 17408, token usage: 0.06, #running-req: 362, #queue-req: 5,
[2026-01-05 18:27:27] Dump 01289th pass to TP0_PP0_Rank0_pid101005/Pass01289.pt
[2026-01-05 18:27:55] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 2944, token usage: 0.07, #running-req: 396, #queue-req: 0,
[2026-01-05 18:28:03] Dump 01290th pass to TP0_PP0_Rank0_pid101005/Pass01290.pt
[2026-01-05 18:28:06] Dump 01291th pass to TP0_PP0_Rank0_pid101005/Pass01291.pt
[2026-01-05 18:28:08] Dump 01292th pass to TP0_PP0_Rank0_pid101005/Pass01292.pt
[2026-01-05 18:28:11] Dump 01293th pass to TP0_PP0_Rank0_pid101005/Pass01293.pt
[2026-01-05 18:28:14] Dump 01294th pass to TP0_PP0_Rank0_pid101005/Pass01294.pt
[2026-01-05 18:28:17] Dump 01295th pass to TP0_PP0_Rank0_pid101005/Pass01295.pt
[2026-01-05 18:28:19] Dump 01296th pass to TP0_PP0_Rank0_pid101005/Pass01296.pt
[2026-01-05 18:28:22] Dump 01297th pass to TP0_PP0_Rank0_pid101005/Pass01297.pt
[2026-01-05 18:28:25] Dump 01298th pass to TP0_PP0_Rank0_pid101005/Pass01298.pt
[2026-01-05 18:28:28] Dump 01299th pass to TP0_PP0_Rank0_pid101005/Pass01299.pt
[2026-01-05 18:28:30] Dump 01300th pass to TP0_PP0_Rank0_pid101005/Pass01300.pt
[2026-01-05 18:28:32] Dump 01301th pass to TP0_PP0_Rank0_pid101005/Pass01301.pt
[2026-01-05 18:28:35] Dump 01302th pass to TP0_PP0_Rank0_pid101005/Pass01302.pt
[2026-01-05 18:28:37] Dump 01303th pass to TP0_PP0_Rank0_pid101005/Pass01303.pt
[2026-01-05 18:28:40] Dump 01304th pass to TP0_PP0_Rank0_pid101005/Pass01304.pt
[2026-01-05 18:28:43] Dump 01305th pass to TP0_PP0_Rank0_pid101005/Pass01305.pt
[2026-01-05 18:28:45] Decode batch, #running-req: 380, #token: 104704, token usage: 0.07, cpu graph: False, gen throughput (token/s): 9.49, #queue-req: 0,
[2026-01-05 18:28:45] Dump 01306th pass to TP0_PP0_Rank0_pid101005/Pass01306.pt
[2026-01-05 18:28:48] Dump 01307th pass to TP0_PP0_Rank0_pid101005/Pass01307.pt
[2026-01-05 18:28:51] Dump 01308th pass to TP0_PP0_Rank0_pid101005/Pass01308.pt
[2026-01-05 18:28:53] Dump 01309th pass to TP0_PP0_Rank0_pid101005/Pass01309.pt
[2026-01-05 18:28:56] Dump 01310th pass to TP0_PP0_Rank0_pid101005/Pass01310.pt
[2026-01-05 18:28:58] Dump 01311th pass to TP0_PP0_Rank0_pid101005/Pass01311.pt
[2026-01-05 18:29:00] Dump 01312th pass to TP0_PP0_Rank0_pid101005/Pass01312.pt
[2026-01-05 18:29:02] Dump 01313th pass to TP0_PP0_Rank0_pid101005/Pass01313.pt
[2026-01-05 18:29:05] Dump 01314th pass to TP0_PP0_Rank0_pid101005/Pass01314.pt
[2026-01-05 18:29:07] Dump 01315th pass to TP0_PP0_Rank0_pid101005/Pass01315.pt
[2026-01-05 18:29:10] Dump 01316th pass to TP0_PP0_Rank0_pid101005/Pass01316.pt
[2026-01-05 18:29:12] Dump 01317th pass to TP0_PP0_Rank0_pid101005/Pass01317.pt
[2026-01-05 18:29:15] Dump 01318th pass to TP0_PP0_Rank0_pid101005/Pass01318.pt
[2026-01-05 18:29:17] Dump 01319th pass to TP0_PP0_Rank0_pid101005/Pass01319.pt
[2026-01-05 18:29:19] Dump 01320th pass to TP0_PP0_Rank0_pid101005/Pass01320.pt
[2026-01-05 18:29:22] Dump 01321th pass to TP0_PP0_Rank0_pid101005/Pass01321.pt
[2026-01-05 18:29:24] Dump 01322th pass to TP0_PP0_Rank0_pid101005/Pass01322.pt
[2026-01-05 18:29:26] Dump 01323th pass to TP0_PP0_Rank0_pid101005/Pass01323.pt
[2026-01-05 18:29:29] Dump 01324th pass to TP0_PP0_Rank0_pid101005/Pass01324.pt
[2026-01-05 18:29:31] Dump 01325th pass to TP0_PP0_Rank0_pid101005/Pass01325.pt
[2026-01-05 18:29:33] Dump 01326th pass to TP0_PP0_Rank0_pid101005/Pass01326.pt
[2026-01-05 18:29:36] Dump 01327th pass to TP0_PP0_Rank0_pid101005/Pass01327.pt
[2026-01-05 18:29:38] Dump 01328th pass to TP0_PP0_Rank0_pid101005/Pass01328.pt
[2026-01-05 18:29:41] Dump 01329th pass to TP0_PP0_Rank0_pid101005/Pass01329.pt
[2026-01-05 18:29:44] Dump 01330th pass to TP0_PP0_Rank0_pid101005/Pass01330.pt
[2026-01-05 18:29:46] Dump 01331th pass to TP0_PP0_Rank0_pid101005/Pass01331.pt
[2026-01-05 18:29:49] Dump 01332th pass to TP0_PP0_Rank0_pid101005/Pass01332.pt
[2026-01-05 18:29:52] Dump 01333th pass to TP0_PP0_Rank0_pid101005/Pass01333.pt
[2026-01-05 18:29:54] Dump 01334th pass to TP0_PP0_Rank0_pid101005/Pass01334.pt
[2026-01-05 18:29:57] Dump 01335th pass to TP0_PP0_Rank0_pid101005/Pass01335.pt
[2026-01-05 18:29:59] Dump 01336th pass to TP0_PP0_Rank0_pid101005/Pass01336.pt
[2026-01-05 18:30:02] Dump 01337th pass to TP0_PP0_Rank0_pid101005/Pass01337.pt
[2026-01-05 18:30:04] Dump 01338th pass to TP0_PP0_Rank0_pid101005/Pass01338.pt
[2026-01-05 18:30:06] Dump 01339th pass to TP0_PP0_Rank0_pid101005/Pass01339.pt
[2026-01-05 18:30:09] Dump 01340th pass to TP0_PP0_Rank0_pid101005/Pass01340.pt
[2026-01-05 18:30:11] Dump 01341th pass to TP0_PP0_Rank0_pid101005/Pass01341.pt
[2026-01-05 18:30:14] Dump 01342th pass to TP0_PP0_Rank0_pid101005/Pass01342.pt
[2026-01-05 18:30:16] Dump 01343th pass to TP0_PP0_Rank0_pid101005/Pass01343.pt
[2026-01-05 18:30:18] Dump 01344th pass to TP0_PP0_Rank0_pid101005/Pass01344.pt
[2026-01-05 18:30:20] Dump 01345th pass to TP0_PP0_Rank0_pid101005/Pass01345.pt
[2026-01-05 18:30:23] Decode batch, #running-req: 317, #token: 101248, token usage: 0.07, cpu graph: False, gen throughput (token/s): 142.35, #queue-req: 0,
[2026-01-05 18:30:23] Dump 01346th pass to TP0_PP0_Rank0_pid101005/Pass01346.pt
[2026-01-05 18:30:26] Dump 01347th pass to TP0_PP0_Rank0_pid101005/Pass01347.pt
[2026-01-05 18:30:28] Dump 01348th pass to TP0_PP0_Rank0_pid101005/Pass01348.pt
[2026-01-05 18:30:31] Dump 01349th pass to TP0_PP0_Rank0_pid101005/Pass01349.pt
[2026-01-05 18:30:34] Dump 01350th pass to TP0_PP0_Rank0_pid101005/Pass01350.pt
[2026-01-05 18:30:36] Dump 01351th pass to TP0_PP0_Rank0_pid101005/Pass01351.pt
[2026-01-05 18:30:38] Dump 01352th pass to TP0_PP0_Rank0_pid101005/Pass01352.pt
[2026-01-05 18:30:41] Dump 01353th pass to TP0_PP0_Rank0_pid101005/Pass01353.pt
[2026-01-05 18:30:44] Dump 01354th pass to TP0_PP0_Rank0_pid101005/Pass01354.pt
[2026-01-05 18:30:46] Dump 01355th pass to TP0_PP0_Rank0_pid101005/Pass01355.pt
[2026-01-05 18:30:49] Dump 01356th pass to TP0_PP0_Rank0_pid101005/Pass01356.pt
[2026-01-05 18:30:51] Dump 01357th pass to TP0_PP0_Rank0_pid101005/Pass01357.pt
[2026-01-05 18:30:54] Dump 01358th pass to TP0_PP0_Rank0_pid101005/Pass01358.pt
[2026-01-05 18:30:56] Dump 01359th pass to TP0_PP0_Rank0_pid101005/Pass01359.pt
[2026-01-05 18:30:59] Dump 01360th pass to TP0_PP0_Rank0_pid101005/Pass01360.pt
[2026-01-05 18:31:01] Dump 01361th pass to TP0_PP0_Rank0_pid101005/Pass01361.pt
[2026-01-05 18:31:04] Dump 01362th pass to TP0_PP0_Rank0_pid101005/Pass01362.pt
[2026-01-05 18:31:06] Dump 01363th pass to TP0_PP0_Rank0_pid101005/Pass01363.pt
[2026-01-05 18:31:09] Dump 01364th pass to TP0_PP0_Rank0_pid101005/Pass01364.pt
[2026-01-05 18:31:11] Dump 01365th pass to TP0_PP0_Rank0_pid101005/Pass01365.pt
[2026-01-05 18:31:14] Dump 01366th pass to TP0_PP0_Rank0_pid101005/Pass01366.pt
[2026-01-05 18:31:16] Dump 01367th pass to TP0_PP0_Rank0_pid101005/Pass01367.pt
[2026-01-05 18:31:19] Dump 01368th pass to TP0_PP0_Rank0_pid101005/Pass01368.pt
[2026-01-05 18:31:21] Dump 01369th pass to TP0_PP0_Rank0_pid101005/Pass01369.pt
[2026-01-05 18:31:24] Dump 01370th pass to TP0_PP0_Rank0_pid101005/Pass01370.pt
[2026-01-05 18:31:26] Dump 01371th pass to TP0_PP0_Rank0_pid101005/Pass01371.pt
[2026-01-05 18:31:29] Dump 01372th pass to TP0_PP0_Rank0_pid101005/Pass01372.pt
[2026-01-05 18:31:31] Dump 01373th pass to TP0_PP0_Rank0_pid101005/Pass01373.pt
[2026-01-05 18:31:33] Dump 01374th pass to TP0_PP0_Rank0_pid101005/Pass01374.pt
[2026-01-05 18:31:36] Dump 01375th pass to TP0_PP0_Rank0_pid101005/Pass01375.pt
[2026-01-05 18:31:38] Dump 01376th pass to TP0_PP0_Rank0_pid101005/Pass01376.pt
[2026-01-05 18:31:41] Dump 01377th pass to TP0_PP0_Rank0_pid101005/Pass01377.pt
[2026-01-05 18:31:43] Dump 01378th pass to TP0_PP0_Rank0_pid101005/Pass01378.pt
[2026-01-05 18:31:45] Dump 01379th pass to TP0_PP0_Rank0_pid101005/Pass01379.pt
[2026-01-05 18:31:48] Dump 01380th pass to TP0_PP0_Rank0_pid101005/Pass01380.pt
[2026-01-05 18:31:50] Dump 01381th pass to TP0_PP0_Rank0_pid101005/Pass01381.pt
[2026-01-05 18:31:52] Dump 01382th pass to TP0_PP0_Rank0_pid101005/Pass01382.pt
[2026-01-05 18:31:54] Dump 01383th pass to TP0_PP0_Rank0_pid101005/Pass01383.pt
[2026-01-05 18:31:57] Dump 01384th pass to TP0_PP0_Rank0_pid101005/Pass01384.pt
[2026-01-05 18:31:59] Dump 01385th pass to TP0_PP0_Rank0_pid101005/Pass01385.pt
[2026-01-05 18:32:01] Decode batch, #running-req: 259, #token: 93696, token usage: 0.06, cpu graph: False, gen throughput (token/s): 117.01, #queue-req: 0,
[2026-01-05 18:32:01] Dump 01386th pass to TP0_PP0_Rank0_pid101005/Pass01386.pt
[2026-01-05 18:32:03] Dump 01387th pass to TP0_PP0_Rank0_pid101005/Pass01387.pt
[2026-01-05 18:32:06] Dump 01388th pass to TP0_PP0_Rank0_pid101005/Pass01388.pt
[2026-01-05 18:32:08] Dump 01389th pass to TP0_PP0_Rank0_pid101005/Pass01389.pt
[2026-01-05 18:32:10] Dump 01390th pass to TP0_PP0_Rank0_pid101005/Pass01390.pt
[2026-01-05 18:32:12] Dump 01391th pass to TP0_PP0_Rank0_pid101005/Pass01391.pt
[2026-01-05 18:32:14] Dump 01392th pass to TP0_PP0_Rank0_pid101005/Pass01392.pt
[2026-01-05 18:32:16] Dump 01393th pass to TP0_PP0_Rank0_pid101005/Pass01393.pt
[2026-01-05 18:32:18] Dump 01394th pass to TP0_PP0_Rank0_pid101005/Pass01394.pt
[2026-01-05 18:32:20] Dump 01395th pass to TP0_PP0_Rank0_pid101005/Pass01395.pt
[2026-01-05 18:32:22] Dump 01396th pass to TP0_PP0_Rank0_pid101005/Pass01396.pt
[2026-01-05 18:32:24] Dump 01397th pass to TP0_PP0_Rank0_pid101005/Pass01397.pt
[2026-01-05 18:32:26] Dump 01398th pass to TP0_PP0_Rank0_pid101005/Pass01398.pt
[2026-01-05 18:32:28] Dump 01399th pass to TP0_PP0_Rank0_pid101005/Pass01399.pt
[2026-01-05 18:32:30] Dump 01400th pass to TP0_PP0_Rank0_pid101005/Pass01400.pt
[2026-01-05 18:32:32] Dump 01401th pass to TP0_PP0_Rank0_pid101005/Pass01401.pt
[2026-01-05 18:32:35] Dump 01402th pass to TP0_PP0_Rank0_pid101005/Pass01402.pt
[2026-01-05 18:32:36] Dump 01403th pass to TP0_PP0_Rank0_pid101005/Pass01403.pt
[2026-01-05 18:32:38] Dump 01404th pass to TP0_PP0_Rank0_pid101005/Pass01404.pt
[2026-01-05 18:32:40] Dump 01405th pass to TP0_PP0_Rank0_pid101005/Pass01405.pt
[2026-01-05 18:32:42] Dump 01406th pass to TP0_PP0_Rank0_pid101005/Pass01406.pt
[2026-01-05 18:32:44] Dump 01407th pass to TP0_PP0_Rank0_pid101005/Pass01407.pt
[2026-01-05 18:32:45] Dump 01408th pass to TP0_PP0_Rank0_pid101005/Pass01408.pt
[2026-01-05 18:32:47] Dump 01409th pass to TP0_PP0_Rank0_pid101005/Pass01409.pt
[2026-01-05 18:32:49] Dump 01410th pass to TP0_PP0_Rank0_pid101005/Pass01410.pt
[2026-01-05 18:32:51] Dump 01411th pass to TP0_PP0_Rank0_pid101005/Pass01411.pt
[2026-01-05 18:32:53] Dump 01412th pass to TP0_PP0_Rank0_pid101005/Pass01412.pt
[2026-01-05 18:32:55] Dump 01413th pass to TP0_PP0_Rank0_pid101005/Pass01413.pt
[2026-01-05 18:32:57] Dump 01414th pass to TP0_PP0_Rank0_pid101005/Pass01414.pt
[2026-01-05 18:32:59] Dump 01415th pass to TP0_PP0_Rank0_pid101005/Pass01415.pt
[2026-01-05 18:33:00] Dump 01416th pass to TP0_PP0_Rank0_pid101005/Pass01416.pt
[2026-01-05 18:33:02] Dump 01417th pass to TP0_PP0_Rank0_pid101005/Pass01417.pt
[2026-01-05 18:33:03] Dump 01418th pass to TP0_PP0_Rank0_pid101005/Pass01418.pt
[2026-01-05 18:33:05] Dump 01419th pass to TP0_PP0_Rank0_pid101005/Pass01419.pt
[2026-01-05 18:33:07] Dump 01420th pass to TP0_PP0_Rank0_pid101005/Pass01420.pt
[2026-01-05 18:33:08] Dump 01421th pass to TP0_PP0_Rank0_pid101005/Pass01421.pt
[2026-01-05 18:33:10] Dump 01422th pass to TP0_PP0_Rank0_pid101005/Pass01422.pt
[2026-01-05 18:33:12] Dump 01423th pass to TP0_PP0_Rank0_pid101005/Pass01423.pt
[2026-01-05 18:33:13] Dump 01424th pass to TP0_PP0_Rank0_pid101005/Pass01424.pt
[2026-01-05 18:33:15] Dump 01425th pass to TP0_PP0_Rank0_pid101005/Pass01425.pt
[2026-01-05 18:33:16] Decode batch, #running-req: 198, #token: 81920, token usage: 0.06, cpu graph: False, gen throughput (token/s): 120.60, #queue-req: 0,
[2026-01-05 18:33:17] Dump 01426th pass to TP0_PP0_Rank0_pid101005/Pass01426.pt
[2026-01-05 18:33:18] Dump 01427th pass to TP0_PP0_Rank0_pid101005/Pass01427.pt
[2026-01-05 18:33:20] Dump 01428th pass to TP0_PP0_Rank0_pid101005/Pass01428.pt
[2026-01-05 18:33:21] Dump 01429th pass to TP0_PP0_Rank0_pid101005/Pass01429.pt
[2026-01-05 18:33:23] Dump 01430th pass to TP0_PP0_Rank0_pid101005/Pass01430.pt
[2026-01-05 18:33:24] Dump 01431th pass to TP0_PP0_Rank0_pid101005/Pass01431.pt
[2026-01-05 18:33:26] Dump 01432th pass to TP0_PP0_Rank0_pid101005/Pass01432.pt
[2026-01-05 18:33:28] Dump 01433th pass to TP0_PP0_Rank0_pid101005/Pass01433.pt
[2026-01-05 18:33:29] Dump 01434th pass to TP0_PP0_Rank0_pid101005/Pass01434.pt
[2026-01-05 18:33:30] Dump 01435th pass to TP0_PP0_Rank0_pid101005/Pass01435.pt
[2026-01-05 18:33:32] Dump 01436th pass to TP0_PP0_Rank0_pid101005/Pass01436.pt
[2026-01-05 18:33:33] Dump 01437th pass to TP0_PP0_Rank0_pid101005/Pass01437.pt
[2026-01-05 18:33:35] Dump 01438th pass to TP0_PP0_Rank0_pid101005/Pass01438.pt
[2026-01-05 18:33:36] Dump 01439th pass to TP0_PP0_Rank0_pid101005/Pass01439.pt
[2026-01-05 18:33:38] Dump 01440th pass to TP0_PP0_Rank0_pid101005/Pass01440.pt
[2026-01-05 18:33:39] Dump 01441th pass to TP0_PP0_Rank0_pid101005/Pass01441.pt
[2026-01-05 18:33:40] Dump 01442th pass to TP0_PP0_Rank0_pid101005/Pass01442.pt
[2026-01-05 18:33:42] Dump 01443th pass to TP0_PP0_Rank0_pid101005/Pass01443.pt
[2026-01-05 18:33:43] Dump 01444th pass to TP0_PP0_Rank0_pid101005/Pass01444.pt
[2026-01-05 18:33:44] Dump 01445th pass to TP0_PP0_Rank0_pid101005/Pass01445.pt
[2026-01-05 18:33:46] Dump 01446th pass to TP0_PP0_Rank0_pid101005/Pass01446.pt
[2026-01-05 18:33:47] Dump 01447th pass to TP0_PP0_Rank0_pid101005/Pass01447.pt
[2026-01-05 18:33:48] Dump 01448th pass to TP0_PP0_Rank0_pid101005/Pass01448.pt
[2026-01-05 18:33:50] Dump 01449th pass to TP0_PP0_Rank0_pid101005/Pass01449.pt
[2026-01-05 18:33:51] Dump 01450th pass to TP0_PP0_Rank0_pid101005/Pass01450.pt
[2026-01-05 18:33:52] Dump 01451th pass to TP0_PP0_Rank0_pid101005/Pass01451.pt
[2026-01-05 18:33:53] Dump 01452th pass to TP0_PP0_Rank0_pid101005/Pass01452.pt
[2026-01-05 18:33:55] Dump 01453th pass to TP0_PP0_Rank0_pid101005/Pass01453.pt
[2026-01-05 18:33:56] Dump 01454th pass to TP0_PP0_Rank0_pid101005/Pass01454.pt
[2026-01-05 18:33:57] Dump 01455th pass to TP0_PP0_Rank0_pid101005/Pass01455.pt
[2026-01-05 18:33:58] Dump 01456th pass to TP0_PP0_Rank0_pid101005/Pass01456.pt
[2026-01-05 18:33:59] Dump 01457th pass to TP0_PP0_Rank0_pid101005/Pass01457.pt
[2026-01-05 18:34:00] Dump 01458th pass to TP0_PP0_Rank0_pid101005/Pass01458.pt
[2026-01-05 18:34:01] Dump 01459th pass to TP0_PP0_Rank0_pid101005/Pass01459.pt
[2026-01-05 18:34:03] Dump 01460th pass to TP0_PP0_Rank0_pid101005/Pass01460.pt
[2026-01-05 18:34:04] Dump 01461th pass to TP0_PP0_Rank0_pid101005/Pass01461.pt
[2026-01-05 18:34:05] Dump 01462th pass to TP0_PP0_Rank0_pid101005/Pass01462.pt
[2026-01-05 18:34:06] Dump 01463th pass to TP0_PP0_Rank0_pid101005/Pass01463.pt
[2026-01-05 18:34:07] Dump 01464th pass to TP0_PP0_Rank0_pid101005/Pass01464.pt
[2026-01-05 18:34:08] Dump 01465th pass to TP0_PP0_Rank0_pid101005/Pass01465.pt
[2026-01-05 18:34:09] Decode batch, #running-req: 126, #token: 59264, token usage: 0.04, cpu graph: False, gen throughput (token/s): 122.93, #queue-req: 0,
[2026-01-05 18:34:09] Dump 01466th pass to TP0_PP0_Rank0_pid101005/Pass01466.pt
[2026-01-05 18:34:10] Dump 01467th pass to TP0_PP0_Rank0_pid101005/Pass01467.pt
[2026-01-05 18:34:11] Dump 01468th pass to TP0_PP0_Rank0_pid101005/Pass01468.pt
[2026-01-05 18:34:12] Dump 01469th pass to TP0_PP0_Rank0_pid101005/Pass01469.pt
[2026-01-05 18:34:13] Dump 01470th pass to TP0_PP0_Rank0_pid101005/Pass01470.pt
[2026-01-05 18:34:14] Dump 01471th pass to TP0_PP0_Rank0_pid101005/Pass01471.pt
[2026-01-05 18:34:15] Dump 01472th pass to TP0_PP0_Rank0_pid101005/Pass01472.pt
[2026-01-05 18:34:16] Dump 01473th pass to TP0_PP0_Rank0_pid101005/Pass01473.pt
[2026-01-05 18:34:17] Dump 01474th pass to TP0_PP0_Rank0_pid101005/Pass01474.pt
[2026-01-05 18:34:18] Dump 01475th pass to TP0_PP0_Rank0_pid101005/Pass01475.pt
[2026-01-05 18:34:19] Dump 01476th pass to TP0_PP0_Rank0_pid101005/Pass01476.pt
[2026-01-05 18:34:20] Dump 01477th pass to TP0_PP0_Rank0_pid101005/Pass01477.pt
[2026-01-05 18:34:21] Dump 01478th pass to TP0_PP0_Rank0_pid101005/Pass01478.pt
[2026-01-05 18:34:21] Dump 01479th pass to TP0_PP0_Rank0_pid101005/Pass01479.pt
[2026-01-05 18:34:22] Dump 01480th pass to TP0_PP0_Rank0_pid101005/Pass01480.pt
[2026-01-05 18:34:23] Dump 01481th pass to TP0_PP0_Rank0_pid101005/Pass01481.pt
[2026-01-05 18:34:24] Dump 01482th pass to TP0_PP0_Rank0_pid101005/Pass01482.pt
[2026-01-05 18:34:25] Dump 01483th pass to TP0_PP0_Rank0_pid101005/Pass01483.pt
[2026-01-05 18:34:26] Dump 01484th pass to TP0_PP0_Rank0_pid101005/Pass01484.pt
[2026-01-05 18:34:27] Dump 01485th pass to TP0_PP0_Rank0_pid101005/Pass01485.pt
[2026-01-05 18:34:28] Dump 01486th pass to TP0_PP0_Rank0_pid101005/Pass01486.pt
[2026-01-05 18:34:29] Dump 01487th pass to TP0_PP0_Rank0_pid101005/Pass01487.pt
[2026-01-05 18:34:29] Dump 01488th pass to TP0_PP0_Rank0_pid101005/Pass01488.pt
[2026-01-05 18:34:30] Dump 01489th pass to TP0_PP0_Rank0_pid101005/Pass01489.pt
[2026-01-05 18:34:31] Dump 01490th pass to TP0_PP0_Rank0_pid101005/Pass01490.pt
[2026-01-05 18:34:32] Dump 01491th pass to TP0_PP0_Rank0_pid101005/Pass01491.pt
[2026-01-05 18:34:33] Dump 01492th pass to TP0_PP0_Rank0_pid101005/Pass01492.pt
[2026-01-05 18:34:33] Dump 01493th pass to TP0_PP0_Rank0_pid101005/Pass01493.pt
[2026-01-05 18:34:34] Dump 01494th pass to TP0_PP0_Rank0_pid101005/Pass01494.pt
[2026-01-05 18:34:35] Dump 01495th pass to TP0_PP0_Rank0_pid101005/Pass01495.pt
[2026-01-05 18:34:36] Dump 01496th pass to TP0_PP0_Rank0_pid101005/Pass01496.pt
[2026-01-05 18:34:37] Dump 01497th pass to TP0_PP0_Rank0_pid101005/Pass01497.pt
[2026-01-05 18:34:37] Dump 01498th pass to TP0_PP0_Rank0_pid101005/Pass01498.pt
[2026-01-05 18:34:38] Dump 01499th pass to TP0_PP0_Rank0_pid101005/Pass01499.pt
[2026-01-05 18:34:39] Dump 01500th pass to TP0_PP0_Rank0_pid101005/Pass01500.pt
[2026-01-05 18:34:39] Dump 01501th pass to TP0_PP0_Rank0_pid101005/Pass01501.pt
[2026-01-05 18:34:40] Dump 01502th pass to TP0_PP0_Rank0_pid101005/Pass01502.pt
[2026-01-05 18:34:41] Dump 01503th pass to TP0_PP0_Rank0_pid101005/Pass01503.pt
[2026-01-05 18:34:41] Dump 01504th pass to TP0_PP0_Rank0_pid101005/Pass01504.pt
[2026-01-05 18:34:42] Dump 01505th pass to TP0_PP0_Rank0_pid101005/Pass01505.pt
[2026-01-05 18:34:43] Decode batch, #running-req: 76, #token: 41728, token usage: 0.03, cpu graph: False, gen throughput (token/s): 121.29, #queue-req: 0,
[2026-01-05 18:34:43] Dump 01506th pass to TP0_PP0_Rank0_pid101005/Pass01506.pt
[2026-01-05 18:34:43] Dump 01507th pass to TP0_PP0_Rank0_pid101005/Pass01507.pt
[2026-01-05 18:34:44] Dump 01508th pass to TP0_PP0_Rank0_pid101005/Pass01508.pt
[2026-01-05 18:34:45] Dump 01509th pass to TP0_PP0_Rank0_pid101005/Pass01509.pt
[2026-01-05 18:34:45] Dump 01510th pass to TP0_PP0_Rank0_pid101005/Pass01510.pt
[2026-01-05 18:34:46] Dump 01511th pass to TP0_PP0_Rank0_pid101005/Pass01511.pt
[2026-01-05 18:34:46] Dump 01512th pass to TP0_PP0_Rank0_pid101005/Pass01512.pt
[2026-01-05 18:34:47] Dump 01513th pass to TP0_PP0_Rank0_pid101005/Pass01513.pt
[2026-01-05 18:34:47] Dump 01514th pass to TP0_PP0_Rank0_pid101005/Pass01514.pt
[2026-01-05 18:34:48] Dump 01515th pass to TP0_PP0_Rank0_pid101005/Pass01515.pt
[2026-01-05 18:34:48] Dump 01516th pass to TP0_PP0_Rank0_pid101005/Pass01516.pt
[2026-01-05 18:34:49] Dump 01517th pass to TP0_PP0_Rank0_pid101005/Pass01517.pt
[2026-01-05 18:34:49] Dump 01518th pass to TP0_PP0_Rank0_pid101005/Pass01518.pt
[2026-01-05 18:34:49] Dump 01519th pass to TP0_PP0_Rank0_pid101005/Pass01519.pt
[2026-01-05 18:34:50] Dump 01520th pass to TP0_PP0_Rank0_pid101005/Pass01520.pt
[2026-01-05 18:34:50] Dump 01521th pass to TP0_PP0_Rank0_pid101005/Pass01521.pt
[2026-01-05 18:34:51] Dump 01522th pass to TP0_PP0_Rank0_pid101005/Pass01522.pt
[2026-01-05 18:34:51] Dump 01523th pass to TP0_PP0_Rank0_pid101005/Pass01523.pt
[2026-01-05 18:34:51] Dump 01524th pass to TP0_PP0_Rank0_pid101005/Pass01524.pt
[2026-01-05 18:34:52] Dump 01525th pass to TP0_PP0_Rank0_pid101005/Pass01525.pt
[2026-01-05 18:34:52] Dump 01526th pass to TP0_PP0_Rank0_pid101005/Pass01526.pt
[2026-01-05 18:34:53] Dump 01527th pass to TP0_PP0_Rank0_pid101005/Pass01527.pt
[2026-01-05 18:34:53] Dump 01528th pass to TP0_PP0_Rank0_pid101005/Pass01528.pt
[2026-01-05 18:34:53] Dump 01529th pass to TP0_PP0_Rank0_pid101005/Pass01529.pt
[2026-01-05 18:34:54] Dump 01530th pass to TP0_PP0_Rank0_pid101005/Pass01530.pt
[2026-01-05 18:34:54] Dump 01531th pass to TP0_PP0_Rank0_pid101005/Pass01531.pt
[2026-01-05 18:34:54] Dump 01532th pass to TP0_PP0_Rank0_pid101005/Pass01532.pt
[2026-01-05 18:34:55] Dump 01533th pass to TP0_PP0_Rank0_pid101005/Pass01533.pt
[2026-01-05 18:34:55] Dump 01534th pass to TP0_PP0_Rank0_pid101005/Pass01534.pt
[2026-01-05 18:34:55] Dump 01535th pass to TP0_PP0_Rank0_pid101005/Pass01535.pt
[2026-01-05 18:34:55] Dump 01536th pass to TP0_PP0_Rank0_pid101005/Pass01536.pt
[2026-01-05 18:34:56] Dump 01537th pass to TP0_PP0_Rank0_pid101005/Pass01537.pt
[2026-01-05 18:34:56] Dump 01538th pass to TP0_PP0_Rank0_pid101005/Pass01538.pt
[2026-01-05 18:34:56] Dump 01539th pass to TP0_PP0_Rank0_pid101005/Pass01539.pt
[2026-01-05 18:34:56] Dump 01540th pass to TP0_PP0_Rank0_pid101005/Pass01540.pt
[2026-01-05 18:34:56] Dump 01541th pass to TP0_PP0_Rank0_pid101005/Pass01541.pt
[2026-01-05 18:34:56] Dump 01542th pass to TP0_PP0_Rank0_pid101005/Pass01542.pt
[2026-01-05 18:34:57] Dump 01543th pass to TP0_PP0_Rank0_pid101005/Pass01543.pt
[2026-01-05 18:34:57] Dump 01544th pass to TP0_PP0_Rank0_pid101005/Pass01544.pt
[2026-01-05 18:34:57] Dump 01545th pass to TP0_PP0_Rank0_pid101005/Pass01545.pt
[2026-01-05 18:34:57] Decode batch, #running-req: 8, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 108.51, #queue-req: 0,
[2026-01-05 18:34:57] Dump 01546th pass to TP0_PP0_Rank0_pid101005/Pass01546.pt
[2026-01-05 18:34:57] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:34:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:34:57] Dump 01547th pass to TP0_PP0_Rank0_pid101005/Pass01547.pt
[2026-01-05 18:34:57] Dump 01548th pass to TP0_PP0_Rank0_pid101005/Pass01548.pt
[2026-01-05 18:34:58] INFO:     127.0.0.1:51154 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 18:34:59] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 390,
[2026-01-05 18:35:01] Dump 01549th pass to TP0_PP0_Rank0_pid101005/Pass01549.pt
./usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b93e4-227cdd404e494f233f584ca0;aa4d1bb9-7da7-46b4-b86b-e9e615fe971c)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b93e4-227cdd404e494f233f584ca0;aa4d1bb9-7da7-46b4-b86b-e9e615fe971c)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-18:35:17 (PID:106219, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
[2026-01-05 18:35:41] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 11, #queue-req: 379,
[2026-01-05 18:35:45] Dump 01550th pass to TP0_PP0_Rank0_pid101005/Pass01550.pt
[2026-01-05 18:36:22] Prefill batch, #new-seq: 15, #new-token: 8192, #cached-token: 4608, token usage: 0.01, #running-req: 21, #queue-req: 365,
[2026-01-05 18:36:24] Dump 01551th pass to TP0_PP0_Rank0_pid101005/Pass01551.pt
[2026-01-05 18:37:00] Prefill batch, #new-seq: 24, #new-token: 8192, #cached-token: 9728, token usage: 0.01, #running-req: 35, #queue-req: 342,
[2026-01-05 18:37:02] Dump 01552th pass to TP0_PP0_Rank0_pid101005/Pass01552.pt
[2026-01-05 18:37:32] Prefill batch, #new-seq: 21, #new-token: 8192, #cached-token: 9216, token usage: 0.02, #running-req: 58, #queue-req: 322,
[2026-01-05 18:37:36] Dump 01553th pass to TP0_PP0_Rank0_pid101005/Pass01553.pt
[2026-01-05 18:38:13] Prefill batch, #new-seq: 29, #new-token: 8192, #cached-token: 14336, token usage: 0.02, #running-req: 79, #queue-req: 293,
[2026-01-05 18:38:15] Dump 01554th pass to TP0_PP0_Rank0_pid101005/Pass01554.pt
[2026-01-05 18:38:47] Prefill batch, #new-seq: 34, #new-token: 8192, #cached-token: 17408, token usage: 0.02, #running-req: 108, #queue-req: 259,
[2026-01-05 18:38:49] Dump 01555th pass to TP0_PP0_Rank0_pid101005/Pass01555.pt
[2026-01-05 18:39:19] Prefill batch, #new-seq: 35, #new-token: 8192, #cached-token: 19584, token usage: 0.03, #running-req: 142, #queue-req: 224,
[2026-01-05 18:39:21] Dump 01556th pass to TP0_PP0_Rank0_pid101005/Pass01556.pt
[2026-01-05 18:39:57] Prefill batch, #new-seq: 38, #new-token: 8192, #cached-token: 21888, token usage: 0.04, #running-req: 177, #queue-req: 186,
[2026-01-05 18:39:59] Dump 01557th pass to TP0_PP0_Rank0_pid101005/Pass01557.pt
[2026-01-05 18:40:27] Prefill batch, #new-seq: 31, #new-token: 8192, #cached-token: 16384, token usage: 0.04, #running-req: 214, #queue-req: 156,
[2026-01-05 18:40:29] Dump 01558th pass to TP0_PP0_Rank0_pid101005/Pass01558.pt
[2026-01-05 18:41:04] Prefill batch, #new-seq: 43, #new-token: 8192, #cached-token: 25600, token usage: 0.05, #running-req: 245, #queue-req: 113,
[2026-01-05 18:41:06] Dump 01559th pass to TP0_PP0_Rank0_pid101005/Pass01559.pt
[2026-01-05 18:41:36] Prefill batch, #new-seq: 40, #new-token: 8192, #cached-token: 22272, token usage: 0.05, #running-req: 288, #queue-req: 73,
[2026-01-05 18:41:38] Dump 01560th pass to TP0_PP0_Rank0_pid101005/Pass01560.pt
[2026-01-05 18:42:09] Prefill batch, #new-seq: 38, #new-token: 8192, #cached-token: 23040, token usage: 0.06, #running-req: 327, #queue-req: 36,
[2026-01-05 18:42:11] Dump 01561th pass to TP0_PP0_Rank0_pid101005/Pass01561.pt
[2026-01-05 18:42:42] Prefill batch, #new-seq: 36, #new-token: 7424, #cached-token: 23936, token usage: 0.06, #running-req: 365, #queue-req: 0,
[2026-01-05 18:42:44] Dump 01562th pass to TP0_PP0_Rank0_pid101005/Pass01562.pt
[2026-01-05 18:43:10] Dump 01563th pass to TP0_PP0_Rank0_pid101005/Pass01563.pt
[2026-01-05 18:43:13] Dump 01564th pass to TP0_PP0_Rank0_pid101005/Pass01564.pt
[2026-01-05 18:43:16] Dump 01565th pass to TP0_PP0_Rank0_pid101005/Pass01565.pt
[2026-01-05 18:43:19] Dump 01566th pass to TP0_PP0_Rank0_pid101005/Pass01566.pt
[2026-01-05 18:43:23] Dump 01567th pass to TP0_PP0_Rank0_pid101005/Pass01567.pt
[2026-01-05 18:43:26] Dump 01568th pass to TP0_PP0_Rank0_pid101005/Pass01568.pt
[2026-01-05 18:43:31] Dump 01569th pass to TP0_PP0_Rank0_pid101005/Pass01569.pt
[2026-01-05 18:43:34] Dump 01570th pass to TP0_PP0_Rank0_pid101005/Pass01570.pt
[2026-01-05 18:43:37] Dump 01571th pass to TP0_PP0_Rank0_pid101005/Pass01571.pt
[2026-01-05 18:43:40] Dump 01572th pass to TP0_PP0_Rank0_pid101005/Pass01572.pt
[2026-01-05 18:43:42] Dump 01573th pass to TP0_PP0_Rank0_pid101005/Pass01573.pt
[2026-01-05 18:43:46] Dump 01574th pass to TP0_PP0_Rank0_pid101005/Pass01574.pt
[2026-01-05 18:43:48] Dump 01575th pass to TP0_PP0_Rank0_pid101005/Pass01575.pt
[2026-01-05 18:43:51] Dump 01576th pass to TP0_PP0_Rank0_pid101005/Pass01576.pt
[2026-01-05 18:43:54] Dump 01577th pass to TP0_PP0_Rank0_pid101005/Pass01577.pt
[2026-01-05 18:43:57] Dump 01578th pass to TP0_PP0_Rank0_pid101005/Pass01578.pt
[2026-01-05 18:43:59] Dump 01579th pass to TP0_PP0_Rank0_pid101005/Pass01579.pt
[2026-01-05 18:44:01] Dump 01580th pass to TP0_PP0_Rank0_pid101005/Pass01580.pt
[2026-01-05 18:44:04] Dump 01581th pass to TP0_PP0_Rank0_pid101005/Pass01581.pt
[2026-01-05 18:44:07] Dump 01582th pass to TP0_PP0_Rank0_pid101005/Pass01582.pt
[2026-01-05 18:44:09] Dump 01583th pass to TP0_PP0_Rank0_pid101005/Pass01583.pt
[2026-01-05 18:44:12] Dump 01584th pass to TP0_PP0_Rank0_pid101005/Pass01584.pt
[2026-01-05 18:44:15] Dump 01585th pass to TP0_PP0_Rank0_pid101005/Pass01585.pt
[2026-01-05 18:44:18] Dump 01586th pass to TP0_PP0_Rank0_pid101005/Pass01586.pt
[2026-01-05 18:44:21] Dump 01587th pass to TP0_PP0_Rank0_pid101005/Pass01587.pt
[2026-01-05 18:44:24] Dump 01588th pass to TP0_PP0_Rank0_pid101005/Pass01588.pt
[2026-01-05 18:44:27] Dump 01589th pass to TP0_PP0_Rank0_pid101005/Pass01589.pt
[2026-01-05 18:44:29] Dump 01590th pass to TP0_PP0_Rank0_pid101005/Pass01590.pt
[2026-01-05 18:44:32] Dump 01591th pass to TP0_PP0_Rank0_pid101005/Pass01591.pt
[2026-01-05 18:44:35] Dump 01592th pass to TP0_PP0_Rank0_pid101005/Pass01592.pt
[2026-01-05 18:44:38] Dump 01593th pass to TP0_PP0_Rank0_pid101005/Pass01593.pt
[2026-01-05 18:44:41] Dump 01594th pass to TP0_PP0_Rank0_pid101005/Pass01594.pt
[2026-01-05 18:44:44] Dump 01595th pass to TP0_PP0_Rank0_pid101005/Pass01595.pt
[2026-01-05 18:44:47] Dump 01596th pass to TP0_PP0_Rank0_pid101005/Pass01596.pt
[2026-01-05 18:44:49] Dump 01597th pass to TP0_PP0_Rank0_pid101005/Pass01597.pt
[2026-01-05 18:44:52] Dump 01598th pass to TP0_PP0_Rank0_pid101005/Pass01598.pt
[2026-01-05 18:44:55] Dump 01599th pass to TP0_PP0_Rank0_pid101005/Pass01599.pt
[2026-01-05 18:44:58] Dump 01600th pass to TP0_PP0_Rank0_pid101005/Pass01600.pt
[2026-01-05 18:45:01] Decode batch, #running-req: 343, #token: 99328, token usage: 0.07, cpu graph: False, gen throughput (token/s): 22.45, #queue-req: 0,
[2026-01-05 18:45:01] Dump 01601th pass to TP0_PP0_Rank0_pid101005/Pass01601.pt
[2026-01-05 18:45:04] Dump 01602th pass to TP0_PP0_Rank0_pid101005/Pass01602.pt
[2026-01-05 18:45:07] Dump 01603th pass to TP0_PP0_Rank0_pid101005/Pass01603.pt
[2026-01-05 18:45:11] Dump 01604th pass to TP0_PP0_Rank0_pid101005/Pass01604.pt
[2026-01-05 18:45:14] Dump 01605th pass to TP0_PP0_Rank0_pid101005/Pass01605.pt
[2026-01-05 18:45:17] Dump 01606th pass to TP0_PP0_Rank0_pid101005/Pass01606.pt
[2026-01-05 18:45:19] Dump 01607th pass to TP0_PP0_Rank0_pid101005/Pass01607.pt
[2026-01-05 18:45:22] Dump 01608th pass to TP0_PP0_Rank0_pid101005/Pass01608.pt
[2026-01-05 18:45:25] Dump 01609th pass to TP0_PP0_Rank0_pid101005/Pass01609.pt
[2026-01-05 18:45:28] Dump 01610th pass to TP0_PP0_Rank0_pid101005/Pass01610.pt
[2026-01-05 18:45:31] Dump 01611th pass to TP0_PP0_Rank0_pid101005/Pass01611.pt
[2026-01-05 18:45:33] Dump 01612th pass to TP0_PP0_Rank0_pid101005/Pass01612.pt
[2026-01-05 18:45:36] Dump 01613th pass to TP0_PP0_Rank0_pid101005/Pass01613.pt
[2026-01-05 18:45:39] Dump 01614th pass to TP0_PP0_Rank0_pid101005/Pass01614.pt
[2026-01-05 18:45:42] Dump 01615th pass to TP0_PP0_Rank0_pid101005/Pass01615.pt
[2026-01-05 18:45:44] Dump 01616th pass to TP0_PP0_Rank0_pid101005/Pass01616.pt
[2026-01-05 18:45:46] Dump 01617th pass to TP0_PP0_Rank0_pid101005/Pass01617.pt
[2026-01-05 18:45:49] Dump 01618th pass to TP0_PP0_Rank0_pid101005/Pass01618.pt
[2026-01-05 18:45:52] Dump 01619th pass to TP0_PP0_Rank0_pid101005/Pass01619.pt
[2026-01-05 18:45:55] Dump 01620th pass to TP0_PP0_Rank0_pid101005/Pass01620.pt
[2026-01-05 18:45:57] Dump 01621th pass to TP0_PP0_Rank0_pid101005/Pass01621.pt
[2026-01-05 18:46:00] Dump 01622th pass to TP0_PP0_Rank0_pid101005/Pass01622.pt
[2026-01-05 18:46:02] Dump 01623th pass to TP0_PP0_Rank0_pid101005/Pass01623.pt
[2026-01-05 18:46:05] Dump 01624th pass to TP0_PP0_Rank0_pid101005/Pass01624.pt
[2026-01-05 18:46:08] Dump 01625th pass to TP0_PP0_Rank0_pid101005/Pass01625.pt
[2026-01-05 18:46:11] Dump 01626th pass to TP0_PP0_Rank0_pid101005/Pass01626.pt
[2026-01-05 18:46:13] Dump 01627th pass to TP0_PP0_Rank0_pid101005/Pass01627.pt
[2026-01-05 18:46:15] Dump 01628th pass to TP0_PP0_Rank0_pid101005/Pass01628.pt
[2026-01-05 18:46:17] Dump 01629th pass to TP0_PP0_Rank0_pid101005/Pass01629.pt
[2026-01-05 18:46:20] Dump 01630th pass to TP0_PP0_Rank0_pid101005/Pass01630.pt
[2026-01-05 18:46:22] Dump 01631th pass to TP0_PP0_Rank0_pid101005/Pass01631.pt
[2026-01-05 18:46:24] Dump 01632th pass to TP0_PP0_Rank0_pid101005/Pass01632.pt
[2026-01-05 18:46:27] Dump 01633th pass to TP0_PP0_Rank0_pid101005/Pass01633.pt
[2026-01-05 18:46:29] Dump 01634th pass to TP0_PP0_Rank0_pid101005/Pass01634.pt
[2026-01-05 18:46:31] Dump 01635th pass to TP0_PP0_Rank0_pid101005/Pass01635.pt
[2026-01-05 18:46:34] Dump 01636th pass to TP0_PP0_Rank0_pid101005/Pass01636.pt
[2026-01-05 18:46:36] Dump 01637th pass to TP0_PP0_Rank0_pid101005/Pass01637.pt
[2026-01-05 18:46:38] Dump 01638th pass to TP0_PP0_Rank0_pid101005/Pass01638.pt
[2026-01-05 18:46:41] Dump 01639th pass to TP0_PP0_Rank0_pid101005/Pass01639.pt
[2026-01-05 18:46:44] Dump 01640th pass to TP0_PP0_Rank0_pid101005/Pass01640.pt
[2026-01-05 18:46:46] Decode batch, #running-req: 282, #token: 95488, token usage: 0.06, cpu graph: False, gen throughput (token/s): 117.94, #queue-req: 0,
[2026-01-05 18:46:46] Dump 01641th pass to TP0_PP0_Rank0_pid101005/Pass01641.pt
[2026-01-05 18:46:49] Dump 01642th pass to TP0_PP0_Rank0_pid101005/Pass01642.pt
[2026-01-05 18:46:51] Dump 01643th pass to TP0_PP0_Rank0_pid101005/Pass01643.pt
[2026-01-05 18:46:53] Dump 01644th pass to TP0_PP0_Rank0_pid101005/Pass01644.pt
[2026-01-05 18:46:55] Dump 01645th pass to TP0_PP0_Rank0_pid101005/Pass01645.pt
[2026-01-05 18:46:58] Dump 01646th pass to TP0_PP0_Rank0_pid101005/Pass01646.pt
[2026-01-05 18:47:00] Dump 01647th pass to TP0_PP0_Rank0_pid101005/Pass01647.pt
[2026-01-05 18:47:02] Dump 01648th pass to TP0_PP0_Rank0_pid101005/Pass01648.pt
[2026-01-05 18:47:04] Dump 01649th pass to TP0_PP0_Rank0_pid101005/Pass01649.pt
[2026-01-05 18:47:06] Dump 01650th pass to TP0_PP0_Rank0_pid101005/Pass01650.pt
[2026-01-05 18:47:08] Dump 01651th pass to TP0_PP0_Rank0_pid101005/Pass01651.pt
[2026-01-05 18:47:10] Dump 01652th pass to TP0_PP0_Rank0_pid101005/Pass01652.pt
[2026-01-05 18:47:13] Dump 01653th pass to TP0_PP0_Rank0_pid101005/Pass01653.pt
[2026-01-05 18:47:15] Dump 01654th pass to TP0_PP0_Rank0_pid101005/Pass01654.pt
[2026-01-05 18:47:17] Dump 01655th pass to TP0_PP0_Rank0_pid101005/Pass01655.pt
[2026-01-05 18:47:19] Dump 01656th pass to TP0_PP0_Rank0_pid101005/Pass01656.pt
[2026-01-05 18:47:21] Dump 01657th pass to TP0_PP0_Rank0_pid101005/Pass01657.pt
[2026-01-05 18:47:23] Dump 01658th pass to TP0_PP0_Rank0_pid101005/Pass01658.pt
[2026-01-05 18:47:24] Dump 01659th pass to TP0_PP0_Rank0_pid101005/Pass01659.pt
[2026-01-05 18:47:27] Dump 01660th pass to TP0_PP0_Rank0_pid101005/Pass01660.pt
[2026-01-05 18:47:29] Dump 01661th pass to TP0_PP0_Rank0_pid101005/Pass01661.pt
[2026-01-05 18:47:30] Dump 01662th pass to TP0_PP0_Rank0_pid101005/Pass01662.pt
[2026-01-05 18:47:33] Dump 01663th pass to TP0_PP0_Rank0_pid101005/Pass01663.pt
[2026-01-05 18:47:35] Dump 01664th pass to TP0_PP0_Rank0_pid101005/Pass01664.pt
[2026-01-05 18:47:36] Dump 01665th pass to TP0_PP0_Rank0_pid101005/Pass01665.pt
[2026-01-05 18:47:38] Dump 01666th pass to TP0_PP0_Rank0_pid101005/Pass01666.pt
[2026-01-05 18:47:40] Dump 01667th pass to TP0_PP0_Rank0_pid101005/Pass01667.pt
[2026-01-05 18:47:42] Dump 01668th pass to TP0_PP0_Rank0_pid101005/Pass01668.pt
[2026-01-05 18:47:44] Dump 01669th pass to TP0_PP0_Rank0_pid101005/Pass01669.pt
[2026-01-05 18:47:46] Dump 01670th pass to TP0_PP0_Rank0_pid101005/Pass01670.pt
[2026-01-05 18:47:48] Dump 01671th pass to TP0_PP0_Rank0_pid101005/Pass01671.pt
[2026-01-05 18:47:50] Dump 01672th pass to TP0_PP0_Rank0_pid101005/Pass01672.pt
[2026-01-05 18:47:51] Dump 01673th pass to TP0_PP0_Rank0_pid101005/Pass01673.pt
[2026-01-05 18:47:53] Dump 01674th pass to TP0_PP0_Rank0_pid101005/Pass01674.pt
[2026-01-05 18:47:55] Dump 01675th pass to TP0_PP0_Rank0_pid101005/Pass01675.pt
[2026-01-05 18:47:57] Dump 01676th pass to TP0_PP0_Rank0_pid101005/Pass01676.pt
[2026-01-05 18:47:59] Dump 01677th pass to TP0_PP0_Rank0_pid101005/Pass01677.pt
[2026-01-05 18:48:00] Dump 01678th pass to TP0_PP0_Rank0_pid101005/Pass01678.pt
[2026-01-05 18:48:02] Dump 01679th pass to TP0_PP0_Rank0_pid101005/Pass01679.pt
[2026-01-05 18:48:03] Dump 01680th pass to TP0_PP0_Rank0_pid101005/Pass01680.pt
[2026-01-05 18:48:05] Decode batch, #running-req: 212, #token: 84608, token usage: 0.06, cpu graph: False, gen throughput (token/s): 126.00, #queue-req: 0,
[2026-01-05 18:48:05] Dump 01681th pass to TP0_PP0_Rank0_pid101005/Pass01681.pt
[2026-01-05 18:48:07] Dump 01682th pass to TP0_PP0_Rank0_pid101005/Pass01682.pt
[2026-01-05 18:48:09] Dump 01683th pass to TP0_PP0_Rank0_pid101005/Pass01683.pt
[2026-01-05 18:48:11] Dump 01684th pass to TP0_PP0_Rank0_pid101005/Pass01684.pt
[2026-01-05 18:48:12] Dump 01685th pass to TP0_PP0_Rank0_pid101005/Pass01685.pt
[2026-01-05 18:48:14] Dump 01686th pass to TP0_PP0_Rank0_pid101005/Pass01686.pt
[2026-01-05 18:48:16] Dump 01687th pass to TP0_PP0_Rank0_pid101005/Pass01687.pt
[2026-01-05 18:48:17] Dump 01688th pass to TP0_PP0_Rank0_pid101005/Pass01688.pt
[2026-01-05 18:48:19] Dump 01689th pass to TP0_PP0_Rank0_pid101005/Pass01689.pt
[2026-01-05 18:48:21] Dump 01690th pass to TP0_PP0_Rank0_pid101005/Pass01690.pt
[2026-01-05 18:48:22] Dump 01691th pass to TP0_PP0_Rank0_pid101005/Pass01691.pt
[2026-01-05 18:48:23] Dump 01692th pass to TP0_PP0_Rank0_pid101005/Pass01692.pt
[2026-01-05 18:48:25] Dump 01693th pass to TP0_PP0_Rank0_pid101005/Pass01693.pt
[2026-01-05 18:48:27] Dump 01694th pass to TP0_PP0_Rank0_pid101005/Pass01694.pt
[2026-01-05 18:48:28] Dump 01695th pass to TP0_PP0_Rank0_pid101005/Pass01695.pt
[2026-01-05 18:48:30] Dump 01696th pass to TP0_PP0_Rank0_pid101005/Pass01696.pt
[2026-01-05 18:48:32] Dump 01697th pass to TP0_PP0_Rank0_pid101005/Pass01697.pt
[2026-01-05 18:48:33] Dump 01698th pass to TP0_PP0_Rank0_pid101005/Pass01698.pt
[2026-01-05 18:48:35] Dump 01699th pass to TP0_PP0_Rank0_pid101005/Pass01699.pt
[2026-01-05 18:48:36] Dump 01700th pass to TP0_PP0_Rank0_pid101005/Pass01700.pt
[2026-01-05 18:48:38] Dump 01701th pass to TP0_PP0_Rank0_pid101005/Pass01701.pt
[2026-01-05 18:48:39] Dump 01702th pass to TP0_PP0_Rank0_pid101005/Pass01702.pt
[2026-01-05 18:48:40] Dump 01703th pass to TP0_PP0_Rank0_pid101005/Pass01703.pt
[2026-01-05 18:48:42] Dump 01704th pass to TP0_PP0_Rank0_pid101005/Pass01704.pt
[2026-01-05 18:48:43] Dump 01705th pass to TP0_PP0_Rank0_pid101005/Pass01705.pt
[2026-01-05 18:48:45] Dump 01706th pass to TP0_PP0_Rank0_pid101005/Pass01706.pt
[2026-01-05 18:48:46] Dump 01707th pass to TP0_PP0_Rank0_pid101005/Pass01707.pt
[2026-01-05 18:48:48] Dump 01708th pass to TP0_PP0_Rank0_pid101005/Pass01708.pt
[2026-01-05 18:48:49] Dump 01709th pass to TP0_PP0_Rank0_pid101005/Pass01709.pt
[2026-01-05 18:48:50] Dump 01710th pass to TP0_PP0_Rank0_pid101005/Pass01710.pt
[2026-01-05 18:48:52] Dump 01711th pass to TP0_PP0_Rank0_pid101005/Pass01711.pt
[2026-01-05 18:48:53] Dump 01712th pass to TP0_PP0_Rank0_pid101005/Pass01712.pt
[2026-01-05 18:48:55] Dump 01713th pass to TP0_PP0_Rank0_pid101005/Pass01713.pt
[2026-01-05 18:48:56] Dump 01714th pass to TP0_PP0_Rank0_pid101005/Pass01714.pt
[2026-01-05 18:48:58] Dump 01715th pass to TP0_PP0_Rank0_pid101005/Pass01715.pt
[2026-01-05 18:48:59] Dump 01716th pass to TP0_PP0_Rank0_pid101005/Pass01716.pt
[2026-01-05 18:49:00] Dump 01717th pass to TP0_PP0_Rank0_pid101005/Pass01717.pt
[2026-01-05 18:49:02] Dump 01718th pass to TP0_PP0_Rank0_pid101005/Pass01718.pt
[2026-01-05 18:49:03] Dump 01719th pass to TP0_PP0_Rank0_pid101005/Pass01719.pt
[2026-01-05 18:49:04] Dump 01720th pass to TP0_PP0_Rank0_pid101005/Pass01720.pt
[2026-01-05 18:49:06] Decode batch, #running-req: 165, #token: 74240, token usage: 0.05, cpu graph: False, gen throughput (token/s): 122.91, #queue-req: 0,
[2026-01-05 18:49:06] Dump 01721th pass to TP0_PP0_Rank0_pid101005/Pass01721.pt
[2026-01-05 18:49:07] Dump 01722th pass to TP0_PP0_Rank0_pid101005/Pass01722.pt
[2026-01-05 18:49:09] Dump 01723th pass to TP0_PP0_Rank0_pid101005/Pass01723.pt
[2026-01-05 18:49:10] Dump 01724th pass to TP0_PP0_Rank0_pid101005/Pass01724.pt
[2026-01-05 18:49:11] Dump 01725th pass to TP0_PP0_Rank0_pid101005/Pass01725.pt
[2026-01-05 18:49:12] Dump 01726th pass to TP0_PP0_Rank0_pid101005/Pass01726.pt
[2026-01-05 18:49:14] Dump 01727th pass to TP0_PP0_Rank0_pid101005/Pass01727.pt
[2026-01-05 18:49:15] Dump 01728th pass to TP0_PP0_Rank0_pid101005/Pass01728.pt
[2026-01-05 18:49:16] Dump 01729th pass to TP0_PP0_Rank0_pid101005/Pass01729.pt
[2026-01-05 18:49:18] Dump 01730th pass to TP0_PP0_Rank0_pid101005/Pass01730.pt
[2026-01-05 18:49:19] Dump 01731th pass to TP0_PP0_Rank0_pid101005/Pass01731.pt
[2026-01-05 18:49:20] Dump 01732th pass to TP0_PP0_Rank0_pid101005/Pass01732.pt
[2026-01-05 18:49:21] Dump 01733th pass to TP0_PP0_Rank0_pid101005/Pass01733.pt
[2026-01-05 18:49:23] Dump 01734th pass to TP0_PP0_Rank0_pid101005/Pass01734.pt
[2026-01-05 18:49:24] Dump 01735th pass to TP0_PP0_Rank0_pid101005/Pass01735.pt
[2026-01-05 18:49:25] Dump 01736th pass to TP0_PP0_Rank0_pid101005/Pass01736.pt
[2026-01-05 18:49:26] Dump 01737th pass to TP0_PP0_Rank0_pid101005/Pass01737.pt
[2026-01-05 18:49:27] Dump 01738th pass to TP0_PP0_Rank0_pid101005/Pass01738.pt
[2026-01-05 18:49:28] Dump 01739th pass to TP0_PP0_Rank0_pid101005/Pass01739.pt
[2026-01-05 18:49:29] Dump 01740th pass to TP0_PP0_Rank0_pid101005/Pass01740.pt
[2026-01-05 18:49:30] Dump 01741th pass to TP0_PP0_Rank0_pid101005/Pass01741.pt
[2026-01-05 18:49:31] Dump 01742th pass to TP0_PP0_Rank0_pid101005/Pass01742.pt
[2026-01-05 18:49:32] Dump 01743th pass to TP0_PP0_Rank0_pid101005/Pass01743.pt
[2026-01-05 18:49:34] Dump 01744th pass to TP0_PP0_Rank0_pid101005/Pass01744.pt
[2026-01-05 18:49:35] Dump 01745th pass to TP0_PP0_Rank0_pid101005/Pass01745.pt
[2026-01-05 18:49:36] Dump 01746th pass to TP0_PP0_Rank0_pid101005/Pass01746.pt
[2026-01-05 18:49:36] Dump 01747th pass to TP0_PP0_Rank0_pid101005/Pass01747.pt
[2026-01-05 18:49:37] Dump 01748th pass to TP0_PP0_Rank0_pid101005/Pass01748.pt
[2026-01-05 18:49:38] Dump 01749th pass to TP0_PP0_Rank0_pid101005/Pass01749.pt
[2026-01-05 18:49:39] Dump 01750th pass to TP0_PP0_Rank0_pid101005/Pass01750.pt
[2026-01-05 18:49:40] Dump 01751th pass to TP0_PP0_Rank0_pid101005/Pass01751.pt
[2026-01-05 18:49:41] Dump 01752th pass to TP0_PP0_Rank0_pid101005/Pass01752.pt
[2026-01-05 18:49:42] Dump 01753th pass to TP0_PP0_Rank0_pid101005/Pass01753.pt
[2026-01-05 18:49:43] Dump 01754th pass to TP0_PP0_Rank0_pid101005/Pass01754.pt
[2026-01-05 18:49:44] Dump 01755th pass to TP0_PP0_Rank0_pid101005/Pass01755.pt
[2026-01-05 18:49:45] Dump 01756th pass to TP0_PP0_Rank0_pid101005/Pass01756.pt
[2026-01-05 18:49:46] Dump 01757th pass to TP0_PP0_Rank0_pid101005/Pass01757.pt
[2026-01-05 18:49:47] Dump 01758th pass to TP0_PP0_Rank0_pid101005/Pass01758.pt
[2026-01-05 18:49:48] Dump 01759th pass to TP0_PP0_Rank0_pid101005/Pass01759.pt
[2026-01-05 18:49:48] Dump 01760th pass to TP0_PP0_Rank0_pid101005/Pass01760.pt
[2026-01-05 18:49:49] Decode batch, #running-req: 102, #token: 54272, token usage: 0.04, cpu graph: False, gen throughput (token/s): 121.37, #queue-req: 0,
[2026-01-05 18:49:49] Dump 01761th pass to TP0_PP0_Rank0_pid101005/Pass01761.pt
[2026-01-05 18:49:50] Dump 01762th pass to TP0_PP0_Rank0_pid101005/Pass01762.pt
[2026-01-05 18:49:51] Dump 01763th pass to TP0_PP0_Rank0_pid101005/Pass01763.pt
[2026-01-05 18:49:52] Dump 01764th pass to TP0_PP0_Rank0_pid101005/Pass01764.pt
[2026-01-05 18:49:52] Dump 01765th pass to TP0_PP0_Rank0_pid101005/Pass01765.pt
[2026-01-05 18:49:53] Dump 01766th pass to TP0_PP0_Rank0_pid101005/Pass01766.pt
[2026-01-05 18:49:54] Dump 01767th pass to TP0_PP0_Rank0_pid101005/Pass01767.pt
[2026-01-05 18:49:55] Dump 01768th pass to TP0_PP0_Rank0_pid101005/Pass01768.pt
[2026-01-05 18:49:55] Dump 01769th pass to TP0_PP0_Rank0_pid101005/Pass01769.pt
[2026-01-05 18:49:56] Dump 01770th pass to TP0_PP0_Rank0_pid101005/Pass01770.pt
[2026-01-05 18:49:57] Dump 01771th pass to TP0_PP0_Rank0_pid101005/Pass01771.pt
[2026-01-05 18:49:57] Dump 01772th pass to TP0_PP0_Rank0_pid101005/Pass01772.pt
[2026-01-05 18:49:58] Dump 01773th pass to TP0_PP0_Rank0_pid101005/Pass01773.pt
[2026-01-05 18:49:59] Dump 01774th pass to TP0_PP0_Rank0_pid101005/Pass01774.pt
[2026-01-05 18:49:59] Dump 01775th pass to TP0_PP0_Rank0_pid101005/Pass01775.pt
[2026-01-05 18:50:00] Dump 01776th pass to TP0_PP0_Rank0_pid101005/Pass01776.pt
[2026-01-05 18:50:01] Dump 01777th pass to TP0_PP0_Rank0_pid101005/Pass01777.pt
[2026-01-05 18:50:01] Dump 01778th pass to TP0_PP0_Rank0_pid101005/Pass01778.pt
[2026-01-05 18:50:02] Dump 01779th pass to TP0_PP0_Rank0_pid101005/Pass01779.pt
[2026-01-05 18:50:02] Dump 01780th pass to TP0_PP0_Rank0_pid101005/Pass01780.pt
[2026-01-05 18:50:03] Dump 01781th pass to TP0_PP0_Rank0_pid101005/Pass01781.pt
[2026-01-05 18:50:04] Dump 01782th pass to TP0_PP0_Rank0_pid101005/Pass01782.pt
[2026-01-05 18:50:04] Dump 01783th pass to TP0_PP0_Rank0_pid101005/Pass01783.pt
[2026-01-05 18:50:05] Dump 01784th pass to TP0_PP0_Rank0_pid101005/Pass01784.pt
[2026-01-05 18:50:05] Dump 01785th pass to TP0_PP0_Rank0_pid101005/Pass01785.pt
[2026-01-05 18:50:06] Dump 01786th pass to TP0_PP0_Rank0_pid101005/Pass01786.pt
[2026-01-05 18:50:06] Dump 01787th pass to TP0_PP0_Rank0_pid101005/Pass01787.pt
[2026-01-05 18:50:07] Dump 01788th pass to TP0_PP0_Rank0_pid101005/Pass01788.pt
[2026-01-05 18:50:07] Dump 01789th pass to TP0_PP0_Rank0_pid101005/Pass01789.pt
[2026-01-05 18:50:08] Dump 01790th pass to TP0_PP0_Rank0_pid101005/Pass01790.pt
[2026-01-05 18:50:08] Dump 01791th pass to TP0_PP0_Rank0_pid101005/Pass01791.pt
[2026-01-05 18:50:09] Dump 01792th pass to TP0_PP0_Rank0_pid101005/Pass01792.pt
[2026-01-05 18:50:09] Dump 01793th pass to TP0_PP0_Rank0_pid101005/Pass01793.pt
[2026-01-05 18:50:09] Dump 01794th pass to TP0_PP0_Rank0_pid101005/Pass01794.pt
[2026-01-05 18:50:10] Dump 01795th pass to TP0_PP0_Rank0_pid101005/Pass01795.pt
[2026-01-05 18:50:10] Dump 01796th pass to TP0_PP0_Rank0_pid101005/Pass01796.pt
[2026-01-05 18:50:11] Dump 01797th pass to TP0_PP0_Rank0_pid101005/Pass01797.pt
[2026-01-05 18:50:11] Dump 01798th pass to TP0_PP0_Rank0_pid101005/Pass01798.pt
[2026-01-05 18:50:11] Dump 01799th pass to TP0_PP0_Rank0_pid101005/Pass01799.pt
[2026-01-05 18:50:12] Dump 01800th pass to TP0_PP0_Rank0_pid101005/Pass01800.pt
[2026-01-05 18:50:12] Decode batch, #running-req: 33, #token: 23424, token usage: 0.02, cpu graph: False, gen throughput (token/s): 116.49, #queue-req: 0,
[2026-01-05 18:50:12] Dump 01801th pass to TP0_PP0_Rank0_pid101005/Pass01801.pt
[2026-01-05 18:50:12] Dump 01802th pass to TP0_PP0_Rank0_pid101005/Pass01802.pt
[2026-01-05 18:50:12] Dump 01803th pass to TP0_PP0_Rank0_pid101005/Pass01803.pt
[2026-01-05 18:50:13] Dump 01804th pass to TP0_PP0_Rank0_pid101005/Pass01804.pt
[2026-01-05 18:50:13] Dump 01805th pass to TP0_PP0_Rank0_pid101005/Pass01805.pt
[2026-01-05 18:50:13] Dump 01806th pass to TP0_PP0_Rank0_pid101005/Pass01806.pt
[2026-01-05 18:50:13] Dump 01807th pass to TP0_PP0_Rank0_pid101005/Pass01807.pt
[2026-01-05 18:50:14] Dump 01808th pass to TP0_PP0_Rank0_pid101005/Pass01808.pt
[2026-01-05 18:50:14] Dump 01809th pass to TP0_PP0_Rank0_pid101005/Pass01809.pt
[2026-01-05 18:50:14] Dump 01810th pass to TP0_PP0_Rank0_pid101005/Pass01810.pt
[2026-01-05 18:50:14] Dump 01811th pass to TP0_PP0_Rank0_pid101005/Pass01811.pt
[2026-01-05 18:50:14] Dump 01812th pass to TP0_PP0_Rank0_pid101005/Pass01812.pt
[2026-01-05 18:50:15] Dump 01813th pass to TP0_PP0_Rank0_pid101005/Pass01813.pt
[2026-01-05 18:50:15] Dump 01814th pass to TP0_PP0_Rank0_pid101005/Pass01814.pt
[2026-01-05 18:50:15] Dump 01815th pass to TP0_PP0_Rank0_pid101005/Pass01815.pt
[2026-01-05 18:50:15] Dump 01816th pass to TP0_PP0_Rank0_pid101005/Pass01816.pt
[2026-01-05 18:50:15] Dump 01817th pass to TP0_PP0_Rank0_pid101005/Pass01817.pt
[2026-01-05 18:50:15] Dump 01818th pass to TP0_PP0_Rank0_pid101005/Pass01818.pt
[2026-01-05 18:50:16] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 18:50:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 18:50:16] Dump 01819th pass to TP0_PP0_Rank0_pid101005/Pass01819.pt
[2026-01-05 18:50:16] Dump 01820th pass to TP0_PP0_Rank0_pid101005/Pass01820.pt
[2026-01-05 18:50:17] INFO:     127.0.0.1:56642 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 18:50:17] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 389,
[2026-01-05 18:50:20] Dump 01821th pass to TP0_PP0_Rank0_pid101005/Pass01821.pt
./usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695b977a-08aa21f33786a6144f4ed871;22daf3eb-10ab-4b1e-a824-e88710602811)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 5055, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4552, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 318, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 677, in __post_init__
    self._handle_gpu_memory_settings(gpu_mem)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 954, in _handle_gpu_memory_settings
    model_config = self.get_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4566, in get_model_config
    self.model_config = ModelConfig.from_server_args(self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 240, in from_server_args
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/configs/model_config.py", line 126, in __init__
    self.hf_config = get_config(
                     ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 3190, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 273, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.
401 Client Error. (Request ID: Root=1-695b977a-08aa21f33786a6144f4ed871;22daf3eb-10ab-4b1e-a824-e88710602811)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.
[ERROR] 2026-01-05-18:50:36 (PID:106627, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
[2026-01-05 18:51:02] Prefill batch, #new-seq: 14, #new-token: 8192, #cached-token: 384, token usage: 0.01, #running-req: 11, #queue-req: 376,
[2026-01-05 18:51:04] Dump 01822th pass to TP0_PP0_Rank0_pid101005/Pass01822.pt
[2026-01-05 18:51:47] Prefill batch, #new-seq: 16, #new-token: 8192, #cached-token: 4480, token usage: 0.01, #running-req: 24, #queue-req: 361,
[2026-01-05 18:51:49] Dump 01823th pass to TP0_PP0_Rank0_pid101005/Pass01823.pt
[2026-01-05 18:52:26] Prefill batch, #new-seq: 22, #new-token: 8192, #cached-token: 5760, token usage: 0.01, #running-req: 40, #queue-req: 339,
[2026-01-05 18:52:28] Dump 01824th pass to TP0_PP0_Rank0_pid101005/Pass01824.pt
[2026-01-05 18:53:02] Prefill batch, #new-seq: 30, #new-token: 8192, #cached-token: 11264, token usage: 0.02, #running-req: 61, #queue-req: 310,
[2026-01-05 18:53:05] Dump 01825th pass to TP0_PP0_Rank0_pid101005/Pass01825.pt
[2026-01-05 18:53:38] Prefill batch, #new-seq: 30, #new-token: 8192, #cached-token: 12288, token usage: 0.02, #running-req: 90, #queue-req: 281,
[2026-01-05 18:53:41] Dump 01826th pass to TP0_PP0_Rank0_pid101005/Pass01826.pt
[2026-01-05 18:54:11] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 14080, token usage: 0.03, #running-req: 119, #queue-req: 250,
[2026-01-05 18:54:14] Dump 01827th pass to TP0_PP0_Rank0_pid101005/Pass01827.pt
[2026-01-05 18:54:47] Prefill batch, #new-seq: 29, #new-token: 8192, #cached-token: 13824, token usage: 0.03, #running-req: 150, #queue-req: 222,
[2026-01-05 18:54:50] Dump 01828th pass to TP0_PP0_Rank0_pid101005/Pass01828.pt
[2026-01-05 18:55:24] Prefill batch, #new-seq: 34, #new-token: 8192, #cached-token: 18304, token usage: 0.04, #running-req: 179, #queue-req: 188,
[2026-01-05 18:55:26] Dump 01829th pass to TP0_PP0_Rank0_pid101005/Pass01829.pt
[2026-01-05 18:55:57] Prefill batch, #new-seq: 34, #new-token: 8192, #cached-token: 16768, token usage: 0.04, #running-req: 212, #queue-req: 155,
[2026-01-05 18:56:00] Dump 01830th pass to TP0_PP0_Rank0_pid101005/Pass01830.pt
[2026-01-05 18:56:35] Prefill batch, #new-seq: 35, #new-token: 8192, #cached-token: 15616, token usage: 0.05, #running-req: 245, #queue-req: 121,
[2026-01-05 18:56:38] Dump 01831th pass to TP0_PP0_Rank0_pid101005/Pass01831.pt
